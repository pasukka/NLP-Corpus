

Применение автоматического машинного обучения к нейросетям с архитектурой «трансформер» / Habr


              17  July  2019 at 10:00  Применение автоматического машинного обучения к нейросетям с архитектурой «трансформер» Machine learning *Artificial Intelligence  
        Translation
         
                Original author:
                
                  David So
                  Из блога Google AI


С момента обнародования информации о них в 2017 году, нейросети архитектуры типа "трансформер" применялись к задачам различного толка, от генерирования текстов в стиле фэнтези до написания музыкальных гармоний. Что важно, высокое качество работы «трансформеров» показало, что в применении к последовательным задачам, например, к моделированию языка и переводу, нейросети с прямым распространением могут быть настолько же эффективными, как и рекуррентные. И хотя популярность трансформера и других моделей с прямым распространением, используемых в последовательных задачах, растёт, их архитектуры почти всегда создаются вручную, в отличие от области компьютерного зрения, где подходы автоматического машинного обучения (АОМ) уже обнаружили передовые модели, опережающие те, что подвергались ручной настройке. Естественно, нас интересовало, может ли применение АОМ к последовательным задачам достичь такого же успеха.


Проведя эволюционный поиск нейроархитектуры (neural architecture search, NAS), и используя перевод в качестве образца последовательных задач, мы обнаружили эволюционировавший трансформер (ЭТ) – новую архитектуру трансформера, демонстрирующую улучшения на различных задачах обработки естественного языка (ОЕЯ). ЭТ не только достигает передовых результатов в переводе, но и демонстрирует улучшение эффективности при моделировании языка по сравнению с изначальным трансформером. Мы публикуем новую модель в библиотеке Tensor2Tensor, где её можно использовать для любой последовательной задачи.

Разработка техник

Чтобы начать эволюционный поиск нейроархитектуры, нам необходимо было разработать новые техники, поскольку задача, использовавшаяся для оценки «приспособленности» каждой из архитектур, перевод с английского на немецкий WMT’14, была требовательной к вычислительным ресурсам. В результате эти поиски оказываются более требовательными, чем схожие поиски в области компьютерного зрения, способные оперировать меньшими базами данных, например, CIFAR-10. Первая из этих техник – тёплый старт, засев изначальной эволюционной популяции архитектурами вида трансформер вместо случайных моделей. Это помогает концентрировать поиски в заведомо сильной области поискового пространства, что позволяет нам быстрее находить лучшие модели.


Вторая техника – новый разработанный нами метод под названием прогрессивный динамический бег с препятствиями (Progressive Dynamic Hurdles, PDH). Этот алгоритм дополняет эволюционный поиск, позволяя выделять больше ресурсов наиболее сильным кандидатам, в отличие от предыдущих работ, где каждой модели-кандидату в NAS выделялось одинаковое количество ресурсов. PDH позволяет нам заканчивать оценку модели раньше, если она ужасно плохая, награждая при этом перспективные архитектуры большим количеством ресурсов.

Эволюционировавший трансформер

С использованием этих методов мы провели крупномасштабный поиск NAS на нашей задаче перевода и обнаружили ЭТ. Как и большинство архитектур нейросетей типа «последовательность за последовательностью» (sequence to sequence, seq2seq), у неё есть кодировщик, кодирующий входящую последовательность во вставки, и декодер, использующий эти вставки для создания выходной последовательности. В случае с переводом входная последовательность – это предложение на перевод, а выходная – перевод.


Самая интересная особенность ЭТ – свёрточные слои на дне модулей как кодировщика, так и декодера, добавленные схожим ветвящимся образом в оба эти места (то есть, входы проходят через два разных свёрточных слоя перед тем, как сложиться).


Сравнение архитектуры кодировщиков обычного трансформера и ЭТ. Обратите внимание ветвящуюся свёрточную структуру внизу модуля, независимо сформировавшуюся и в кодировщике, и в декодере. Декодер подробно описан в нашей работе.


Это особенно интересно, поскольку кодировщик и декодер во время NAS не делятся архитектурами друг с другом, и полезность этой архитектуры была открыта независимо и в кодировщике, и в декодере, что говорит в пользу такой схемы. Если оригинальный трансформер полностью полагался на применении внимания к тем же данным, что он сам и порождал [self-attention], ЭТ – это гибрид, пользующийся как преимуществами self-attention, так и широкой свёрточности [wide convolution].

Оценка ЭТ

Для проверки эффективности этой новой архитектуры мы сначала сравнили её с оригинальным трансформером, работавшим с задачей перевода с английского на немецкий, которую мы использовали во время поиска. Мы обнаружили, что у ЭТ лучшие показатели по BLEU и связности на всех размерах параметров, и наибольший выигрыш в размере сопоставим с мобильными устройствами (~7 млн параметров), что говорит об эффективном использовании параметров. На более крупных размерах ЭТ достигает передовых результатов на WMT’ 14 En-De с показателем BLEU в 29.8 и SacreBLEU в 29.2.


Сравнение ЭТ и оригинального трансформера на WMT’14 En-De при разных объёмах. Наибольшее преимущество достигается при небольших размерах, при этом ЭТ демонстрирует хорошие показатели и на более крупных размерах, опережая крупнейшего трансформера с количеством параметров, меньшим на 37,6% (сравнимые модели взяты в кружочки).


Для проверки обобщаемости мы сравнили ЭТ с трансформером на дополнительных задачах обработки естественного языка. Сначала мы проверяли переводы для разных пар языков, и обнаружили, что эффективность ЭТ выше, и его отрыв примерно соответствует тому, что был продемонстрирован на переводе English-German; и вновь, благодаря эффективному использованию параметров, самый большой отрыв наблюдается на моделях среднего размера. Мы также сравнили декодеры обеих моделей на моделировании языка в LM1B, и увидели значительное улучшение показателей по связности.



Планы на будущее

Эти результаты – первый шаг в изучении приложения поиска архитектур к последовательным моделям с прямым распространением. ЭТ распространяется в виде открытого кода в рамках проекта Tensor2Tensor, где его можно использовать на любых последовательных проблемах. Для улучшения воспроизводимости мы также открываем код поискового пространства, который мы использовали в нашем поиске, и Colab с реализацией PDH. Мы с нетерпением ждём результатов от исследовательского сообщества, вооружённого новыми моделями, и надеемся, что другие сумеют взять за основу эти новые техники поиска!    Tags: искусственный интеллектgoogleтрансформерэволюционный ии Hubs: Machine learningArtificial Intelligence          


