

Neurotic Bikes: генезис / Habr


              17  October  2018 at 17:13  Neurotic Bikes: генезис Machine learning *Artificial Intelligence  
        Sandbox
           На днях Youtube посчитал, что мне покажется интересным видео с названием «AI Learns to play Hill Climb Racing». Забавно, ведь за пару минут до этого я закоммитил очередные изменения в проект, где мы с коллегами в перерывах между работой и работой решаем именно эту задачу. Никакого «AI» в том видео, правда, не обнаружилось – автор поразвлекал публику баловством с Box2D и на том успокоился. Тем не менее, предлагаю считать этот факт убедительным доказательством актуальности темы и разобрать устройство нашей погремушки.


Коротко о задаче: транспортное средство – в нашем случае это то ли Чужой, то ли швейная машинка «Зингеръ» на колесах, назовем его просто «агент» – должно проехать по наперлинным одноименным шумом барханам от старта до финиша. Вот так выглядит агент в своей песочнице:




Агент, коснувшийся спиной трека или не демонстрирующий должного рвения в продвижении к цели, снимается с трассы.


Решать задачу будем с использованием нейронных сетей, но оптимизируемых генетическим алгоритмом (ГА) – такой процесс называют нейроэволюцией. Мы воспользовались методом NEAT (NeuroEvolution of Augmenting Topologies), изобретенным Кеннетом Стенли и Ристо Мииккулайненом в начале века [1]: во-первых, он хорошо зарекомендовал себя в важных для народного хозяйства проблемах, во-вторых, к началу работы над проектом у нас уже был свой фреймворк, реализующий NEAT. Так что, честно говоря, метод решения мы не выбирали – скорее, выбирали задачу, где можно погонять уже готовое.


На рисунке изображена примерная схема работы генетических алгоритмов:




Видно, что любой приличный ГА стартует с начальной популяции (популяция – набор потенциальных решений). Займемся ее созданием и заодно познакомимся с первым принципом NEAT. Согласно этому принципу все агенты стартовой популяции должны иметь простейшую, «минимальную» топологию нейронной сети. При чем тут топология? Дело в том, что в NEAT вместе с оптимизацией весов связей эволюционирует и архитектура сети. Это, между прочим, избавляет от необходимости ее проектирования под задачу. Идти от простых архитектур к сложным не просто логично, но и практично (меньше пространство поиска), поэтому и начинать надо с минимальной из возможных топологий – примерно так рассуждали авторы метода.


Для нашего и всех похожих случаев эта минимальная топология выводится из следующих соображений. Чтобы делать что-то осмысленное агенту нужно:


иметь данные об окружающей среде и своем состоянии,
обрабатывать эту информацию,
взаимодействовать со своим миром.


Первую роль выполняют сенсоры – нейроны входного слоя, на которые будем подавать полезную агенту информацию. Нейроны выходного слоя будут обрабатывать данные с сенсоров. За взаимодействие со средой отвечают актуаторы – устройства, выполняющие механические действия в ответ на сигнал со «своего» нейрона выходного слоя. Общий принцип построения начальной конфигурации, таким образом, следующий: определяемся с сенсорами и актуаторами, заводим по одному нейрону на актуатор, все сенсоры и еще один специальный нейрон — нейрон смещения (bias, о нем – ниже) соединяем связями со случайными весами со всеми нейронами выходного слоя. Как-то так:



b – bias, s – сенсоры, o – нейроны выходного слоя, a – актуаторы, n – кол-во сенсоров, k – кол-во актуаторов


А вот так выглядит минимальная НС для нашей задачи:




У нас всего один актуатор – это двигатель нашего колесного создания. Стрелять, прыгать и играть на дуде оно пока не умеет. На двигатель с единственного нейрона выходного слоя (его и слоем-то называть стыдновато) подается вот такое значение:



Здесь wb – значение веса связи, идущей от bias’а к выходному нейрону, умноженное на то, что «производит» всякий bias, т.е. +1, si – отнормированное к диапазону [0,1] значение на i-том сенсоре, wi – значение веса связи от i-го сенсора до выходного нейрона, а f – функция активации. 


В качестве функции активации мы используем вот эту фантазию на тему softsign:



– она продемонстрировала лучшую производительность в тестах одного небезызвестного в узких кругах нейроэволюциониста [2]. А сравнивать приятную глазу мягкость изгибов и симметричность графика этой функции с угловато-кособоким Leaky ReLU вообще смысла нет:




На этом рисунке показана реакция агента на разные значения функции активации. При близких к единице значениях двигатель крутит колеса по часовой стрелке, разгоняя агента вперед и сильно отклоняя корпус назад, так что слабоумные, но отважные быстро опрокидываются на спину и погибают. При значениях, близких к 0 – все наоборот, а при значении 0.5 мотор агента не работает.


На этом же рисунке показана роль нейрона смещения – вес связи, идущей от него к нейрону выходного слоя, отвечает, как следует из (1), за величину и направление смещения f(x) вдоль оси абсцисс. Пунктиром на рисунке изображен график функции активации при wb=-1. Получается, что даже при отсутствии сигнала на сенсорах, агент с такой связью будет довольно резво ехать назад: f(x)=f(-1+0)