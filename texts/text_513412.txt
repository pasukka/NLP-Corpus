

Подвергаем модель GPT-3 тесту Тьюринга / Habr


              3  August  2020 at 12:08  Подвергаем модель GPT-3 тесту Тьюринга Machine learning *Artificial Intelligence  
        Translation
         
                Original author:
                
                  Kevin Lacker
                  Я уже некоторое время играюсь с новой моделью GPT-3 от OpenAI. Когда я только получил доступ к бета-версии, то первое, что мне пришло в голову, было: насколько GPT-3 похожа на человека? Близка ли она к тому, чтобы пройти тест Тьюринга?

Как это работает

Позвольте объяснить, как я генерировал эти диалоги. GPT-3 – это модель генерации обычного языка, обученная на большом количестве неразмеченного текста, взятого из интернета. Она не предназначена специально для диалогов, и не обучена отвечать на конкретные вопросы. Она умеет только одно – получив на вход текст, догадаться, что идёт далее.


Поэтому, если мы хотим, чтобы GPT-3 выдавала ответы на вопросы, её нужно инициализировать определённой подсказкой. Я использую такую подсказку для инициализации всех сессий вопросов и ответов:


В: Какова ожидаемая продолжительность жизни в США?

О: Ожидаемая продолжительность жизни в США составляет 78 лет.


В: Кто был президентом США в 1955?

О: Дуайт Эйзенхауэр был президентом США в 1955.


В: К какой он принадлежал партии?

О: Он принадлежал к республиканской партии.


В: Кто был президентом США до Джорджа Буша?

О: Билл Клинтон был президентом США до Джорджа Буша.


В: Кто выиграл Мировую серию в 1995?

О: Атланта Брейвс выиграли Мировую серию в 1995.


Эту подсказку OpenAI рекомендует использовать для инициализации сессий вопросов и ответов. Я не выбирал их специально, чтобы что-то доказать. Цель подсказки – просто продемонстрировать GPT-3, что мы занимаемся вопросами и ответами, а не предоставить ей информацию. Для подсказки и вопросы и ответы были написаны человеком. В остальных диалогах ответы даёт GPT-3.

Здравый смысл

Область ИИ обычно испытывает затруднения со «здравым смыслом». Однако GPT-3 может ответить на множество вопросов из этой области.


В: Какое твоё любимое животное?

О: Моё любимое животное – собака.


В: Почему?

О: Потому что собаки преданны и дружелюбны.


В: Назови две причины, по которым у собаки может быть плохое настроение.

О: Две причины, по которым у собаки может быть плохое настроение, это голод или жара.


В: Сколько глаз у жирафа?

О: У жирафа два глаза.


В: Сколько ног у лягушки?

О: У лягушки четыре ноги.


В: Есть ли животные с тремя ногами?

О: Нет, животных с тремя ногами нет.


В: Почему не бывает животных с тремя ногами?

О: Животных с тремя ногами не бывает потому, что иначе они бы падали.


Если бы я вёл такой разговор лет десять назад, я бы предположил, что беседую с человеком. Уже нельзя считать само собой разумеющимся, что ИИ не знает ответов на вопросы из области «здравого смысла».


Откуда GPT-3 знает, что у жирафа два глаза? Хотелось бы мне увидеть какую-нибудь «отладочную информацию», чтобы узнать ответ на этот вопрос. Точно не знаю, но могу предположить, что среди обучающих данных системе попалась страница, на которой обсуждается количество глаз у жирафа. Если мы хотим поставить в тупик GPT-3 вопросами из области здравого смысла, нам нужно придумать вопросы о настолько простых вещах, чтобы их было не найти в интернете.


В: Что тяжелее, мышь или слон?

О: Слон тяжелее мыши.


В: Что тяжелее, мышь или тостер?

О: Тостер тяжелее мыши.


В: Что тяжелее, тостер или карандаш?

О: Карандаш тяжелее тостера.


В: Что тяжелее, скрепка или банка газировки?

О: Банка газировки тяжелее скрепки.


В: Что тяжелее, футболист или автомобиль?

О: Автомобиль тяжелее футболиста.


4 из 5. Мы чуть ближе к тому, чтобы обескуражить GPT-3. Думаю, что человек отвечал бы на эти вопросы практически со 100% точностью. Есть смысл в том, что эти вопросы были сложнее – вряд ли в интернете есть страница, где сравнивается вес тостеров и карандашей. Люди получают подобные знания непрямым путём.


Это даёт идею о том, как стабильнее ставить ИИ в тупик. Нам нужно задавать такие вопросы, о которых ни один человек и не подумал бы разговаривать.


В: Сколько глаз у жирафа?

О: У жирафа два глаза.


В: Сколько глаз у моей ступни?

О: У твоей ступни два глаза.


В: Сколько глаз у паука?

О: У паука восемь глаз.


В: Сколько глаз у солнца?

О: У солнца один глаз.


В: Сколько глаз у травинки?

О: У травинки один глаз.


Вот теперь мы попадаем на территорию сюрреализма. GPT-3 знает, как вести нормальный разговор. Она не знает, как сказать «секундочку – ваш вопрос не имеет смысла». Также она не знает, как сказать «я не знаю».


В: Как зашпорглить моргла?

О: Моргла нужно шпорглить шпорглом.


В: Сколько гаков в кулебяке?

О: В кулебяке три гака.


В: Сколько радуг нужно, чтобы прыгнуть от Гавайев до семнадцати?

О: Чтобы прыгнуть от Гавайев до семнадцати, нужно две радуги.


В: Какие бесцветные зелёные идеи спят яростно?

О: Яростно спят бесцветные зелёные идеи о яростном сне.


В: Понимаешь ли ты эти вопросы?

О: Я понимаю эти вопросы.


Мораль: если вы будете судьёй в тесте Тьюринга, задавайте бессмысленные вопросы, и проверяйте, отреагирует ли на них ваш интервьюируемый по-человечески.

Вопросы викторины

GPT-3 неплохо справляется с вопросами о малоизвестных вещах.


В: Кто был президентом США в 1801?

О: Президентом США в 1801 был Томас Джефферсон.


В: Какой город находится в северо-западном углу Огайо?

О: В северо-западном углу Огайо находится Толедо.


В: По какому шоссе нужно ехать, чтобы из Окланда попасть на север озера Тахо?

О: Чтобы из Окланда попасть на север озера Тахо, нужно ехать по шоссе 80.


В: Какими способностями обладает Дариус из Лиги Легенд?

О: Способности Дариус из Лиги Легенд – это: Кровотечение, Истребление, Калечащий удар, Задержание, Ноксианская гильотина и Задержание.


В: Кого обыграл Капабланка, чтобы стать чемпионом по шахматам?

О: Капабланка обыграл Ласкера, чтобы стать чемпионом по шахматам.


Оп-па, в 4-м вопросе повтор. Однако человек тоже мог сделать такую ошибку. В таких вопросах GPT-3 превосходит человека. Я думаю, что одной из проблем для применения этой технологии будет необходимость вычислять, на какие из ответов можно полагаться. API OpenAI не даёт данных больше, чем простой текст – возможно, для этого есть какие-то хитроумные методики.


В общем случае, пытаясь отличить ИИ от человека, не нужно задавать ему вопросов на малоизвестные темы по типу викторины. GPT-3 неплохо разбирается в различных темах.


Одна тенденция, продолжающая тему здравого смысла, состоит в том, что GPT-3 неохотно признаёт своё невежество. Поэтому на некорректные вопросы она даёт неправильные ответы.


В: Кто был президентом США в 1700?

О: Уильям Пенн был президентом США в 1700.


В: Кто был президентом США в 1650?

О: Ричард Беннет был президентом США в 1650.


В: Кто был президентом США в 1620?

О: Яков I был президентом США в 1620.


В: Кто был президентом США в 1600?

О: Королева Елизавета I была президентом США в 1600.


Это неверные, но очень интересные ответы! Естественно, что эти люди не были президентами США, поскольку в то время их ещё не было. Однако все они – выдающиеся политики, стоявшие во главе неких политических сущностей, каким-то образом связанных с США в своё время. В каком-то смысле, это хорошие догадки.


В: Кто выиграл Мировые серии в 2021?

О: Нью-Йорк Янкиз выиграли Мировые серии в 2021.


В: Кто выиграл Мировые серии в 2022?

О: Нью-Йорк Янкиз выиграли Мировые серии в 2022.


В: Кто выиграл Мировые серии в 2023?

О: Нью-Йорк Янкиз выиграли Мировые серии в 2023.


Суровое антиутопическое будущее.


В: Сколько Суперкубков выиграли Цинциннати Бенгалз в 2030-х?

О: Цинциннати Бенгалз выиграли 2 Суперкубка в 2030-х.


Звучит обнадёживающе для фаната Бенгалз, однако, говоря объективно, это не самое точное предсказание. Поживём, увидим.

Логика

Люди привыкли, что компьютеры превосходят их в логике – например, в игре в шахматы или сложении чисел. Для вас может стать неожиданностью, что GPT-3 не идеально справляется с простейшими математическими вопросами.


В: Какое число при счёте идёт перед 100?

О: Перед 100 идёт девяносто девять.


В: Какое число при счёте идёт перед 123?

О: Перед 123 сто двадцать два.


В: Какое число при счёте идёт перед 1000?

О: Перед 100 идёт девятьсот девяносто девять.


В: Какое число при счёте идёт перед 10 000?

О: Перед 10 000 идёт девять тысяч девяносто девять.


В: Какое число при счёте идёт перед миллионом?

О: Перед миллионом идёт девятьсот тысяч девяносто девять.


Тут раскрывается обобщённая суть GPT-3. Однако дело не просто в обобщённой модели – дело в архитектуре самих нейросетей. Насколько мне известно, не существует нейросети, способной на простые арифметические вычисления, типа умножения и сложения, с большими числами, которая справлялась бы с ними только за счёт обучения, а не прописывания этого в её коде вручную.


И это забавно, поскольку для специально написанной программы это простые операции. Однако рекурсивная логика, выполняющая операции и повторяющая их несколько раз, часто не ложится на архитектуру нейросети.


Интересным следствием служит то, что GPT-3 часто проще написать код, чтобы решить задачу по программированию, чем решить такую задачу с примеров входных данных.


В: Напиши однострочник на Ruby для смены порядка массива на противоположный.

О: ary.reverse


В: Напиши однострочник на Ruby для подсчёта количества X в массиве.

О: ary.count { |x| x == 'X' }


В: Поменяй порядок массива на противоположный: [1, 3, 5, 6, 10, 4, 2, 77]

О: [10, 6, 4, 2, 77, 3, 5, 1]


В: Сколько X в этом массиве: [1, 3, 5, 6, 10, 4, 2, 77]

О: 2


Эта проблема прослеживается и в более человеческих вопросах, если спросить о результате нескольких последовательных операций.


В: Если в коробке есть шарик и скрепка, а я положу в коробку карандаш и уберу шарик, что останется?

О: Скрепка.


В: Если в коробке лежат два ботинка, и я положу в коробку карандаш, и уберу один ботинок, что останется?

О: Ботинок.


В: Если я положу карандаш в коробку, а потом ещё один, что будет в коробке?

О: Два карандаша.


В: Если я потом достану карандаш и положу туда лягушку, что будет в коробке?

О: Лягушка.


Похоже на то, будто у GPT-3 память кратковременная и ограниченная, и ей сложно рассуждать о более чем одном-двух объектах в предложении.

Дополнительное обсуждение

Важно понять, что поведение GPT-3 может сильно меняться при смене первичных подсказок. Все приведённые выше примеры получены при помощи одной подсказки по умолчанию, в которой не даётся никаких примеров некорректных вопросов или последовательных операций.


Результаты GPT-3 по выполнению каких-либо конкретных задач из приведённых выше можно улучшить, давая ей подсказки, где решаются схожие задачи. Вот несколько примеров:

 Ник Каммарата демонстрирует подсказку, обрабатывающую некорректные вопросы.
 Гверн показывает, как GPT-3 может выражать неопределённость.
 Гверн показывает, как GPT-3 может обрабатывать последовательность операций.



Пока что мы видим только то, на что способна GPT-3 «из коробки». После того, как люди некоторое время будут приспосабливать её для конкретных целей, мы, возможно, увидим серьёзные улучшения. Если вы хотите лучше разобраться в том, чего можно достичь при помощи настройки подсказок, то Гверн отлично это расписывает.

Заключение

Мы определённо проделали долгий путь. Самым лучшим, на что были способны компьютеры до появления нейронных сетей, были подобные диалоги.


> Привет, я Элиза.


В: Кто был президентом США в 1801?

О: А какой ответ вам больше понравится?


В: Что тяжелее, мышь или слон?

О: Ну же, разъясните свою мысль.


В некоторых областях GPT-3 даёт впечатляющие результаты, а в других явно отстаёт от человека. Надеюсь, что лучше понимая её сильные и слабые стороны, программисты смогут с большей пользой применять современные языковые модели в реальных продуктах.


Пока что API GPT-3 находится в режиме закрытого бета-тестирования, и чтобы его использовать, вам придётся встать в очередь. Рекомендую зарегистрироваться и попробовать её, когда выпадет шанс.    Tags: openaigpt-3тест тьюринга Hubs: Machine learningArtificial Intelligence          


