

Визуальное RPG с долговременной памятью, генерируемое из 3 нейросетей и LLamы / Habr


                Визуальное RPG с долговременной памятью, генерируемое из 3 нейросетей и LLamы Level of difficulty  
    Easy
   Reading time  
    17 min
   Views  7.5K RUVDS.com corporate blog Python *Machine learning *Artificial Intelligence Natural Language Processing * 
    Tutorial
        

Языковые модели (NLP) сейчас активно развиваются и находят себе всё больше интересных применений. Начиналась же их эпоха с классики жанра — D&D. Это настольная игра, где несколько друзей или просто знакомых синхронно галлюцинируют, представляя себя командой героев в некоем вымышленном мире. Прав же во внутриигровых выборах тот, кто выкинул большее число на игральной кости. Судить сейчас об их мотивации у меня нет никакого желания, да и статья вообще-то не об этом.


Важно только понимать, что движущей силой сюжета в их сессиях является лишь один из игроков, называемый Dungeon Master. Когда только начали появляться первые GPT-модели, одной из первых хотелок гиков оказалось желание сварить из нейросетей автоматического Dungeon Masterа.


Так и появился AIDungeon — уникальная для своего времени (2019 год) вещь, которая не сильно потеряла в популярности и по сей день. Однако, если вы любите смотреть глубже, то играть в него вам быстро надоест. Я же в своей серии из нескольких статей (посвящённых GPT) стараюсь показать простому обывателю механизм безболезненного использования нейросетевых моделей в простых проектах при помощи Python и Hugging Face Transformers.


Фактически, статья является логическим продолжением прошлой: «Реально Бесконечное (лето) RuGPT3.5: Генерация новеллы на ходу нейросетью». Связано это с тем, что в ней были получены достаточно спорные результаты: с одной стороны, она оставила после себя уникальный легаси для тренировки квантованных лор, с другой же, итоговый результат оказался едва ли удовлетворителен. Да и «новелла» получилась совершенно не визуальной. Идея генерации сценария для RenPY в свою очередь и вовсе оказалась мертворождённой.


Тут я хотел бы учесть совершённые в прошлый раз ошибки и… переработать всё заново.

Оглавление

 Задумка
 Модели
 Схема работы

 Переводчик
 Логика
 Интерфейс
 Тесты
 Заключение

Задумка

В комментариях и моих личных диалогах накопился приличный фидбек по статье о модели Сбера. Некоторые из этих отзывов даже несли в себе неплохие соображения и, что немаловажно, предложения.


Src: раз, два и картиночка, если лень ходить по ссылкам:




Сразу оговорюсь, что далее в этой статье вы увидите только мою пересборку велосипеда, а не уникальную и инновационную научную работу. Вышеупомянутый Silly Tavern, KoboldAI, AIDungeon, Endless VN — и это лишь самые распространённые из примеров, в которых описанная далее мной идея отшлифована до состояния идеала и уже давно монетизируется при помощи платных подписок.


Эта статья, соответственно, является развёрнутым комментарием и proof of concept того, как можно обклеить GPT-модель саппортами, которые будут управлять её поведением.


Своей задачей я ставлю написание максимально простого для понимания кода, который будет по максимуму использовать высокоуровневые сущности. Рассчитываю, что суть будет полностью понятна даже тем, кто абсолютно не знает, что же находится под капотом.


Пример работы результата
Ввод: Рин Тосака — маг-тсундере,

Вывод: Рин Тосака — маг-цундере, 17 лет, ученица 2-го класса средней школы и участница войны за Святой Грааль. Носит красную рубашку и синюю юбку. Она является членом семьи Тосака, одной из трех великих семей города. Ее волосы собраны в два хвостика, а глаза карие.


Иллюстрации получились немного откровенными, так что пусть лучше будут скрыты по умолчанию.








Если вам интересно, как получить такой результат в 100 строк кода, то переходим сразу к делу.

Models

Первым делом определимся с набором моделей (нейросетей), которые нам могут понадобиться:


LLama-2 (HF meta) — основа нашего проекта. Англоязычная модель, 7B версия — 13гб, 13B версия — 20+гб. Я буду использовать GPTQ версию от TheBloke на 13B, которая, будучи квантованной, весит 10гб. Итого: TheBloke/Llama-2-13B-chat-GPTQ.
Summary (HF knkarthick) — говоря простыми словами, превращает много текста в мало текста, но сохраняя смысл. Весит 1.6гб.
Semantic similarity (HF Sakil) — сравнивает тексты, считая синусное расстояние между их многомерными векторными представлениями. Простыми словами, говорит, насколько две строки похожи по смыслу друг на друга от 0 до 1. Весит всего 260мб.
Counterfeit V3 (CivitAI) — пока все ждут нормальные файн-тюны Stable Diffusion XL, я использую проверенный временем чекпоинт Stable Diffusion 1.5 под названием Counterfeit, который высококачественно рисует в аниме стиле. Весит 4гб, и запускать я его буду в отдельном блокноте (для распределения нагрузки) с auto1111 webui, обращаясь к fastAPI. Если вы не знаете, как бесплатно и без блокировок запускать webui в google colab, то вам сюда: «Запуск блокнотов, запрещённых Google Colab TOS или SD webui в колабе без ограничений» (с момента её написания произошли ещё небольшие поправки в методе, но об этом позже).

Concepts

Теперь продемонстрирую своё видение схемы работы нашего пайпа на высокоуровневом языке моделирования Paint 3D:




Одна итерация на пальцах:


Есть некое окно контекста, назовём его ruX. Оно находится в текстовом поле перед пользователем и свободно редактируется им.
Также есть массив строк «воспоминаний» [A,B,C].
Пользователь жмёт кнопку «Генерировать».
ruX переводится в enX обычным переводчиком.
Дописываем к enX в начало самые связанные с ним «воспоминания» (сортируем массив воспоминаний по семантическому сходству с самим enX и берём топ этого массива), получаем A+B+C+enX. Таким образом, мы допишем некоторое количество выжимок из прошлых контекстов, в которых будет максимально много схожих с текущим контекстом определений и идей.
Передаём A+B+C+enX в LLama, которая дописывает продолжение, получаем A+B+C+enX+N.
Убираем A+B+C, ведь они нужны были только для правильного направления ламы.
Переводим enX+N обратно на русский и отдаём пользователю как ruY.
Раз в несколько таких циклов или по желанию юзера сжимаем текущий enY при помощи суммаризатора в M и добавляем в массив воспоминаний. Теперь он такой: [A,B,C,M].
Если пользователь того хочет, отправляем M в auto1111 webui API, который запущен в соседнем блокноте и получаем в ответ пикчу.

То, что для генерации нормальных артов обязательно условное перечисление тегов danbooru  — это не совсем правда. Stable Diffusion спокойно абстрагируется от контекста и кушает обычные тексты без явных отличий в качестве результата от чистых тегов.

 На самом деле, если бы целью этой работы я ставил написание продукта для максимального опыта от использования, то стоило бы дообучить специально для связки с SD используемый мной MEETING SUMMARY на наборе данных, где описание сцены и визуала текста, поданного на вход, было приоритетнее вычленения смысла и событий. Но мы постараемся обойтись без хардкора и файн-тюна. 

Переводить нужно будет много
Google Translator не осиливает художественные тексты. Они получаются слишком буквальными и ненатуральными. Так ещё и один перевод ограничен лимитом символов. У Яндекса проблем с фигуральными выражениями нет, да и с большими объёмами текста он тоже справляется. Вот только с api у него серьёзно больше гемора, так что я использую библиотеку, которую я вам настоятельно не рекомендую абьюзить коммерчески.


Распинаться долго не буду. Вот вам репка: https://github.com/alekssamos/yandexfreetranslate.


Вот установка:

!python -m pip install git+https://github.com/alekssamos/yandexfreetranslate.git
!python -m pip install yandexfreetranslate

Всё, что нам нужно для быстрого перевода больших текстов:

from yandexfreetranslate import YandexFreeTranslate
yt = YandexFreeTranslate(api='ios') #Работает только так
def ru(txt):
  return yt.translate("en", "ru", txt)
def en(txt):
  return yt.translate("ru", "en", txt)
Логика

Далее переходим к написанию основного тела программы. Нам понадобятся не только transformers, но и их братец sentence_transformers. Auto_gptq, если вы, как и я сейчас, собираетесь использовать квантованную модель. Само собой, если я не акцентирую на установке какой-либо библиотеки внимания, то она должна корректно вставать при помощи pip и не будет использоваться в специфичных кейсах, где была бы принципиальна её точная версия. 


from sentence_transformers import SentenceTransformer, util
import torch
from transformers import pipeline
from transformers import AutoTokenizer, pipeline, logging
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
model_name_or_path = "TheBloke/Llama-2-13B-chat-GPTQ"
simm = SentenceTransformer('Sakil/sentence_similarity_semantic_search')
summarizer = pipeline("summarization", model="knkarthick/MEETING_SUMMARY")
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)
use_triton = False
model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,
        model_basename="model",
        use_safetensors=True,
        trust_remote_code=True,
        device="cuda:0",
        use_triton=use_triton,
        quantize_config=None)


Тут мы импортируем упомянутые выше библиотеки и скачиваем объёмные модели с Hugging Face.


Ах, да. Совсем забыл — я собирался идти по пути наименьшего сопротивления, так что раздельные объекты «модель» и «токенизатор» нам ни к чему. Создаём объект pipeline, который совместит в себе их оба.

gpt = pipeline("text-generation",
              model=model,
              tokenizer=tokenizer,          # Так-то лучше
              torch_dtype=torch.float16,
              )

Так-то лучше. Теперь заведём глобальный список blocks, в него будем кидать «воспоминания».

blocks=[]
Пишем обёртки
