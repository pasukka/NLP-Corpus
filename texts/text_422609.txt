

Языковой барьер и NLP. Почему чат-боты нас не понимают? / Habr


              18  September  2018 at 18:11  Языковой барьер и NLP. Почему чат-боты нас не понимают? Binary District corporate blog Machine learning *Artificial Intelligence       Люди давно хотели научить машину понимать человека. Однако только сейчас мы немного приблизились к сюжетам фантастических фильмов: можем попросить Алису убавить громкость, Google Assistant — заказать такси или Siri — завести будильник. Технологии языкового процессинга востребованы в разработках, связанных с построением искусственного интеллекта: в поисковых системах, для извлечения фактов, оценки тональности текста, машинного перевода и диалога.



О двух последних направлениях мы и поговорим: они имеют богатую историю и оказали значительное влияние на языковой процессинг. Кроме того, разберемся с основными возможностями обработки естественного языка при создании чат-бота вместе со спикером нашего курса AI Weekend компьютерным лингвистом Анной Власовой.

С чего все начиналось?

Первые разговоры об обработке естественного языка компьютером начались еще в 30-е годы XX-го века с философских рассуждений Айера — он предлагал отличать разумного человека от глупой машины с помощью эмпирического теста. В 1950 году Алан Тьюринг в философском журнале Mind предложил такой тест, где судья должен определить, с кем он ведет диалог: с человеком или компьютером. С помощью теста задали критерии оценки работы искусственного интеллекта, возможность его построения не подвергали сомнению. Тест имеет множество ограничений и недостатков, но он оказал значительное влияние на развитие чат-ботов.

Первой областью, где успешно применили языковой процессинг, стал машинный перевод. В 1954 году Джорджтаунский университет совместно с компанией IBM продемонстрировали программу машинного перевода с русского на английский, которая работала на базе словаря из 250 слов и набора из 6 грамматических правил. Программа была далека от того, что действительно можно назвать машинным переводом, и на демонстрации перевела 49 заранее отобранных предложений. До середины 60-х было предпринято множество попыток создать полноценно работающую программу-переводчик, но в 1966 году Консультативная комиссия по автоматической обработке языка (англ. ALPAC) объявила машинный перевод бесперспективным направлением. Государственные дотации на какое-то время прекратились, общественный интерес к машинному переводу снизился, однако исследования на этом не остановились.




Параллельно с попытками научить компьютер переводить текст, ученые и целые университеты думали над созданием робота, способного имитировать речевое поведение человека. Первой успешной реализацией чат-бота стал виртуальный собеседник ELIZA, написанный в 1966 году Джозефом Вейценбаумом. Элиза пародировала поведение психотерапевта, выделяя значимые слова из фразы собеседника и задавая встречный вопрос. Можно считать, что это был первый чат-бот, построенный на правилах (rule-based bot), и он положил начало целому классу таких систем. Без Элизы не появились бы такие программы-собеседники, как Cleverbot, WeChat Xiaoice, Eugene Goostman — формально прошедший тест Тьюринга в 2014 году, — и даже Siri, Jarvis и Alexa. 

В 1968 году Терри Виноградом на языке LISP была разработана программа SHRDLU. Она по команде перемещала простые объекты: конусы, кубики, шары и могла поддерживать контекст — понимала, какой именно элемент надо переместить, если о нем говорилось ранее. Следующим шагом в развитии чат-ботов стала программа A.L.I.C.E., для которой Ричард Уоллес разработал специальный язык разметки — AIML (англ. Artificial Intelligence Markup Language). Тогда, в 1995 году, ожидания от чат-бота были завышены: думали, что ALICE будет даже умнее человека. Разумеется, быть умнее у чат-бота не получилось, и на какое-то время бизнес в чат-ботах разочаровался, а инвесторы еще долго обходили тему виртуальных помощников стороной.

Язык имеет значение

Сегодня чат-боты по-прежнему работают на основе набора правил и сценариев поведения, однако естественный язык нечеткий и неоднозначный, одна мысль может иметь много способов изложения, поэтому коммерческий успех диалоговых систем зависит от решения задач языкового процессинга. Машину нужно научить четко классифицировать все разнообразие входящих вопросов и четко их интерпретировать. 

Все языки устроены по-разному, и это имеет большое значение для парсинга. С точки зрения морфологического состава значимые элементы слова могут присоединяться к корню последовательно, как, например, в тюркских языках, а могут разбивать корень, как в арабском и иврите. С точки зрения синтаксиса одни языки допускают свободный порядок слов во фразе, а другие организованы более жестко. В классических системах порядок слов играет существенную роль. Для современных статистических методов NLP он не имеет такого значения, поскольку обработка происходит не на уровне слов, а целых предложений. 

Другие сложности при разработке чат-ботов возникают в связи с развитием мультиязыковой коммуникации. Сейчас люди часто общаются не на родных языках, используют слова неправильно. Например, во фразе «I have shipped two days ago, but goods didn’t come» с точки зрения лексики речь должна идти о доставке физических объектов, например, товаров, а не об электронной денежной транзакции, которую описывает этими словами человек, говорящий не на родном языке. Но в реальном общении человек поймет собеседника верно, а у чат-бота могут возникнуть проблемы. В определенных темах, как например инвестиции, банкинг или IT, люди часто переходят на другие языки. Но чат-бот вряд ли поймет, о чем идет речь, поскольку с большой вероятностью обучен на одном языке.

История успеха: машинные переводчики

До появления голосовых помощников и масштабного распространения чат-ботов наиболее востребованной интеллектуальной задачей, где требовалась обработка естественного языка, был машинный перевод. Разговоры о нейронных сетях и глубоком обучении ходили уже в 90-е годы, а первый нейрокомпьютер «Марк-1» появился вообще в 1958 году. Но повсеместно применять их не было возможности из-за низкой производительности ЭВМ и отсутствия достаточных по объему языковых корпусов. Только крупные научные коллективы могли себе позволить заниматься исследованиями в области нейронных сетей.

Машинные переводчики в середине XX века были далеки от Google Translate и Яндекс.Переводчика, но с каждым новым методом перевода появлялись идеи, которые применяются в том или ином виде даже сегодня.
1970 г. Машинный перевод на основе правил (англ. RBMT) был первой попыткой научить машину переводить. Перевод получался как у пятиклассника со словарем, но в том или ином виде правила для машинного переводчика или чат-бота используют и сейчас. 
1984 г. Машинный перевод на основе примеров (англ. EBMT) был способен переводить даже совсем не похожие друг на друга языки, где задавать какие-то правила было бесполезно. Все современные машинные переводчики и чат-боты используют готовые примеры и паттерны.
1990 г. Статистический машинный перевод (англ. SMT) в эпоху развития интернета позволил использовать не только готовые языковые корпуса, но даже книги и вольно переведенные статьи. Большее количество имеющихся данных повышало качество перевода. Статистические методы и сейчас активно используются в языковом процессинге.

Нейронные сети на службе у NLP

По мере развития обработки естественного языка множество задач решалось классическими статистическими методами и множеством правил, однако проблему нечеткости и неоднозначности в языке это не решало. Если мы скажем «лук» без какого-либо контекста, то даже живой собеседник вряд ли поймет, о чем идет речь. Семантику слова в тексте определяют слова-соседи. Но как объяснить это машине, если она понимает только числовое представление? Так родился статистический метод анализа текста word2vec (англ. Word to vector).


Векторы лук_1 и лук_2 параллельны, следовательно это одно слово, а лук_3 — омоним.


Идея вполне очевидна из названия: представить слово в виде вектора с координатами (x1, x2,...,xn). Для борьбы с омонимией одинаковым словам присоединяется тег: «лук_1», «лук_2» и так далее. Если векторы «лук_n» и «лук_m» параллельны, то их можно считать одним словом. В ином случае эти слова — омонимы. На выходе у каждого слова появляется свое векторное представление в многомерном пространстве (размерность векторного пространства может варьироваться от 50 до 1000).




Остается открытым вопрос, какой тип нейросети использовать для обучения условного чат-бота. В человеческой речи важна последовательность: мы делаем какие-то выводы и принимаем решение с учетом того, о чем говорилось в предыдущем предложении или даже абзаце. Под эти критерии отлично подходит рекуррентная нейронная сеть (RNN), однако по мере увеличения расстояния между связанными частями текста необходимо увеличивать и размер RNN, из-за чего падает качество обработки информации. Эту проблему решает сеть LSTM (англ. Long short-term memory). Она обладает одной важной особенностью — состояние ячейки, которое может оставаться постоянным, либо меняться, если в этом есть необходимость. Таким образом информация в цепочке не теряется, что критически важно для обработки естественного языка.

На сегодняшний день есть огромное множество библиотек для обработки естественного языка. Если говорить о языке Python, который часто используется для анализа данных, то это NLTK и Spacy. Крупные компании также принимают участие в разработке библиотек для NLP, как например NLP Architect от Intel или PyTorch от исследователей из Facebook и Uber. Несмотря на такую большую заинтересованность в нейросетевых методах обработки языка со стороны крупных компаний, связные диалоги строятся в основном на основе классических методов, а нейросеть играет вспомогательную роль, решая задачи предварительной обработки речи и классификации.

Как можно применять NLP в бизнесе?

Самой очевидной сферой применения обработки естественного языка можно назвать машинные переводчики, чат-боты и голосовые ассистенты — то, с чем мы сталкиваемся каждый день. Большую часть сотрудников колл-центра можно заменить на виртуальных помощников, поскольку около 80% обращений клиентов в банки касаются довольно типичных вопросов. Чат-бот также спокойно справится с первичным собеседованием кандидата и запишет его на «живую» встречу. Как ни странно, юриспруденция достаточно точное направление, поэтому даже здесь чат-бот может стать успешным консультантом.




Направление b2c не единственное, где можно применять чат-ботов. В крупных компаниях ротация сотрудников происходит достаточно активно, поэтому каждому приходится помогать в адаптации к новой среде. Поскольку вопросы нового сотрудника довольно типичны, весь процесс легко автоматизируется. Нет необходимости искать человека, который объяснит, как заправить принтер, к кому обратиться по какому-то вопросу. Внутренний чат-бот компании отлично с этим справится.


С помощью NLP можно с высокой точностью измерять удовлетворенность пользователей новым продуктом, анализируя отзывы в интернете. Если программа определила отзыв как отрицательный, то репорт автоматически отправляется в соответствующий отдел, где с ним уже работают живые люди.


Возможности языкового процессинга будут только расширяться, а вместе с ними и сферы его применения. Если в колл-центре вашей компании работает 40 человек, стоит задуматься: может, лучше заменить их на команду программистов, которые сколотят вам чат-бота?


Больше о возможностях языкового процессинга вы сможете узнать на нашем курсе AI Weekend, где в рамках темы искусственного интеллекта Анна Власова подробно расскажет о чат-ботах.    Tags: binary districtNLPобработка естественного языкачат-ботыискусственный интеллектмашинный перевод Hubs: Binary District corporate blogMachine learningArtificial Intelligence          


