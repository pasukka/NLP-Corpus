

Тестируем логику ChatGPT на простых задачах / Habr


               Тестируем логику ChatGPT на простых задачах Level of difficulty  
    Easy
   Reading time  
    6 min
   Views  19K Artificial Intelligence       В новостях технологий мы часто видим заметки об успехах ChatGPT. Например о том, что умный бот успешно сдал экзамены в очередной ВУЗ. Или о том, что многие скоро останутся без работы, поскольку их заменит генеративная система на базе большой языковой модели. Наверняка у многих возникало желание потестировать возможности ChatGPT и выяснить действительно ли он так умён, как об этом пишут в прессе. Если у вас тоже было такое желание, эта заметка для вас.Главный вопрос - как собственно тестировать? Какой бы набор вопросов мы не взяли - это будет максимально субъективная оценка. Ведь другой тестировщик взял бы другие вопросы - интересные лично ему и получил бы иные результаты. Держим в уме эту субъективность и делаем на неё скидку. Как программисту, мне, конечно, интереснее как ChatGPT работает с логикой и насколько точно “понимает” заданный вопрос. Также для теста нужны вопросы, ответы на которые можно оценить однозначно и формально. Для примера, просьба написать стих или эссе - не подходят для поставленных целей. Самым логичным показалось взять набор школьных задач и оценить процент успешных решений. Это похоже на какую-то метрику. Задачи подобраны из интернета, критерий выбора: чтобы формулировки были не очень длинные и чётко сформулированные. Вот пример такой задачи:Имеется 16 конфет. Как разделить их между Колей и Петей, чтобы у Коли было на две конфеты больше, чем у Пети?Остальные задачи такие же простые. Полный список задач будет приведён в конце. Учитывая, что в новостях ChatGPT “успешно сдаёт ВУЗовские экзамены”, логично ожидать, что простые школьные задачи он решит с вероятностью близкой к 100%. Ещё несколько моментов:Поскольку в ТГ-каналах посвящённых AI пишут, что лучше задавать вопросы на английском (мол, так бот даёт ответы точнее), то задачи задавал именно на английском. Кстати, позже сравнив результаты не увидел разницы в качестве ответов (если задавать вопросы на русском).Поскольку в каналах посвящённых AI пишут, что на качество ответа сильно влияет “промптинг”, то перед каждой задачей добавлялся следующий префикс “Реши математическую задачу. Давай думать по шагам. Задача:”. В результате ChatGPT действительно рассуждает по шагам и объясняет результат. Хотя, он часто и без такого префикса рассуждает по шагам. Пробовал и другие промпты, но разницы в качестве ответов не заметил.Ещё один момент, о котором ни разу не читал в каналах посвящённых AI. Если одну и ту же задачу задавать несколько раз (очищая контекст), то часто ответы оказываются разными. То есть, один раз бот ответит “1.5 землекопа”, а в следующий раз уже отвечает “2.5 землекопа”. Поэтому для оценки недостаточно задать вопрос один раз - пришлось каждую задачу задавать по несколько раз (меня хватило на 10 попыток для 10 задач) и считать процент корректных ответов. Предыдущий пункт имеет ещё одно важное следствие. Если вам показывают скриншот с ответом GPT, чтобы доказать какой он умный (или наоборот глупый), то не известно сколько попыток сделал автор, чтобы добиться такого хорошего (или наоборот плохого) ответа. Но даже если автор получил такой ответ с первой попытки, это не означает, что данный ответ является типичным, а не редким выбросом статистики. Поэтому вряд ли стоит расценивать приводимые кем-то примеры, как убедительное доказательство чего-либо. Это, кстати, относится не только к GPT.Ещё один момент: понятно, что 10 попыток - очень мало для серьёзной статистики. Если сделать ещё 10 попыток, то результат может сильно отличаться. Но у нас и не серьёзное исследование, а всего лишь “быстрый тест на коленке”. На большее он не претендует.Анализируя ошибочные ответы сети я корректировал формулировку некоторых задач. Для примера, в задаче о количестве партий в шашки потребовалось добавить такое уточнение “Каждую партию в шашки играют два человека”. Процент корректных ответов сразу увеличился. Интересно, что не до 100% (ChatGPT всё равно иногда игнорировал это уточнение), но процент стал заметно выше. Наверное, добавление такой подсказки можно посчитать нечестным подгоном под ответ, но пусть это будет небольшим авансом для ChatGPT.Кстати, интересная деталь. Если задать отдельный вопрос “Сколько человек играют партию в шашки?”, то бот уверенно отвечает, что два человека. Однако, в математической задаче, в которой нужно использовать это знание, бот в рандомные моменты времени забывает о нём. Возможно, это говорит о том, что выстроенные при обучении модели логические связи - довольно разрозненные.Но перейдём к результатам теста. Ниже список, где каждое число - это количество корректных ответов из 10 попыток (для каждой задачи): 3, 10, 6, 0, 1, 3, 6, 4, 6, 2.  То есть, на первую задачу было получено 3 правильных ответа из 10 заданных, на вторую задачу - 10 правильных ответов из 10 и так далее. Какие выводы? Во-первых, видим, что процент правильных ответов сильно скачет между задачами (есть даже крайние значения в 0 и 10 правильных ответов из 10). Хотя, задачи кажутся довольно близкими по уровню сложности.Суммарный процент правильных ответов в этом тесте получился: 41%. Много это или мало? Судя по ТГ-каналам про AI, люди довольно сильно радикализировались в своих оценках возможностей ChatGPT. Одни находят примеры, где бот ошибся на простейшей задаче и делают из этого далеко идущие выводы о слабости бота - по крайней мере в его текущей реализации. С точки зрения таких “критиков” - 41% на простых школьных задачах - это полный провал. Особенно учитывая перегретые ожидания, сформированные на новостях об успешной сдаче ВУЗовских экзаменов и прочих невероятных успехах. Есть и другая категория людей, которые напротив максимально лояльны к ChatGPT и воодушевлены его возможностями. С их точки зрения 41% - очень крутой результат, ведь ещё недавно мы и не думали, что машина вот-вот научится понимать логику человеческой речи, рассуждать, решать задачи - всё это звучит, как абсолютная фантастика. А ведь мы только в начале пути!Под конец припас небольшой твист. Озвученные выше результаты были получены на языковой модели GPT-3.5. Но уже есть версия GPT-4. Причём, в каналах видел мнение, что “Четвёрка по сравнению с версией 3.5 - это как профессор по сравнению с младшеклассником!”. Ок, предположим, что так. У нас ведь есть под рукой некий тест, давайте прогоним его же на версии 4 и сравним результаты. Итак, вот они: 9, 10, 10, 1, 2, 10, 6, 7, 10, 7.  Итоговый процент: 72%Оценка снова зависит от лояльности “оценщика”. Критически настроенные граждане скажут “Пф-ф... 72 процента на простых школьных-то задачах - уже не катастрофа, но всё равно слабовато!”. Лояльные же граждане скажут “Полный успех! Практически двукратный буст за одну версию - нереально круто! А в следующей версии будет ещё круче!”.  Пока вы выбираете какая из позиций вам ближе, есть ещё один твист. Точнее небольшое дополнение. Нашёл в интернете отличный набор задач - на этот раз без вычислений, а полностью из категории формальной логики. Ниже пример такой задачи (промпт о необходимости объяснять ход рассуждений, конечно, тоже использовался):Задача: Люди, которые либо высокие, либо тяжёлые, либо высокие и тяжёлые - нам не подходят. Джордж нам подходит.Варианты ответов:A. Джордж не высокийB. Джордж тяжёлыйC. Джордж высокий, но не тяжёлыйD. Ни один вариант из перечисленныхКажется, что подобные задачи прямо таки созданы для оценки умения AI понимать вопрос и рассуждать. Оценка валидности ответов на эти задачи делалась не только исходя из правильности буквы ответа, но и тщательно проверялись все логические выводы. Если встречалась грубая логическая ошибка, то ответ не засчитывался. Если выводы были недостаточно строгими, то бал пропорционально понижался.Результаты модели GPT-3.5: 5, 0, 0, 3.5, 0, 1.3, 1, 0, 0.  Суммарно: 10.8%Результаты модели GPT-4: 10, 7.8, 9.3, 7.2, 9.5, 8.7, 4.5, 10, 10. Суммарно: 77%В логических задачах разница уже не в 2 раза, а в 7 раз - это впечатляет. С нетерпением ждём новые версии моделей - ещё более сильные в математике, логике и здравом смысле. Всем добра!Приложение (тексты задач):Задача: У Лёвы, Гены, Васи, Толи и Миши были три барабана и две трубы. Какой музыкальный инструмент был у каждого мальчика, если у Гены, Левы и Миши были одинаковые инструменты?Задача: Имеется 16 конфет. Как разделить их между Колей и Петей, чтобы у Коли было на две конфеты больше, чем у Пети?Задача: Книга стоит один доллар плюс половину стоимости книги. Сколько долларов стоит книга?Задача: Ваня имеет столько же братьев, сколько и сестёр, а у его сестры вдвое меньше сестёр, чем братьев. Сколько сестёр и сколько братьев в этой семье?Задача: Плывут несколько уток друг за другом. Если взять одну из этих уток, то перед ней будет две утки. Если взять другую из этих уток, то позади нее будет две утки. Если взять третью из этих уток, то перед ней будет одна утка и позади нее будет одна утка. Какое минимальное число уток подходит под это описание?Задача: Два поезда, находящиеся на расстоянии 200 км друг от друга, начинают двигаться навстречу друг другу со скоростью 50 км/ч каждый. Муха начинает лететь от одного поезда  и летит по направлению к другому поезду со скоростью 75 км/ч. Долетев до другого поезда, муха летит назад к первому поезду. И так далее, пока поезда не встретятся. Какое расстояние пролетит муха за это время?Задача: Полное ведро с молоком весит 10 килограммов. Ведро наполненное до половины весит 6 килограммов. Сколько весит пустое ведро?Задача: Коля, Вася и Боря играли в шашки. Каждый из них сыграл две партии. Сколько всего партий было сыграно? Необходимо учесть, что каждую партию в шашки играют два человека.Задача: Два года назад возраст брата плюс возраст сестры равнялся 15 годам. Сейчас сестре 13 лет. Сколько должно пройти лет, чтобы брату исполнилось 9 лет?Задача: В городе живут два типа жителей: лжецы, которые всегда лгут и рыцари, которые всегда говорят правду. Путешественник встретил двух жителей города. Первый из них сказал: как минимум один из нас лжец. Кто из двух жителей лжец, а кто рыцарь?      Tags: chatgptaitesting  Hubs: Artificial Intelligence          


