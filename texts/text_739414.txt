

OpenAI будет бороться с «галлюцинациями» ИИ с помощью нового метода обучения / Habr


               OpenAI будет бороться с «галлюцинациями» ИИ с помощью нового метода обучения  Reading time  
    2 min
   Views  2.6K Machine learning *Artificial Intelligence       OpenAI объявила, что будет бороться с «галлюцинациями» ИИ путём внедрения нового метода обучения моделей искусственного интеллекта.t.me/yandexhq«Даже самые современные модели склонны к ложным выводам: они изобретают факты в моменты неопределённости», — пишут исследователи OpenAI в своём отчёте. По их словам, такие проблемы обычно возникают там, где требуется многоступенчатое рассуждение, поскольку для ложного вывода достаточно одной логической ошибки.Теперь OpenAI намерена вознаграждать модели ИИ в процессе обучения за каждый правильный шаг в рассуждениях вместо того, чтобы выдавать вознаграждение за окончательный вывод. По словам исследователей, этот подход называется «наблюдение за процессом», а не «наблюдение за результатом», и позволит прийти к более понятному ИИ, поскольку стратегия побуждает модели следовать более похожей на логику человека цепочке «мыслей».«Обнаружение и устранение логических ошибок модели, или галлюцинаций, является важным шагом на пути к созданию согласованного искусственного интеллекта общего назначения», — говорит Карл Коббе, математик из OpenAI.По словам Коббе, OpenAI выпустила сопроводительный набор данных из 800 тысяч вручную размеченных записей, которые использовались для обучения модели, упомянутой в исследовательской статье.Бен Уинтерс, старший советник Информационного центра электронной конфиденциальности и руководитель проекта по искусственному интеллекту и правам человека, выразил скептицизм по этому поводу. Он заявил, что хотел бы изучить полный набор данных OpenAI и сопутствующие примеры.Директор центра технологической ответственности в Университете Брауна Суреш Венкатасубраманян также рассматривает исследование OpenAI скорее как предварительное наблюдение. По его словам, «то, что может работать в одних условиях, модели и контексте, может не работать в других условиях». Исследователь также указал на то, что OpenAI не приводит никаких доказательств в пользу работы своей теории.Компания представила свою новую модель ИИ интерпретации изображений и текста GPT-4 в марте. Однако тогда OpenAI отказалась публиковать исследовательские материалы, лежащие в её основе. Участники сообщества ИИ раскритиковали это решение, отметив, что оно подрывает дух компании как исследовательской организации и затрудняет воспроизведение её работы другими. Одновременно это затрудняет разработку средств защиты от угроз, исходящих от систем ИИ.  Вскоре после этого некоммерческая организация Future of Life опубликовала письмо, в котором глава SpaceX Илон Маск, соучредитель Apple Стив Возняк, филантроп Эндрю Янг и ещё около тысячи исследователей искусственного интеллекта призвали «немедленно приостановить» обучение систем ИИ, «более мощных, чем GPT-4».      Tags: openaigpt-4chatgptискусственный интеллектмашинное обучениеобщий искусственный интеллектвознаграждениегаллюцинациифейки  Hubs: Machine learningArtificial Intelligence          


