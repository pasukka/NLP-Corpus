

Как нарушают законы робототехники: изучаем новые риски безопасности, связанные с ИИ / Habr


               Как нарушают законы робототехники: изучаем новые риски безопасности, связанные с ИИ  Reading time  
    5 min
   Views  2.2K Innostage corporate blog Information Security *Machine learning *Artificial Intelligence  
    Opinion
        Искусственный интеллект (ИИ) и машинное обучение (МО) становятся очень популярными. Технология ИИ уже во всю используется в самых разных сферах — от беспилотных автомобилей до здравоохранения, финансов и обслуживания клиентов.Но по мере того, как все больше и больше компаний внедряют эти технологии в массовом порядке и начинают переплетать их с критически важными бизнес‑операциями, они создают новые риски кибербезопасности. Так что, если вы занимаетесь ИБ, и ваша компания начала внедрять машинное обучение, вам точно стоит почитать этот пост.Меня зовут Ксения Рысаева, в Innostage я руковожу группой аналитики Центра противодействия киберугрозам Innostage CyberART. И в этом посте подробно расскажу об основных угрозах безопасности для приложений AI/ML.В ИИ есть 2 критических актива. Во‑первых, это большие данные, с помощью которых ИИ может учиться/тренироваться и делать выводы из своих прогнозов и идей. Второй актив — это сама модель данных. Модель — результат обучения алгоритма на больших данных, и это конкурентное преимущество каждой компании. Итак, что нужно знать об угрозах безопасности для приложений AI/ML:Искажение моделиИскажение модели — это атака, направленная на манипулирование результатами моделей машинного обучения. Злоумышленники могут попытаться внедрить в модель вредоносные данные, что приведет к неправильной классификации данных моделью и принятию неверных решений.Это эффективный способ обмануть системы ИИ, потому что невозможно сказать, приведет ли конкретный ввод к искаженному прогнозу, пока не будет получен результат.Внедряя строгие политики управления доступом для ограничения доступа к обучающим данным, компании могут предотвратить подделку входных данных модели злоумышленниками.Конфиденциальность данныхРиск утечки конфиденциальных данных существует всегда, ИИ – не исключение. Угрозы конфиденциальности и безопасности могут возникать на любом этапе жизненного цикла данных. Необходимо проводить регулярные аудиты безопасности и внедрять надежные методы защиты данных на всех этапах разработки ИИ.Фальсификация данныхКража данных. Манипуляция ими. Разоблачение пользователей. Если использовать ИИ и машинное обучение, риски всех этих угроз резко растут. Например, хакер может использовать инструменты или программное обеспечение для редактирования изображений, используемых при обучении системы искусственного интеллекта, или изменить информацию в самом наборе данных (например, изменить местоположение). Подделка приведет к неправильной классификации, когда модель работает с реальными примерами.Другой пример. В начале января Microsoft представила модель ИИ под названием VALL-E. Технология анализирует особенности речи человека на основе трехсекундного образца и моделирует голос. Так ИИ может создать речь, которую вы никогда не говорили в реальности. Очевидно, если раньше ИБ-специалисты советовали не говорить слово "да" в телефонном разговоре с незнакомцами, то теперь лучше вообще ничего не говорить.Внутренний инсайдерИнсайдеры могут получить доступ к важной или конфиденциальной информации вашей компании и использовать ее в своих интересах. Угроза данного типа по‑прежнему остаётся актуальной.Внутренние угрозы могут принимать различные формы:Некоторые сотрудники воруют данные и продают их конкурентам.Некоторые сотрудники крадут данные, чтобы шантажировать компанию.Некоторые сотрудники воруют данные из‑за личной обиды.Массовое внедрениеИИ и машинное обучение — это относительно новые и быстроразвивающиеся отрасли, и могут быть уязвимы. По мере роста их популярности и распространения по всему миру хакеры будут находить новые способы вмешательства во входные и выходные данные этих программ.Лучший способ защитить вашу компанию от угроз безопасности, связанных с ИИ, сочетать передовые методы кодирования, процессы тестирования и частые обновления по мере обнаружения новых уязвимостей.Атаки, управляемые ИИЗа последние несколько лет тенденция использования злоумышленниками ИИ в проведении атак заметно возросла.Для проведения атаки злоумышленники могут использовать алгоритмы машинного обучения, чтобы найти способ обойти элементы управления безопасностью. Либо хакеры могут использовать алгоритмы глубокого обучения для создания новых вредоносных программ на основе реальных образцов.ИИ упрощает имитацию доверенных лиц или поиск лазеек в существующих мерах безопасности.Например, вирусный чат‑бот с искусственным интеллектом ChatGPTможет посоветовать злоумышленникам, как легко взломать сеть. ChatGPT (Generative Pre‑trained Transformer) — разработка в области искусственного интеллекта, созданная исследовательской компанией OpenAI. Чат‑бот с искусственным интеллектом может вести беседы с людьми, имитирующими различные стили письма. Текст, созданный ChatGPT, гораздо более творческий и сложный, чем у ранее созданных чат‑ботов. Он был обучен на огромном количестве текстовых данных, полученных из Интернета, архивных книг и Википедии.Для своего эксперимента исследователи использовали обучающую платформу по кибербезопасности Hack the Box. Это виртуальная учебная среда, которая широко используется специалистами по кибербезопасности, студентами и компаниями для совершенствования хакерских навыков. Команда обратилась к ChatGPT, объяснив, что они проводят пентест, т. е. испытание на проникновение. Этот метод используют для воспроизведения взлома с использованием различных инструментов и стратегий. Обнаруженные уязвимости могут помочь организациям укрепить безопасность своих систем. Исследователи спросили: «Я столкнулся с проблемой тестирования на проникновение. Я нахожусь на веб‑сайте с одной кнопкой. Как мне проверить его уязвимости?». Чат‑бот ответил пятью основными отправными точками того, что проверять на веб‑сайте в поисках уязвимостей.Объясняя, что они видят в исходном коде, исследователи получали от ИИ совет, на каких частях кода следует сосредоточиться. Кроме того, они получили примеры предлагаемых изменений кода. Примерно через 45 минут общения с чат‑ботом исследователи смогли взломать предоставленный веб‑сайт.К тому же киберпреступники используют ChatGPT для:Создания стилера на основе Python, который ищет распространенные типы файлов, копирует их в случайную папку внутри папки Temp, архивирует их и загружает на жестко заданный FTP‑сервер.Создания вируса‑шифровальщика.Создания торговой площадки в даркнете с помощью ChatGPT.ВыводСистемами искусственного интеллекта и машинного обучения могут пользоваться как защитники, так и хакеры. При этом ИИ может предоставлять одну и ту же информацию, например, новые способы эксплуатаций уязвимостей. Однако защитникам это помогает залатать «дыры», а для хакеров — это возможность успешной атаки. Вопрос в том, кто окажется быстрее?       Tags: информационная безопасностьискусственный интеллектмашинное обучениеИБ угрозыChatGPTриски кибербезопасности  Hubs: Innostage corporate blogInformation SecurityMachine learningArtificial Intelligence          


