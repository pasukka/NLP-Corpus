

Mozilla сворачивает разработку DeepSpeech и объявляет о программе грантов / Habr


              13  April  2021 at 11:01  Mozilla сворачивает разработку DeepSpeech и объявляет о программе грантов Open source *Machine learning *Artificial Intelligence Voice user interfaces *      В 2017 году команда машинного обучения Mozilla Research запустила инициативу DeepSpeech, направленную на создание открытого источника модели автоматического распознавания речи. Но в ближайшие месяцы Mozilla планирует прекратить разработку и обслуживание модели, поскольку переходит к роли консультанта и запускает программу грантов для финансирования ряда инициатив, демонстрирующих способности приложений для DeepSpeech. В течение четырех лет вышло несколько версий модели, способных транскрибировать лекции, телефонные разговоры, телевизионные программы, радиошоу и другие прямые трансляции с «человеческой точностью».  Модель DeepSpeech представляет собой сквозную обучаемую архитектуру на уровне символов, которая может транскрибировать аудио на различных языках. Одной из основных целей Mozilla было добиться уровня ошибок при транскрипции слов ниже 10%, и новейшие версии предварительно обученной англоязычной модели достигают этой цели, в среднем демонстрируя около 7,5% ошибок.  Теперь Mozilla планирует передать проект «людям и организациям», заинтересованным в продолжении «исследований на основе вариантов его использования». Компания заявляет, что оптимизировала процессы непрерывной интеграции для запуска DeepSpeech с минимальными зависимостями. Она опубликует набор инструментов, который поможет любым другим заинтересованным сторонам использовать модель для создания голосовых решений. Последняя модель DeepSpeech содержит десятки миллионов параметров. Команда Mozilla Research начала обучать ее на ПК с четырьмя графическими процессорами Titan X Pascal, но в конечном итоге перенесла работу на два сервера с 8 Titan XP на каждом. В первые дни проекта обучение высокопроизводительной модели занимало около недели. В последующие годы Mozilla работала над уменьшением модели DeepSpeech, одновременно повышая ее производительность. Англоязычная модель сократилась с 188 МБ до 47 МБ, а потребление памяти упало в 22 раза.  В декабре 2019 года команде удалось заставить DeepSpeech работать «быстрее, чем в реальном времени» на одном ядре Raspberry Pi 4.  Mozilla первоначально обучила DeepSpeech с использованием свободно доступных наборов данных, таких как TED-LIUM и LibriSpeech, а также платных корпусов, таких как Fisher и Switchboard, но этого оказалось недостаточно. Команда обратилась к общественным теле- и радиостанциям, языковым факультетам университетов и другим организациям, которые, по их мнению, могли пометить речевые данные, чтобы поделиться ими. Благодаря этим усилиям они смогли более чем вдвое увеличить объем обучающих данных для англоязычной модели DeepSpeech.  Вдохновленная этими усилиями по сбору данных, исследовательская группа Mozilla в сотрудничестве с группой открытых инноваций запустила проект Common Voice, цель которого заключалась в сборе и проверке речевых данных. Common Voice включает не только речевые записи, но и из добровольно предоставленные метаданные, такие как возраст, пол и акцент говорящего.  Сегодня Common Voice является одним из крупнейших в мире мультиязычных корпусов, являющихся общественным достоянием, с более чем 9 тысячами часов голосовых данных на 60 различных языках, включая такие редкие языки, как валлийский и киньяруанда. Он включает данные более 164 тысяч человек. Чтобы поддержать проект, Nvidia объявила, что инвестирует $1,5 млн. Common Voice теперь будет работать под эгидой Mozilla Foundation в рамках инициатив, направленных на повышение надежности ИИ. Новая грантовая программа Mozilla будет предоставлять приоритет проектам, которые вносят вклад в основную технологию, а также демонстрируют потенциал для «расширения возможностей и обогащения» смежных областей. Более подробная информация будет объявлена 