

Разработка систем генеративного ИИ на базе ML Platform: создаем конкурента ChatGPT без миллионных инвестиций / Habr


                Разработка систем генеративного ИИ на базе ML Platform: создаем конкурента ChatGPT без миллионных инвестиций  Reading time  
    8 min
   Views  4.2K VK corporate blog Big Data *Machine learning *Artificial Intelligence       


2023-й — однозначно год генеративного искусственного интеллекта и сервисов на его основе, которые используют в разных кейсах и сценариях. Но даже при этом для многих сфера генеративного ИИ остается на уровне пользовательского интереса. Это упущение, ведь потенциал GPT-моделей и им подобных не ограничен поиском ответов на классические вопросы и даже ассистированием в процессе разработки. А их создание не относится к числу нерешаемых задач тысячелетия. GPT — технология, которую можно приручить, и это проще, чем кажется.


Меня зовут Александр Волынский, я технический менеджер продукта Cloud ML Platform в VK Cloud. В этом материале я расскажу, что такое GPT-модели, как упростить их создание под свои задачи, и покажу на практических примерах, как команда VK Cloud смогла создать GPT-модель при минимуме затрат. 

Статья подготовлена по мотивам моего выступления на VK Cloud Conf «Данные без ограничений. Как платформы становятся главным драйвером развития Data-проектов».

Немного теории 

GPT (Generative Pre-trained Transformer) — тип модели искусственного интеллекта. Она основана на технологии трансформера, которая используется для обработки естественного языка. Первая GPT-модель была представлена в 2018 году компанией OpenAI.


Для обучения GPT-модели нужно много текстовых данных и вычислительных ресурсов, которых у среднестатистического разработчика нет. При этом корректно обученная модель может «понимать» вопросы и способна выдавать «осмысленный» ответ, что делает ее универсальным инструментом с практически неограниченным спектром сфер применения — модель можно заточить под собственные задачи или задачи своей компании.


Вместе с тем перед обучением модели важно:


выбрать подход к «разработке» с учетом существующих сложностей;

определиться с подходом к работе с датасетом;

решить, где брать вычислительные мощности и как снижать требования к ним.



Теперь о каждом из этих пунктов подробнее.

Насколько сложно создать свою GPT-модель?

Классическая GPT-модель в привычном понимании очень сложна. Лучшее доказательство этого — стоимость обучения популярных моделей и объем задействованных ресурсов. Так, популярная в российском сообществе модель ruGPT3XL состоит из 1,3 млрд параметров, а для ее тренировки были задействованы 256 GPU, которые работали непрерывно 10 дней. Суммарно обучение ruGPT3XL обошлось примерно в 10 млн рублей. 


Другой пример — GPT-3 от OpenAI. Модель содержит 175 млрд параметров, тренировалась с помощью сотен высокопроизводительных GPU. Затраты на обучение модели оцениваются примерно в 345 млн рублей. 


Очевидно, что такие ресурсы есть не у всех компаний и частных лиц. Но это не значит, что создание такого сервиса — цель, доступная только избранным.

 

Порог входа снижает наличие Open-Source-моделей, многие из которых можно бесплатно и свободно использовать не только для исследовательских, но и для коммерческих целей. Среди них LLaMa, Alpaca, Vicuna, Koala и другие. Эти модели значительно меньше по размерам, чем тот же GPT-3, в котором 175 млрд параметров, но при этом они показывают себя во многих бенчмарках так же, как лучшие модели отрасли.


Например:


Alpaca. Показывает результаты, сравнимые с OpenAI. Основана на LLaMa. Датасет сгенерирован с помощью GPT-3.5 от OpenAI. Стоимость тренировки — менее $600. 

Vicuna-13B. Практически достигает уровня GPT-4 и Google Bard. Превосходит Alpaca и LLaMA в 90 % случаев. Датасет на основе ShareGPT. Стоимость тренировки — около $300. 

WizardLM-7B. В сложных сценариях превосходит Alpaca, Vicuna, ChatGPT. Основана на LLaMA. Подход к генерации датасета на основе Evol-Instruct. 



Важно, что эти и другие модели можно использовать не только в классических сценариях, но и для разработки своего решения — то есть в качестве базы, которую можно «доучить» под свои цели. Это позволяет при сравнительно небольших денежных затратах получить свой сервис, сравнимый с лучшими решениями в отрасли.

Что насчет ресурсов и набора данных

Даже работа с Open-Source-моделями не избавляет от необходимости использовать значительные вычислительные ресурсы. Например, модель Vicuna-13B, которая состоит из 13 млрд параметров, требует для обучения 8 GPU А100 с объемом памяти 80 ГБ. Модель Vicuna-7B — 4 GPU А100 с объемом памяти 40 ГБ.


Но и такие вычислительные мощности при запуске проектов есть не у каждой компании: это дорого и при неочевидных выгодах не всегда целесообразно. Поэтому для разработки систем генеративного ИИ многие компании выбирают облако, где нужную инфраструктуру и сервисы можно получить в несколько кликов и с платой только за фактически использованные ресурсы. 


Есть еще один способ обучать модель с помощью малого объема мощностей — Parameter-Efficient Fine-Tuning (PEFT). С его помощью, например, можно получить достаточную производительность обучения, используя всего одну видеокарту с 40 ГБ памяти. Такой эффект достигается благодаря тому, что мы учим не всю модель, а только отдельные ее параметры и адаптеры. Это не делает модель хуже аналогов, просто позволяет заточить отдельные ее «умения» под конкретные задачи. Например, модель размером в 13 миллиардов параметров помещается в память одной A100 GPU с 40 ГБ или даже обычной (потребительской) GPU с 16-24 ГБ. 


Таким образом, применение PEFT позволяет получить результат, сравнимый с полноценной тренировкой всей модели. 


Открытым остается только вопрос подготовки датасета для обучения модели. Рассмотрим два базовых подхода к его созданию: 


Подход Alpaca. Подразумевает генерацию набора с помощью существующих сервисов и GPT-моделей. Например, можно использовать API от OpenAI для создания инструкций и решений к этим инструментам.

Подход Databricks. Подразумевает самостоятельное создание обучающего набора. Подход впервые применила компания Databricks, которая привлекла сотрудников для обучения собственной GPT-модели: каждый из них составил несколько пар вопросов и ответов, всего получилось 15 тысяч таких пар. Этот подход более трудоемкий, чем Alpaca, но качество датасета при нем выше. 


Cloud ML Platform: решение для работы с GPT в облаке

Один из сценариев легкого начала работы с GPT — построение системы в облаке. При таком подходе можно получить масштабируемые ресурсы для обучения модели, делегировать управление инфраструктурой облачному провайдеру и использовать нужные инструменты в виде готовых сервисов. Например, в облаке VK Cloud можно выстроить весь пайплайн работы с данными, от сбора до аналитики потоковых данных. 




Для работы с машинным обучением в VK Cloud есть Cloud ML Platform — платформа для полного цикла ML-разработки. С ее помощью можно закрыть основные сценарии в процессе обучения GPT-модели и отказаться от ручной настройки:


при подключении источников, обработке данных, подготовке признаков;

при экспериментах с данными и ML-моделями, трекинге и версионировании экспериментов, данных и артефактов;

при деплое ML-моделей, интеграции в продукт, мониторинге и обновлении моделей в продакшене. 



Платформа состоит из трех компонентов:


JupyterHub — инструмент, который позволяет по клику создавать Jupyter Notebook для проведения экспериментов.

MLflow — инструмент для трекинга экспериментов и моделей, воспроизводимости экспериментов, деплоя ML-моделей, обеспечения взаимодействия команд при работе над комплексными Data Science- и ML-задачами.

MLflow Deploy — среда для упаковки ML-моделей и их автоматического развертывания в облаке. Компонент интегрирован с MLflow и JupyterHub. 



Также сейчас мы разрабатываем Spark в Kubernetes — сервис для распределенной обработки больших данных с возможностью вызова и управления из JupyterHub.


Такой набор инструментов и реализованных возможностей позволяет быстро приступить к обучению моделей без раздувания штата специалистов и долгой настройки. В том числе платформа позволяет применять набор методов PEFT. 

Создание GPT-модели: практический опыт VK Cloud 

Для демонстрации возможности создания полнофункциональной GPT-модели на основе сервиса Cloud ML Platform мы провели небольшой эксперимент. Взяли предварительно обученную Open-Source-модель Pythia-12B, которая содержит 12 млрд параметров и изначально обучена на англоязычном наборе. Затем собрали собственный датасет и дообучили модель. Для этого использовали одну GPU A100 с 40 ГБ. На обучение потратили 28 часов, после чего модель стала понимать вопросы на русском языке и корректно отвечать на них.


Стоимость обучения не превысила 7000 рублей: мы использовали ресурсы собственного облака с оплатой по модели Pay-as-you-go, что позволило нам, с одной стороны, получить нужные мощности, а с другой — исключить переплаты. 

Что получили: пример решения практических задач
Поиск ответов на узкоспециализированные темы


GPT-модель Pythia-12B, которую мы взяли в качестве основы, изначально не была адаптирована к работе в формате ChatGPT и не понимала вопросы на русском языке. После тренировки на нашем датасете модель научилась понимать вопросы на русском языке и давать на них адекватные ответы. Например, она умеет общаться на тему Kubernetes. Причем может отвечать кратко, а может выдавать достаточно развернутые и подробные ответы с примерами команды. 





Написание кода на Python 


Наша модель умеет по запросу писать код на Python. Она корректно справляется с написанием простых функций и программ. Один из сценариев применения такой возможности — обучение программированию на Python.



Извлечение данных из контекста 


Изначально модель не училась на энциклопедиях, поэтому обширных фактических знаний у нее нет. Вместе с тем она умеет вычленять информацию из контекста. Например, мы загрузили фрагмент текста, задали вопрос по поводу даты рождения Пушкина и получили корректный ответ.




Это позволяет применять нашу GPT-модель, например, при работе с документацией, когда нужно автоматически выдавать пользователям ответы из внутренней базы знаний. 

Перевод текста 


Наша GPT-модель обучена таким образом, что может использовать в качестве первоисточника даже текст на английском языке. При этом отвечает она по-прежнему на русском. В результате мы получили некий «побочный эффект»: если дать модели текст на английском и правильно поставить вопрос, она выдаст корректный перевод. 




Подробнее ознакомиться с подходом PEFT, который использовался для создания нашей модели, можно в репозитории.

Что показывает наш опыт

Получить свой сервис на основе предварительно обученных моделей проще, дешевле и быстрее, чем учить модель с нуля. Причем постепенно технология становится доступнее. Вместе с тем, если подобные сервисы планируется использовать в продакшен, лучше размещать модели внутри вашего контура, чтобы обеспечить конфиденциальность данных.

Развитие GPT снижает порог входа в технологию. Уже сейчас в отдельных кейсах десяток GPU можно заменить одной мощной видеокартой. Судя по тенденции, скоро можно будет использовать более простые видеокарты или быстрее тренировать модели и получать больше функциональности на прежних мощностях.

Благодаря облачным сервисам State-of-the-Art-результаты могут получать даже относительно небольшие команды с использованием малого объема ресурсов.

Для работы с GPT достаточно Open-Source-стека. Уже есть плагины, которые позволяют интегрировать GPT-модели с базами данных, документацией, сторонними сервисами и поисковыми системами. Еще проще, если работать с GPT в облаке, где можно получить все нужное ПО по клику, без сложной настройки. 


Вместо выводов: что нужно сделать сейчас, чтобы приблизить работу с GPT

GPT-модели и сервисы на их основе — долгосрочный тренд. С развитием технологии и повышением ее доступности интеграция этого инструмента в бизнес-процессы будет только углубляться, а его значение — расти. Поэтому к появлению такого сервиса нужно готовиться заранее и основной акцент нужно сделать на датасете: он важнее архитектуры модели, ресурсов и бюджета. 


Определите сценарии и типовые задачи, в которых сервис на основе генеративных моделей был бы полезен для вашего бизнеса. Это поможет просчитать бизнес-выгоды от разработки системы и определить целесообразность ее создания.

Разработайте предварительные сценарии использования модели. Например, перевод текста, поиск ключевых фраз в документе, выполнение математических операций. Это определит, какой датасет нужно собрать, для каждого кейса он свой. 

Уделяйте внимание не только объему датасета, но и качеству. Точность ответов GPT-модели зависит от достоверности и целостности информации, на которой ее обучают. Чтобы исключить использование «ложной» информации, старайтесь придерживаться концепции Data Quality. 

Готовьте компетенции внутри команды или ищите партнера. Это нужно для реализации проекта на всех этапах — от подготовки модели до ее использования на практике. Таким партнером может стать компания VK Cloud: команда облачного провайдера поможет подготовить такой сервис, используя Cloud ML Platform и другие решения платформы, сократив путь к созданию своей GPT-модели.  

      Tags: vk cloudGPTИИгенеративный ИИML  Hubs: VK corporate blogBig DataMachine learningArtificial Intelligence          


