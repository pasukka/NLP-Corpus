

Как сделать трансформер чат-бот на Trax? / Habr


              16  March  2021 at 13:22  Как сделать трансформер чат-бот на Trax? Python *Machine learning *Artificial Intelligence Natural Language Processing * 
        Tutorial
           Экспериментировать с библиотекой Trax и архитектурой трансформер оказалось крайне увлекательно. Предыдущая статья была про саммаризатор. В этой хочу рассказать о том, как я учил трансформер общаться на русском языке.Сравнительно простого чат-бота можно построить на базе языковой модели, которая умеет прогнозировать следующее слово по предыдущим, и которую несложно сделать, используя Трансформер-декодер по аналогии с GPT. В этом случае диалог формируется как связный неструктурированный текст. Чтобы превратить этот текст в чат, нужно вмешиваться в процесс генерации, добавляя реплики пользователя. Но обо всём по порядку.В качестве фреймворка я использовал библиотеку Trax.Trax — библиотека глубокого обучения с фокусом на понятный код и быстрые вычисления. Библиотека активно развивается и поддерживается командой Google Brain. Trax использует tensorflow и является одной из библиотек в его экосистеме, работает на CPU, GPU и TPU. На Google Colab TPU проверил. Нужно следить, чтобы количество текстов в пакете при обучении было кратно восьми.Для модели чат-бота я решил попробовать новую архитектуру — Reformer. Это трансформер, который может работать с длинными текстами размером с хорошую книгу буквально на одном ускорителе. Reformer кардинально снижает используемый объем памяти. Это достигается за счет двух вещей. Во-первых, у Reformer более эффективный в смысле памяти механизм внимания. Во-вторых, реверсивная схема вычислений, позволяющая отказаться от хранения значений активации для расчета градиента. Детали здесь на английском, а здесь перевод заметки из Google AI Blog.Мотивация: не то что бы у меня были какие-то гигантские диалоги,  но очень хотелось посмотреть как Reformer ведет себя на Google Colab и как учится в сравнении с Transformer. Оказалось, не зря."Танцы с бубном" вокруг ReformerВ Trax версии 1.3.7 (устанавливается через pip) в Reformer не удается загрузить веса предобученной модели. Это ошибка была исправлена в версии 1.3.4 и снова проявилась в 1.3.7. На форуме предложили пока использовать версию 1.3.6. Так и сделал, тут же возникла проблема с библиотекой T5, части которой Trax использует для работы с данными. Откатив и T5 к предыдущей версии, я «потерял» trax.data.tokenize, который отказался работать с последней версией sentencepiece. Тут я бросил путешествовать в историю пакетов и просто сегментировал всё заранее при помощи SentencePieceProcessor.ДанныеНужный набор данных я скачал на Яндекс.Толока. Он называется «Toloka Persona Chat Rus» и его можно использовать в некоммерческих целях с упоминанием источника. Упоминаю: Ребята из Яндекс.Толока — вы большие молодцы!  Набор содержит 10000 русскоязычных диалогов на общие темы с возможностью фильтрации по профилю пользователя. Можно было бы например отфильтровать диалоги, где один из пользователей женщина, но я посчитал, что их и так не очень много. Диалоги представлены в виде HTML-текста, пришлось потратить некоторое время на чистку.Примеры текста в процессеВ наборе данных:<span class=participant_2>Пользователь 2: Привет) расскажи о себе</span><br /><span class=participant_1>Пользователь 1: Привет) под вкусный кофеек настроение поболтать появилось<br />)</span><br /><span class=participant_2>Пользователь 2: Что читаешь? Мне нравится классика</span><br /><span class=participant_2>Пользователь 2: Я тоже люблю пообщаться</span><br /><span class=participant_1>Пользователь 1: Люблю животных, просто обожаю, как и свою работу)</span><br /><span class=participant_1>Пользователь 1: Я фантастику люблю</span><br /><span class=participant_2>Пользователь 2: А я выращиваю фиалки</span><br />...После очистки:2: Привет) расскажи о себе1: Привет) под вкусный кофеек настроение поболтать появилось )2: Что читаешь? Мне нравится классика2: Я тоже люблю пообщаться1: Люблю животных, просто обожаю, как и свою работу)1: Я фантастику люблю2: А я выращиваю фиалки...2: И веду здоровый и активный образ жизни!После склейки реплик:2: Привет) расскажи о себе1: Привет) под вкусный кофеек настроение поболтать появилось )2: Что читаешь? Мне нравится классика Я тоже люблю пообщаться1: Люблю животных, просто обожаю, как и свою работу) Я фантастику люблю2: А я выращиваю фиалки И веду здоровый и активный образ жизни!Некоторые диалоги начинаются со второго пользователя, там нужна замена идентификаторов:1: Привет) расскажи о себе2: Привет) под вкусный кофеек настроение поболтать появилось )1: Что читаешь? Мне нравится классика Я тоже люблю пообщаться2: Люблю животных, просто обожаю, как и свою работу) Я фантастику люблю1: А я выращиваю фиалки И веду здоровый и активный образ жизни!Для обучения модели требуется неструктурированный текст, примерно такой:'1: Привет) расскажи о себе 2: Привет) под вкусный кофеек настроение поболтать появилось ) 1: Что читаешь? Мне нравится классика Я тоже люблю пообщаться 2: Люблю животных, просто обожаю, как и свою работу) Я фантастику люблю 1: А я выращиваю фиалки И веду здоровый и активный образ жизни! 2: Ух ты, интересно. 1: Ты случайно не принц на белом коне? Я его очень жду.. 2: А у меня из хобби каждую неделю тусить с моим лучшим другом) STOP'Два момента: Идентификаторы «Пользователь 1:» и «Пользователь 2:» в исходном тексте пришлось сократить до «1:» и «2:», чтобы они не разбивались на два токена.В конец каждого диалога я добавил слово STOP, которое кодируется одним токеном. Это нужно для остановки декодера при генерации диалога.Справедливо решив, что для модели с несколькими десятками миллионов параметров десяти тысяч диалогов будет маловато, я сделал еще 36 тысяч, просто «откусывая» от начала диалогов по четному количеству реплик так, чтобы оставалось не меньше восьми. В общем, и этого не очень-то много, в связи с этим вопрос: где взять еще диалогов на русском?Процесс подготовки данных представлен на схеме:В качестве модели для сегментации текста, как и в эксперименте с саммаризатором, я использовал Byte Pair Encoding (BPE) из библиотеки sentencepiece. Размер словаря 10k токенов.Пример текста после сегментации['