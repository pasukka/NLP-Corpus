

Исследование: обучение на сгенерированных данных может привести к коллапсу большой языковой модели / Habr


               Исследование: обучение на сгенерированных данных может привести к коллапсу большой языковой модели  Reading time  
    3 min
   Views  7.4K Machine learning *Research and forecasts in IT *Artificial Intelligence       Группа учёных из Кембриджского и Оксфордского университетов опубликовала исследование, которое доказывает, что обучение больших языковых моделей на контенте, произведённом другими моделями, вызывает дегенеративный процесс, который с течением времени заставляет модели забывать истинное базовое распределение вероятностей. Они назвали это явление коллапсом модели. Математика показывает, что «через несколько поколений текст становится мусором», — написал в своём блоге один из участников исследования, профессор Кембриджского университета Росс Андерсон. По мнению исследователей, если люди продолжат использовать ИИ для создания контента, обучение языковых моделей при помощи парсинга веб-страниц будет становиться всё менее и менее эффективным.Один из авторов статьи «Проклятие рекурсии: обучение на сгенерированных данных заставляет модели забывать», Илья Шумайлов из Кембриджского университета, так прокомментировал гипотезу для VentureBeat: «Со временем ошибки в сгенерированных данных накапливаются и в конечном итоге вынуждают модели, которые учатся на них, всё хуже воспринимать реальность». Издание проводит аналогию с комедией «Множественность» (Multiplicity, в российском прокате «Множество») с Майклом Китоном в главной роли, когда герой клонирует себя, а затем клонирует клонов, и каждое действие приводит к экспоненциальному снижению уровня интеллекта и увеличению глупости.По словам Шумайлова, исходные данные, созданные человеком, представляют мир более объективно и включают маловероятные данные; а генеративные модели склонны обучаться на популярных данных и часто неправильно понимают или искажают менее популярные.Шумайлов иллюстрирует эту проблему гипотетическим сценарием, в котором модель обучается на наборе данных с изображениями 100 кошек: 10 из них обладают синим мехом, 90 — жёлтым. Модель считает жёлтых кошек более распространёнными и воспринимает синих кошек «более жёлтыми», чем они есть на самом деле. И когда модель просят сгенерировать новые данные, в них появляется несколько зелёных кошек. Со временем, в результате последовательных циклов обучения, первоначально присутствовавший в наборе данных синий мех исчезает, превращаясь в зелёный, а потом в жёлтый. Это прогрессирующее искажение и возможная потеря характеристик и является коллапсом модели. Даже когда исследователи научили модели не выдавать слишком много повторяющихся ответов, сбой всё равно происходил, поскольку модели начинали генерировать ошибочные ответы, чтобы избежать повторов.Важно отметить, что коллапс модели отличается от «катастрофического забывания», когда модели теряют ранее усвоенную информацию. Напротив, коллапс предполагает, что модели неправильно интерпретируют реальность на основе своих подкреплённых убеждений. И даже если использовать для обучения последующих поколений 10% исходных данных, созданных человеком, коллапс всё равно происходит, но не так быстро.Исследователи предлагают два способа, которые могут помочь преодолеть этот сбой. Первый заключается в сохранении оригинального набора данных, созданного исключительно человеком, без загрязнения данными, сгенерированными ИИ. Впоследствии модель можно периодически переобучать на этих данных или полностью обновлять. Второй способ предполагает использование в обучении только новых, «чистых», сгенерированных человеком датасетов. Исследователи обращают внимание, что для этого потребуются определённые усилия со стороны производителей контента или компаний, занимающихся искусственным интеллектом; им придётся разработать и внедрить механизм массовой маркировки контента.      Tags: llmязыковая модельданныедатасетобучениеколлапс модели  Hubs: Machine learningResearch and forecasts in ITArtificial Intelligence          


