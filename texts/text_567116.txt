

Как новый метод упаковки в BERT ускоряет обработку естественного языка в 2 раза / Habr


              11  July  2021 at 17:47  Как новый метод упаковки в BERT ускоряет обработку естественного языка в 2 раза SkillFactory corporate blog Python *Programming *Machine learning *Natural Language Processing * 
        Translation
         
                Original author:
                
                  Dr. Mario Michael Krell, Matej Kosec
                  Используя новый алгоритм упаковки, в Graphcore ускорили обработку естественного языка более чем в 2 раза при обучении BERT-Large. Метод упаковки удаляет заполнение, что позволяет значительно повысить эффективность вычислений. В Graphcore предполагают, что это также может применяться в геномике, в моделях фолдинга белков и других моделях с перекошенным распределением длины, оказывая гораздо более широкое влияние на различные отрасли и приложения. В новой работе Graphcore представили высокоэффективный алгоритм гистограммной упаковки с неотрицательными наименьшими квадратами (или NNLSHP), а также алгоритм BERT, применяемый к упакованным последовательностям. К старту курса о машинном и глубоком обучении представляем перевод обзора соответствующей публикации на ArXiv от её авторов. Ссылку на репозиторий вы найдёте в конце статьи.Вычислительные потери NLP из-за заполнения последовательностейМы начали исследовать новые способы оптимизации обучения BERT во время работы над нашими недавними бенчмарками для MLPerf