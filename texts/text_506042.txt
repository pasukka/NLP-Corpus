

Нейросетевые языковые модели как многоцелевой медицинский ИИ / Habr


               9  June  2020 at 13:52  Нейросетевые языковые модели как многоцелевой медицинский ИИ Big Data *Machine learning *Artificial Intelligence Telemedicine       Нейросетевые языковые модели — это большие нейронные сети, которые обучаются предсказывать следующее слово (или часть слова) в тексте с учетом предыдущего контекста. Несмотря на кажущуюся простоту задачи, оказалось, что такая постановка задачи приводит к появлению весьма многофункциональной нейронной сети.


Некоторые исследователи даже предположили, что языковые модели могут стать путем к AGI — сильному искусственному интеллекту человеческого уровня. Предположение это исходит из того, что сама задача предсказания следующего слова является ИИ-полной (требующей мышления) на уровне человека. В этой статье приведу некоторые примеры того, что может сделать с языковой моделью, обученной на медицинских данных.


Идеи о том, что языковые модели могут учится выполнять самые разные задачи существуют давно, но до самого последнего времени к ним редко относились серьезно. 


Ситуация изменилась с появлением больших вычислительных мощностей. Известные нейронные модели GPT от OpenAI умеют выполнять машинный перевод, отвечать на вопросы и другие задачи без специализированного обучающего набора, при этом недавно выпущенная огромная модель GPT-3 [1], содержащая 170 млрд. параметров, может по нескольким примерам «догадаться» о том, что от нее хотят и начать решать нужную задачу (хотя, как отмечают авторы, это работает не со всеми задачами и не во всех случаях).


На практике пока такие огромные модели сложно использовать в силу ограниченности ресурсов, поэтому в большинстве задач традиционные подходы пока продолжают доминировать. Однако есть области, где их применение уже сейчас может принести полезные результаты.


Одна из таких областей — медицина. В этой области открытых наборов обучающих данных сравнительно мало из-за высоких затрат на их создание и аннотацию, а также из-за проблем с защитой персональных данных. 


В этой статье я приведу несколько примеров того, что может сделать языковая модель в медицинской области. Для изучения этого вопроса мы обучили довольно большую языковую модель на основе набора данных биомедицинской литературы, состоящего из всех аннотаций PubMed + 2 миллиона бесплатных полнотекстовых статей + дискуссий на медицинских форумах в Интернете. Наша модель имеет 800 миллионов параметров, что значительно меньше OpenAI GPT-3, но сравним по размерам с со средней GPT-2. 


Поскольку делалось это с исследовательскими целями, для опытов мы использовали модель работающую на уровне символов, т. е. она получает на вход не разделенную на слова последовательность букв и предсказывает следующий символ. При этом подходе с «абсолютного нуля» понимание текста намного сложнее, так как нужно сначала понять, что есть слова, что слова обозначают понятия, которые в свою очередь связаны между собой. Сможет ли такая модель обучится каким-то серьезным закономерностям в области медицины?

Образцы генерации текста

Во-первых, мы обнаружили, что модель может генерировать связные фрагменты текста, описывая довольно разумные эксперименты. Вот два примера:

1. The present study was designed to evaluate the effect of a single intravenous injection of a single dose of recombinant human erythropoietin (rhEPO) on the serum levels of the pro-inflammatory cytokines IL-1, IL-6, IL-8, IL-10, IL-12p40, and TNF-