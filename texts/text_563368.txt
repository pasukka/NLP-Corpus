

Facebook подключит ИИ для модерации комментариев / Habr


              17  June  2021 at 18:15  Facebook подключит ИИ для модерации комментариев Artificial Intelligence Social networks and communities IT-companies       Facebook запускает в тестовом режиме систему оповещения администраторов сообществ о появляющихся конфликтах, работающую на основе анализа ИИ.      16 июня Facebook анонсировала несколько новых опций для владельцев сообществ и объявила о запуске в тестовом режиме системы оповещения Conflict Alerts. В основе системы лежит работа нейросети, которая будет искать и маркировать «нездоровые» комментарии пометкой «moderation alert» («оповещение модератора»). После администраторам придут сообщения о количестве подозрительных комментариев. Компания не сообщила подробностей о работе новой системы. Сайт Verge обратился к Facebook за дополнительным комментарием и выяснил, что компания будет использовать модель машинного обучения для поиска нежелательного контента. В качестве вводных данных пресс-секретарь Facebook указал лишь частоту и объём комментариев. Новый инструмент от Facebook похож на уже работающее оповещение по ключевым словам Keyword Alerts. Руководитель сообщества самостоятельно настраивает список фраз и отдельных слов. Затем система либо не пропустит комментарии с такими словами, либо оповестит о них. Система определяет и отправляет уведомление администратору о появлении нужных комментариев. В ноябре прошло года Facebook уже внедрила ИИ, помечающий подозрительные и приоритетные комментарии для администраторов соцсети. Компания указала, что в приоритете у алгоритма стоят популярные материалы и контент, связанный с терроризмом, сексуальной эксплуатацией детей или причинением вреда самому себе. Спам алгоритмы воспринимают как наименее важный контент. Алгоритм находит заданные в параметрах высказывания, но не умеет определять сарказм, иронию или сленг. В результате все эти «шутливые» комментарии попадают к администраторам.Невозможность распознать сарказм и шутки — общая проблема для всех подобных нейросетей. 4 июня этого года команда учёных под руководством Оксфордского университета опубликовала исследование, показавшее, что алгоритмы крупных компаний по-разному справляются с поиском нежелательного контента. Они были либо слишком суровыми, либо недостаточно строгими, и ни один из них не смог отличить сарказм, иронию или даже цитаты от вредоносного контента.     Tags: facebookмодерация контентасоциальные сетиискусственный интеллектмашинное обучение Hubs: Artificial IntelligenceSocial networks and communitiesIT-companies          


