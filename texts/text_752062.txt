

Как сделать контекстное окно на 100K в большой языковой модели: обо всех фокусах в одном посте / Habr


               Как сделать контекстное окно на 100K в большой языковой модели: обо всех фокусах в одном посте  Reading time  
    17 min
   Views  4.9K GPGPU *Machine learning *Artificial Intelligence CPU Natural Language Processing *  
    Translation
     
                Original author:
                
                  Galina Alperovich
                  От переводчика: выражаю огромную искреннюю благодарность Дмитрию Малову @malovdmitrijза консультации по ходу этого перевода, помощь в подборе формулировок, пояснение рисунков и незаменимую человеческую поддержку.tldr; в статье рассмотрены приёмы, позволяющие ускорить обучение больших языковых моделей (LLM) и нарастить в них вывод (inference). Для этого нужно использовать большое контекстное окно, в котором умещается до 100K входных токенов. Вот эти приёмы: ALiBi с подмешиванием в вектор позиции слова в последовательности (positional embedding), разреженное внимание (Sparse Attention), мгновенное внимание (Flash Attention),  многозапросное внимание, условные вычисления и GPU A100 на 80 ГБ.   Недавно прошло несколько анонсов по поводу новых больших языковых моделей (LLM), настроенных на приём исключительно крупного контекстного окна, целых 65K токенов (MPT-7B-StoryWriter-65k+ от MosaicML) или даже 100K токенов (описано в статье Introducing 100K Context Windows от Antropic). Google в техническом отчёте по Palm-2 не упоминает конкретный размер контекстного окна, но указывает, что удалось «значительно увеличить длину контекстного окна в модели». Для сравнения: современная модель GPT-4 может работать, получая контекст длиной 32K входных токенов. Большинство же опенсорсных моделей LLM работают с контекстным окном длиной 2K токенов.Это впечатляет, так как при работе с таким огромным контекстным окном промпт может буквально сравниться по размеру с целой книгой. Так, в романе «Великий Гэтсби» насчитывается 72K токенов, 210 страниц, и этот роман прочитывается за 6 часов при скорости чтения 1,7 минут/страница. Так что вот какой объём «специфичной» информации может просканировать модель и далее хранить для обработки запросов! Я пыталась уложить в голове, как такое вообще возможно с технической точки зрения. Поэтому в данном посте по крупицам собрала информацию (первой ниточкой стал для меня этот тред) и решила раскрыть следующие вопросы:Почему так важна длина контекста, и почему она может перевернуть ситуацию   Каковы главные ограничения оригинальной архитектуры трансформеров, если применять её для обработки длинных контекстов    Вычислительная сложность трансформерной архитектуры   Какие в настоящее время существуют приёмы оптимизации, позволяющие ускорить трансформер и увеличить длину контекста до 100K   "Краткая" аннотацияЗдесь и далее я буду в качестве синонимов использовать термины «длина контекста», «окно контекста» и «количество входных токенов», обозначая их как n.Этот пост получился несколько длинным, так что сделаю здесь подборку основных моментов и приёмов, рассмотренных ниже:1-я задача – это квадратичная сложность (пространственная и временная) у некоторого слоя внимания относительно количества n токенов, подаваемых на вход.Когда размер вектора d > n, 2-я задача сводится к квадратичной временной сложности линейных слоёв относительно вектора, имеющего размер d.3-я задача – это позиционный синусоидальный эмбеддинг, применяемый в оригинальной архитектуре.В трансформерной архитектуре очертания доступных для изучения весов матрицы не зависят от количества входных токенов n.Таким образом, трансформер, обученный на 2K контекстных примеров может потреблять токены любой длины, даже 100K. Но на стадии вывода модель не произведёт осмысленного результата на материале 100K токенов, если её не обучали на 100K.Обучение бесхитростного трансформера на гигантском корпусе текстов и только с использованием очень длинных контекстов оказывается невыполнимо затратным из-за квадратичной сложности относительно n и d. По имеющейся оценке, обучение LLaMA на контексте длиной 2K обошлось бы в ~$3M. Следовательно, обучение LLaMA на 100K стоило бы ~$150M.Есть вариант: обучить модель на контексте в 2K токенов, а затем тонко настроить её на более длинных контекстах (например, 65K). Но такой подход не сработает с оригинальной архитектурой трансформеров, всё дело – в позиционной синусоидальной кодировке.[Фокус #1] Можно избавиться от позиционной синусоидальной кодировки и воспользоваться ALiBi; это простой и изящный позиционный эмбеддинг, который не портит точности. С его помощью можно обучить модель на 2K, а далее тонко настраивать на 100K.[Фокус #2] Не требуется вычислять показатели внимания между всеми токенами. Одни токены важнее других, поэтому можно воспользоваться Разреженным Вниманием. Так можно одновременно ускорить и обучение, и вывод.[Фокус #3] Flash Attention эффективно реализует слой внимания для GPU. Этот инструмент использует замощение и обходится без материализации больших промежуточных матриц (n, n), не умещающихся в SRAM GPU. Так можно одновременно ускорить и обучение, и вывод.[Фокус #4] Использовать многозапросное внимание вместо многоголового. В таком случае все головы смогут совместно использовать веса, при этом линейно проецируя K и V. Так удаётся радикально ускорить поступательный вывод.[Фокус #5] При условных вычислениях можно не применять всех параметров модели ко всем токенам, содержащимся во входной последовательности. CoLT5 применяет тяжеловесные вычисления только с самыми важными токенами и процессами, а для обработки остальных токенов применяются облегчённые варианты слоёв. Так можно одновременно ускорить и обучение, и вывод.[Фокус #6] Чтобы вместить большой контекст, вам потребуется много RAM в GPU, поэтому в так их случаях принято использовать GPU на 80 ГБ A100.В сухом остатке: чем сильнее вы  ускоряете обучение и вывод, тем более обширный контекст можете использовать.Теперь давайте обсудим все эти моменты более подробно.Почему длина контекста так важнаДлина контекста – один из важнейших лимитирующих факторов при работе с большими языковыми моделями. Увеличить контекст до 100K – это уже невероятное достижение, ставшее реальностью (занятно, как этот тезис будет восприниматься через год).Вот как выглядит один из важных практических случаев, в которых желательно применять большие языковые модели: «забросить в LLM целую кучу пользовательских данных (документов, касающихся работы компании либо конкретной задачи; также это могут быть различные разнородные тексты, т.д.) и задавать вопросы по этим конкретным данным, а не по какой-нибудь взятой из Интернета отвлечённой информации, которую LLM видела на этапе обучения.В настоящее время обходить это ограничение пробуют по-разному, а именно:При помощи приёмов резюмирования и тщательно подобранных сцепленных промптов Ведя векторные базы данных, в которых хранятся векторы для пользовательских документов с последующим «поиском» по этому корпусу в соответствии с некоторой метрикой схожести Когда это возможно – тонко настраивать LLM на данных, предоставляемых пользователем (такая возможность предоставляется не во всех коммерческих LLM, а для опенсорсных LLM это не самая тривиальная задача)Разработка специализированных сравнительно небольших LLM для конкретных данных, которые нас интересуют (опять же, не самая тривиальная задача)При наличии длинного контекстного окна уже имеющаяся в вашем распоряжении большая языковая модель (видевшая целый Интернет) может изучить имеющийся у вас контекст и данные, а затем взаимодействовать с вами на совершенно ином уровне, предполагающем более высокую персонализацию. Всё это – без изменения весов модели, когда всё «обучение» производится на лету, «в памяти». В целом, чем больше контекстное окно, тем более высокая точность, беглость и изобретательность приобретается моделью.В качестве аналогии здесь можно рассмотреть ОЗУ компьютера, где операционная система хранит в режиме реального времени актуальный контекст для всех ваших приложений. LLM, располагая достаточно длинным контекстом, сравнима с «рассуждающим компьютером», учитывающим широкий контекст, предоставляемый пользователем.Оригинальный трансформер и длина контекстаВажно отметить, что в архитектуре транмформеров формы всех весов матриц, доступных для обучения, не зависят от количества подаваемых на вход токенов n. Все параметры, поддающиеся обучению (поиск по векторам, слои проекций, слой softmax и слои внимания) не зависят от длины входного фрагмента и должны быть в состоянии обрабатывать такие фрагменты варьирующейся длины. Просто отлично, когда такое свойство в архитектуре предоставляется прямо «из коробки».Это значит, что, если вы обучали трансформерную модель с контекстным окном длиной 2K, то можете экстраполировать её на последовательности токенов любой длины. Единственная проблема здесь в том, что в таком случае модель не сможет методом вывода дать осмысленных результатов на материале в 100K токенов, если её не обучали на окне контекста длиной 100K. В таком случае распределение учебных данных будет очень далеким от тех данных, которые приходится логически обрабатывать, поэтому модель потерпит фиаско, как и любая модель машинного обучения в таком сценарии.Если требуется обучить трансформер на таком большом контексте, то можно, например, обучать его в два этапа: сначала базовую модель на окне контекста длиной 2K токенов, а потом продолжить обучение (в качестве тонкой настройки) на более длинных контекстах (например, 65K или 100K). Именно это и было сделано с моделью MosaicML. Но вот загвоздка: такой подход не сработает с оригинальной архитектурой Трансформеров, поэтому придётся прибегать к определённым ухищрениям (см. Фокус #1 ниже в этом посте).Напомню, что такое "многоголовый подход к вниманию"Вызовы, с которыми приходится справляться при работе с очень длинными контекстами, связаны с вычислительной сложностью трансформерной архитектуры. Прежде чем обсудить эту сложность, давайте освежим в памяти, как именно работает слой внимания. Q — запросы, K — ключи и V — значения. Такие обозначения типичны для статьи, касающейся извлечения информации, где в систему подаётся «запрос», а далее подыскивается ближайший к нему «ключ».    n —  количество входных токенов d — размерность текстового векторного представления h — количество голов в слое внимания k— размер линейной проекции для Q и K v — размер линейной проекции для VМногоголовое вниманиеМожно выполнить в слое вектора такой поиск, по результатам которого для заданного токена возвращается вектор размером (1, d). Соответственно, для последовательности из n токенов получаем матрицу текстовых векторных представлений X размера (n, d). Затем суммируем её с позиционным синусоидальным эмбеддингом. Слой многоголового внимания нужен для того, чтобы рассчитать новый вектор для данной последовательности токенов. Эта последовательность такова, что может считаться оригинальным текстом, кодирующим X, но при этом является взвешенной (1) по относительной важности среди всех токенов относительно контекста и (2) по относительным позициям токенов.Эту матрицу вектора X (n, d) мы обрабатываем параллельно на h слоях внимания (головах). Чтобы получить Q, K и V для всех голов в слое внимания, мы линейно  проецируем X на размерности k, k и v, соответственно. Это делается путём умножения X на h матриц формы (d, k), (d, k) и (d, v). Можете трактовать эту операцию как перемножение (n, d) на (h, d, k), (h, d, k) и (h, d, v).Головы в слое внимания возвращают h матриц с баллами внимания размера (n, v). Затем сцепляем фрагменты от всех голов (n, h*v) и линейно проецируем их на следующие этапы.Общий вид архитектуры внимания; схема взята из статьи Attention is All You NeedВзвешенное скалярное вниманиеТеперь давайте как под лупой рассмотрим одну голову в слое внимания.Q, K, V – это 3 линейные проекции X размера (n, k), (n, k) и (n, v), получаемые умножением доступных для обучения весов отдельно для каждой головы.Получаем баллы внимания, отдельно вычисляя расстояние (скалярное произведение) между Q и K (в транспонированном виде). Умножаем матрицу (n, k) на (k, n) и получаем матрицу (n, n). Затем умножаем её на маскирующую матрицу, чтобы обнулить некоторые токены (требуемые в декодере). Затем масштабируем её и применяем softmax в диапазоне от 0 до 1. Таким образом, получаем матрицу формы (n, n) с n_ij – это относительный балл внимания от 0 до 1 между i-м и j-м токеном, демонстрирующий степень «близости» этих токенов в данном конкретном контексте длины n.Затем умножаем эту матрицу баллов внимания (n, n) на “значения” V размером (n, d), чтобы получить текстовое векторное представление, взвешенное по этим относительным баллам внимания.В оригинальной статье матрица баллов внимания для одной головы вычисляется по этой формуле   Давайте рассмотрим следующий фрагмент кода из статьи о многозапросном внимании. В нём показано, как многоголовое внимание рассчитывается с применением батчинга, причём, формы матриц на каждом шаге ясны. Также они включают маскирующее умножение, используемое в ходе декодирования.Очень красивый код, демонстрирующий формы матриц на каждом шаге работы со слоем внимания. Код взят из статьи Multi-Query .Сложность трансформера и длина контекстаСложность перемножения 2 матриц размером (a,b)*(b,c) равна O(a*b*c). Для простоты предположим, что k*h = O(d) и будем  от этого отталкиваться, чтобы вывести сложность внимания.Сложность слоя внимания складывается из двух частей:Линейные проекции для получения Q, K, V: умножение матрицы эмбеддинга размером (n, d) на h матриц, доступных для обучения (d, k), (d, k) и (d, v). Следовательно, сложность ~ O(nd