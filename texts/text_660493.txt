

Как сжать fastText, или Приключение на 20 минут / Habr


               12  April   at 16:16  Как сжать fastText, или Приключение на 20 минут «Антиплагиат» corporate blog Programming *Machine learning *Microservices *Natural Language Processing *      Тема, о которой я хочу вам рассказать, появилась не из-за какого-то оглушительного успеха, громкого провала или желания поделиться каким-то сакральным знанием с и так уже максимально искушённым читателем Хабра. Равно как эта тема не была плодом долгой и кропотливой работы — её не планировали, почти не обсуждали и тем более не утверждали заранее.
Всё выглядело как приключение, после которого остались настолько тёплые воспоминания, что захотелось ими поделиться. Проникнитесь и вы духом приключения, желанием разгадывать загадки и чувством восстановленной справедливости!
И как говорил известный персонаж: «Давай, вошли и вышли, приключение на 20 минут».

Кадр из сериала «Рик и Морти» (англ. Rick and Morty), 3-й сезон, 6-я серия, Adult Swim, 2017 год

Начало приключения
Антиплагиат, несомненно, известен своими решениями по поиску заимствований. Наши возможности растут, а планы воистину грандиозны. К примеру, уже запущен сервис поиска переводных заимствований для сотни языков, среди которых есть и сравнительно редкие: аварский, якутский, эрзянский и т. п. 
В какой-то части пайплайна при работе с этими языками используются fastText-эмбеддинги (если этот зверь вам пока не знаком, потерпите, дальше необходимый минимум информации будет предоставлен, а для нетерпеливых: здесь и/или здесь) слов, которые получаются с помощью fastText-моделей, обученных на корпусах текстов — наборах текстовых данных. История, которую я расскажу, как раз появилась в момент доведения «исследовательского» прототипа этой части пайплайна до решения, отправляемого в прод. 
Итак, дано какое-то количество fastText-моделей для редких языков. Каждая из них весит несколько гигабайт, что для моделей fastText обычное дело. Размеры этих моделей были сопоставимы с размерами моделей для популярных языков. Однако корпусы текстов, на которых обучаются модели для популярных языков, и корпусы редких языков (на которых обучались наши модели) по своим размерам отличаются на порядки — количество уникальных слов в корпусах редких языков чаще значительно меньше. Как минимум этот факт порождает подозрение, что имеет место какая-то неэффективность или несправедливость. 
А можно ли сократить или сжать модель без потерь? Конечно, можно! Ведь об этом писали и уже предложили интересные решения для fastText-моделей популярных языков Андрей Васнецов (на Medium) и Давид Дале (на Хабре, @cointegrated). Здесь же будет акцент именно на то, что языки редкие. Но, прежде чем двигаться дальше, давайте вкратце разберёмся, что такое этот fastText и эмбеддинги слов.
Что такое эмбеддинги
Для анализа текстов хорошо бы понимать смысл написанного. 
А как, например, сравнивать между собой по смыслу слова? Этот вопрос касается как предложений и документов, так и других структурных единиц текста. Предположим, что близкие по контексту слова имеют схожий смысл. Эта идея есть не что иное, как дистрибутивная гипотеза (John R. Firth. A Synopsis of Linguistic Theory, 1930–1955. Studies in Linguistic Analysis, pages 1–32, 1957), одна из краеугольных концепций в области обработки текстов. 
Для машинного анализа представим смысл слова как вектор в пространстве в том понимании, что близкие по контексту слова имеют близкие (в метрическом пространстве) векторные представления. Такие векторные представления называют эмбеддингами слов (далее — просто эмбеддингами). Аналогично можно определить эмбеддинги предложений, документов, параграфов и разных других лингвистических единиц, но для дальнейшего повествования интерес представляют лишь эмбеддинги слов. 
Чем хороши эмбеддинги слов fastText
Один из наиболее популярных способов получать эмбеддинги слов — техника fastText, предложенная Facebook AI Research аж в 2016 году (сайт проекта, оригинальная статья). 

Популярной она стала не просто так. На момент её разработки уже существовали разные способы получения эмбеддингов слов, самым известным из которых был Word2vec говорящее название от «Гугла» (сайт проекта, оригинальная статья, с картинками на Хабре). Однако существующие тогда решения недостаточно эффективно работали со словами, которых не было в обучающих корпусах (т. н. Out-of-Vocabulary words, OOV), и fastText очень элегантно решил эту проблему с приемлемым ущербом для времени обучения и инференса. 
Исследователи из Facebook предположили, что для получения эмбеддинга слова имеет смысл учесть не только контекст, но и структуру на символьном уровне. Для этого они выбрали концепцию эмбеддингов символьных n-грамм, которую можно проиллюстрировать простым примером. 
Возьмём слово рояль. Добавим к нему открывающий и закрывающий теги и получим <рояль>. Давайте получим n-граммы, причём пусть n_min=2, а n_max=4:

Идея заключается в том, чтобы обучать векторы именно n-грамм (а не слов, как в Word2vec), а эмбеддинг целого слова получать как взвешенную сумму векторов n-грамм. Неизвестные слова будут содержать известные n-граммы и таким образом получат адекватные эмбеддинги. Важно уточнить, что n_min и n_max задаются при обучении и их тоже желательно выбирать с умом в зависимости от особенностей языка. 
Для быстрого получения эмбеддингов векторов необходимо, чтобы в момент инференса векторы n-граммы уже были в оперативной памяти. Проблема в том, что уникальных n-грамм может быть сколько угодно и их количество также будет связано с n_min и n_max, а тогда количество требуемой памяти для хранения n-грамм должно быть очень большим. Это ненадёжно и опасно. 
Разработчики fastText учли и это, поэтому используют хеширование FNV-1a, которое ставит в соответствие n-грамме натуральное число от 1 до задаваемого при обучении числа bucket (по умолчанию bucket=2*10^6). Таким образом модель fastText ограничивает потребление памяти тем, что в ней не может быть векторов n-грамм больше, чем значение bucket. Тут у искушённого читателя сразу возникнет вопрос о коллизиях при подобном хешировании, но давайте не будем забегать вперёд. 
Итак, я постарался крайне поверхностно, но сущностно рассказать о том, что представляет из себя fastText, а теперь время продолжить историю.
Заглянем внутрь модели
Рассматривать будем содержимое модели аварского языка (современного), которая была обучена на аварской «Википедии». За основу возьмём пример Андрея Васнецова, то есть будем использовать библиотеку Gensim. 
Итак, наша модель занимает 2,24 Гб (2_410_222_502 B). Загрузим её через Gensim и посмотрим параметры:

Вообще-то в модели хранятся не только векторы n-грамм, но и векторы слов (они однократно вычисляются из векторов n-грамм, чтобы их не пересчитывать каждый раз). Оба набора векторов хранятся в виде матриц соответствующих размеров, где каждая строка — это вектор, соответствующий слову/n-грамме. Размерность векторов — 300, и так как корпус текстов был настолько невелик, что в словаре оказалось всего лишь 4217 слов, то матрица векторов слов имеет размер (4217, 300). Так как bucket = 2_000_000, то и матрица n-грамм ожидаемо будет 2_000_000, 300. 
Если в матрице векторов слов каждый вектор соответствует слову, то как же ставятся в соответствие n-граммы и их векторы? Как нетрудно догадаться, каждой n-грамме посредством хеширования ставится в соответствие натуральное число, которое и является номером строки в матрице n-грамм, содержащим соответствующий вектор n-граммы. Причём мы храним всю матрицу n-грамм целиком, даже если там есть нулевые строки. 
И разве нормально для такого маленького словаря хранить такую большую матрицу n-грамм? Хорошо ли это? Можно ли как-то её сократить? Посмотрим на саму матрицу n-грамм и решим, есть ли там лишние строки. 

Из двух миллионов векторов ненулевыми оказывается меньше трети — 613 408. Но даже это число кажется слишком большим для такого маленького словаря. Посмотрим на оставшиеся ненулевые векторы, а именно на их L2-нормы. 
Если построить гистограмму L2-норм ненулевых векторов (рис. 1), то можно увидеть, что подавляющее большинство из них имеет норму, близкую к нулю. Гистограмма построена в двух смежных отрезках, чтобы контуры хоть как-то различались. 
Это означает, что векторы в левой части гистограммы (которые близки к нулю) вряд ли вносят какой-то существенный вклад в результирующий вектор, так как эмбеддинг слова формируется как взвешенная сумма векторов входящих в него n-грамм. К тому же форма левой части гистограммы подозрительно напоминает нормальное распределение, что, вероятно, говорит об отсутствии информативности. 

Рис. 1. Распределение L2-норм векторов n-грамм. Диапазон оси абсцисс [0.03; 0,037] на левом графике и от [0.37; 6] на правом. Следует обратить внимание на порядок величин по оси ординат.
Чтобы проверить предположение об отсутствии полезной информации в этих векторах, посмотрим на распределение значений компонент-векторов (рис. 2). Гистограмма значений компонент, так же как и предыдущая, построена для трёх смежных отрезков по той же причине. 

Рис. 2. Распределение значений компонент в векторах n-грамм
Судя по всему, та самая бо