

DialoGPT на русском / Habr


              30  March  2021 at 20:36  DialoGPT на русском ICL Services corporate blog Python *Algorithms *Machine learning *Artificial Intelligence       Кадр из фильма "Я, робот"Всем привет. В конце 2019 года вышла одна из работ по GPT-2. Инженеры из Microsoft обучили стандартную GPT-2 вести диалог. Тогда, прочитав их статью, я очень впечатлился и поставил себе цель обучить такую же модель, но уже на русском языке.Время шло, и через год Сбер сделал очень хорошее дело, выложив в открытый доступ несколько небольших вариантов моделей GPT-3, обученных на русском языке.Итак, когда все звёзды сошлись, потратив пару месяцев ночей на конструирование, обработку и очистку датасета, наконец-то обучил модель GPT-3 medium от Сбера вести диалоги. Большое спасибо DevAlone, за то что создал проект Pikastat, без которого потратил бы годы на сбор данных. Модель обучал 2 эпохи на 32 ГБ текста на библиотеке Transformers от Hugging Face. Обучение шло 12 дней на 4x RTX 2080 Ti.Выражаю благодарность компании ICL Services, которая предоставила вычислительные мощности для обучения модели.Датасет состоит из цепочек комментариев (~92% Pikabu, ~6% YouTube, ~2% VK). Количество сэмплов для обучения - 41 миллион. От ненормативной лексики датасет специально не очищал, поэтому будьте готовы к "неожиданным" ответам.Ниже привожу примеры диалогов с обученной моделью. Там, где ответы от GPT длинные (свыше 50 токенов), выбирал наилучший среди трёх сгенерированных ответов. Также на построение каждого диалога были две попытки. В остальном, ничего не изменял и не подстраивал.На скриншотах реплики от GPT производила полностью модель. А потому "все имена и события вымышлены, любые совпадения случайны".Посмотреть еще больше диалоговПосмотреть неудачные (стандартные проблемы диалоговых систем и моделей)Среднее количество реплик при обучении –  4, потому длинные диалоги (от десяти и более реплик) модель воспринимает тяжело. Кроме того, необходимо помнить, что длина последовательности модели – 256 токенов.На валидации был только лосс. Поэтому, чтобы самому оценивать качество модели "на глаз", собрал web-приложение. С JavaScript особо не дружу, но реализовал минимальный функционал, который задумывал. Репозиторий приложения лежит тут - там же инструкция для запуска.Приложение состоит из двух частей:Сам сайт на Flask (Python)Сервис-генератор на FastAPI (Python)После запуска сервиса и приложения взаимодействие будет выглядить примерно так:Качество ответов сильно зависит от параметров генерации (тут хорошая вводная статья с объяснением параметров). При установке генерации нужной длины ("Length generate" в блоке слева), некоторые параметры также следует изменить для лучшей генерации. Сейчас как раз работаю над их оптимизацией, а также работаю над классификатором, который будет отбирать среди длинных ответов наиболее "удачный".Теперь посмотрим в каком виде строка кодируется в модель:Здесь зеленым цветом 0 либо 1, это speaker id. Он показывает какая реплика кому принадлежит. Красным выделен параметр, отвечающий за длину генерации, который принимает следующие значения: [ -, 1, 2, 3]."-" - означает, что мы не ожидаем от модели какой-то конкретной длины генерации"1" - модель будет стараться сгенерировать короткий ответ. При обучении диапазон был до 15 токенов"2" - модель будет стараться сгенерировать средний ответ. При обучении диапазон был от 15 до 50 токенов"3" - модель будет стараться сгенерировать длинный ответ. При обучении диапазон был от 50 до 256 токеновВ данном примере мы ожидаем от системы "длинный" ответ на вопрос "Что нового?"Очень рекомендую пообщаться с моделью самим. Делать открытый сайт для общения не стал, так как генерация довольно ресурсоемкая задача. Когда снимал чекпоинты и общался с ней, давно не испытывал таких эмоций: и смешит, и печаль наводит, и ругается. В общем, все как надо для современной молодежи модели. Также приметил, что неплохо отвечает на философские вопросы.Но и минусов в данной версии модели тоже достаточно: обучение на небольшом датасете (41 миллион, у Microsoft было 147), генерация плохого качества длинных ответов, выдача "размытых" ответов, недообученность модели, плохой "симбиоз" с весами от Сбера.Ну и напоследок: в детстве мне очень нравился фильм "Я робот". Давайте посмотрим, как отвечает модель на вопросы от детектива Спунера:Видно, что до уровня Санни еще очень далеко. И это хорошо, так как есть куда стремиться.В дальнейшем постараюсь улучшать данную модель и выкладывать обновленные версии. До новых встреч!    Tags: gptgpt-2gpt-3chatbotsdialogptneural networksicl services Hubs: ICL Services corporate blogPythonAlgorithmsMachine learningArtificial Intelligence          


