

Google обучила языковую модель ИИ на триллионе параметров / Habr


              14  January  2021 at 12:29  Google обучила языковую модель ИИ на триллионе параметров Machine learning *Artificial Intelligence Natural Language Processing *      


Исследователи Google разработали метод, который, по их утверждению, позволил обучить языковую модель, содержащую более триллиона параметров. Они заявили, что новая модель с 1,6 трлн параметров, по-видимому, является крупнейшей в своем классе на сегодняшний день. 


Данная модель достигла ускорения в 4 раза по сравнению с более ранней, самой крупной языковой моделью Google T5-XXL. Исследователи применили Switch Transformer, метод «редко активируемого», который использует только подмножество весов модели или параметры, которые преобразуют входные данные в модели.


Концепция Switch Transformer состоит в том, чтобы объединить несколько моделей, специализирующихся на различных задачах, внутри более крупной и встроить «стробирующую сеть», выбирающую, к какой из этих моделей обращаться в конкретном случае.


Switch Transformer использует графические процессоры и блоки тензорной обработки (TPU) Google. В распределенной системе обучения модели распределяют уникальные веса по разным устройствам, поэтому сохраняется управляемая память и вычислительные ресурсы на каждом устройстве.


В ходе эксперимента исследователи предварительно обучили несколько различных моделей Switch Transformer, используя 32 ядра TPU на Colossal Clean Crawled Corpus, наборе данных размером 750 ГБ, взятом из Reddit, Wikipedia и других веб-источников. Они поставили перед моделями задачу предсказывать пропущенные слова в отрывках, где 15% слов были замаскированы, а также решать другие задачи, такие как поиск текста для ответа на список из все более сложных вопросов.


Исследователи утверждают, что их модель с 2048 внутренних моделей Switch-C показала «отсутствие обучающей нестабильности вообще» в отличие от предшественника Switch-XXL, содержащей 395 млрд параметров и 64 внутренних модели. 


Предварительное обучение модели удалось ускорить более чем в 7 раз при использовании того же количества вычислительных ресурсов. 


В будущей работе исследователи планируют применить Switch Transformer к работе с изображениями и текстом. Они считают, что разреженность моделей может дать преимущества в целом ряде различных сред, а также в мультимодальных моделях.


К сожалению, в работе исследователей не учитывалось влияние крупных языковых моделей на реальный мир, так как они часто отражают предубеждения, закодированные в общедоступных данных. Исследовательская компания ИИ OpenAI отмечает, что это может привести к размещению таких слов, как «непослушный» рядом с женскими местоимениями и «ислам» рядом с такими словами, как «терроризм». 


Другие исследования от Intel, MIT и канадской инициативы CIFAR в области искусственного интеллекта уже обнаружили высокий уровень стереотипных предубеждений в некоторых популярных моделях, включая Google BERT и XLNet, OpenAI GPT-2 и Facebook RoBERTa.


Уже зафиксировано несколько сомнительных эпизодов применения новой языковой модели GPT-3 от OpenAI. Осенью прошлого года на Reddit появился блог, который фактически вел GPT-3. Он отвечал на вопросы о самоубийствах, домогательствах, теориях заговора, иммиграции, расизме и другие.


А исследователи из французской компании Nabla использовали систему генерации текста для создания медицинского чат-бота. Во время имитационного сеанса с пациентом бот посоветовал ему убить себя.    Tags: google aiязыковые моделимашинное обучениеискусственный интеллектnlp (natural language processing) Hubs: Machine learningArtificial IntelligenceNatural Language Processing          


