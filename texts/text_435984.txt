

Нейросети и философия языка / Habr


              14  January  2019 at 11:13  Нейросети и философия языка Python *Algorithms *Machine learning *Popular science Artificial Intelligence  
        Translation
         
                Original author:
                
                  Massimo Belloni
                  Почему теории Витгенштейна остаются основой всего современного NLP 


Векторное представление слов — пожалуй, одна из самых красивых и романтичных идей в истории искусственного интеллекта. Философия языка — это раздел философии, исследующий связь между языком и реальностью и как сделать сделать речь осмысленной и понятной. А векторное представление слов — очень специфический метод в современной обработке естественного языка (Natural Language Processing, NLP). В некотором смысле он представляет собой эмпирическое доказательство теорий Людвига Витгенштейна, одного из самых актуальных философов прошлого века. Для Витгенштейна использование слов — это ход в социальной языковой игре, в которую играют члены сообщества, понимающие друг друга. Значение слова зависит только от его полезности в контексте, оно не соотносится один к одному с объектом из реального мира.

Для большого класса случаев, в которых мы используем слово «значение», его можно определить как значение слова есть его использование в языке.

Конечно, понять точное значение слова очень сложно. Следует учесть множество аспектов: 


к какому объекту слово, возможно, относится;
какая это часть речи;
является ли оно идиоматическим выражением;
все оттенки смыслов;
и так далее.


Все эти аспекты, в конце концов, сводятся к одному: знать, как использовать слово.


Концепция значения и почему упорядоченный набор символов имеет определённую коннотацию в языке — это не только философский вопрос, но и, вероятно, самая большая проблема, с которой приходится сталкиваться специалистам по ИИ, работающим с NLP. Русскоязычному человеку вполне очевидно, что «собака» — это «животное», и она больше похожа на «кошку», чем на «дельфина», но эта задача далеко не проста для систематического решения.


Немного подкорректировав теории Витгенштейна, можно сказать, что собаки похожи на кошек, потому что они часто появляются в одних и тех же контекстах: скорее можно найти собак и кошек, связанных со словами «дом» и «сад», чем со словами «море» и «океан». Именно эта интуиция заложена в основу Word2Vec, одной из самых известных и успешных реализаций векторного представления слов. Сегодня машины далеки от действительного понимания длинных текстов и отрывков, но векторное представление слов — бесспорно, единственный метод, который позволил сделать самый большой шаг в этом направлении за последнее десятилетие.

От BoW к Word2Vec

Во многих компьютерных задачах первая проблема — представить данные в числовой форме; слова и предложения, вероятно, сложнее всего представить в таком виде. В нашей настройке из словаря выбирается D слов, а каждому слову может быть присвоен числовой индекс i.


В течение многих десятилетий был принят классический подход представлять каждое слово как числовой D-мерный вектор из всех нулей, кроме единицы в позиции i. В качестве примера рассмотрим словарь из трёх слов: «собака», «кошка» и «дельфин» (D=3). Каждое слово может быть представлено в виде трёхмерного вектора: «собака» соответствует [1,0,0], «кошка» — [0,1,0], а «дельфин», очевидно, [0,0,1]. Документ можно представить в виде D-мерного вектора, где каждый элемент считает вхождения i-го слова в документе. Эта модель называется Bag-of-words (BoW), и она использовалась десятилетиями.


Несмотря на свой успех в 90-е годы, у BoW отсутствовала единственная интересная функция слов: их смысл. Мы знаем, что два очень разных слова могут иметь схожие значения, даже если они полностью отличаются с орфографической точки зрения. «Кошка» и «собака» оба представляют собой домашних животных, «король» и «королева» близки друг к другу, «яблоко» и «сигарета» совершенно не связаны. Мы это знаем, но в модели BoW все эти слова находятся на одинаковом расстоянии в векторном пространстве: 1. 


Та же проблема распространяется на документы: с помощью BoW можно сделать вывод о схожести документов только в том случае, если они содержат одно и то же слово определённое количество раз. И вот тут приходит Word2Vec, привнося в термины машинного обучения много философских вопросов, над которыми рассуждал Витгенштейн в своих «Философских исследованиях» 60 лет назад.


В словаре размером D, где слово идентифицируется по его индексу, цель состоит в вычислении N-мерного векторного представления каждого слова при N<<D. В идеале мы хотим, чтобы это был плотный вектор, представляющий некоторые семантически-специфические аспекты значения. Например, мы в идеале хотим, чтобы «собака» и «кошка» имели схожие представления, а «яблоко» и «сигарета» — очень далёкие в векторном пространстве. 


Мы хотим выполнять некоторые основные алгебраические операции над векторами, такие как король+женщина