

Web2Text: глубокое структурированное извлечение содержимого web-страницы / Habr


              24  March  2020 at 15:48  Web2Text: глубокое структурированное извлечение содержимого web-страницы Data Mining *Machine learning * 
        Sandbox
           Привет, Хабр! Представляю вашему вниманию перевод статьи "Web2Text: Deep Structured Boilerplate Removal" коллектива авторов Thijs Vogels, Octavian-Eugen Ganea и Carsten Eickhof.
Веб-страницы являются ценным источником информации для многих задач обработки естественного языка и поиска информации. Эффективное извлечение основного содержимого из этих документов имеет важное значение для производительности производных приложений. Чтобы решить эту проблему, мы представляем новую модель, которая выполняет классификацию и маркировку текстовых блоков на странице HTML как шаблонных блоков, или блоков содержащих основной контент. Наш метод использует Скрытую Марковскую модель поверх потенциалов, полученных из признаков объектной модели HTML-документа (Document Object Model, DOM) с использованием сверточных нейронных сетей (Convolutional Neural Network, CNN). Предложенный метод качественно повышает производительность для извлечения текстовых данных из веб-страниц.
1. Введение
Современные методы обработки естественного языка и поиска информации сильно зависят от больших коллекций текста. Всемирная паутина — неиссякаемый источник контента для таких приложений. Тем не менее, общая проблема заключается в том, что веб-страницы включают в себя не только основной контент (текст), но также рекламу, списки гиперссылок, навигацию, превью других статей, баннеры и т.д. Этот шаблонный контент часто оказывает негативное влияние на производительность производного приложения [15,24]. Задача отделения основного текста на веб-странице от остального (шаблонного) содержимого в литературе известна как «удаление стандартного шаблона», «сегментация веб-страницы» или «извлечение содержимого». Известные популярные методы для этой проблемы используют алгоритмы на основе правил или машинного обучения. Наиболее успешные подходы сначала выполняют разбиение входной веб-страницы на текстовые блоки, а затем двоичную {1, 0} маркировку каждого блока в качестве основного содержимого или шаблона. В этой статье мы предлагаем Скрытую Марковскую модель поверх нейронных потенциалов для задачи удаления шаблонов. Мы используем способность сверточных нейронных сетей для изучения унарных и парных потенциалов по блокам на основе сложных нелинейных комбинаций признаков на основе DOM. Во время прогнозирования мы находим наиболее вероятную метку блока {1, 0}, максимизируя совместную вероятность последовательности меток с использованием алгоритма Витерби [23]. Эффективность нашего метода продемонстрирована на стандартных наборах сравнительных данных.
Остальная часть этого документа структурирована следующим образом. Раздел 2 дает обзор связанных с тематикой работ различных авторов. Раздел 3 формально определяет проблему извлечения основного контента, описывает процедуру сегментации блоков данных и детализирует нашу модель. Раздел 4 демонстрирует достоинства нашего метода на нескольких эталонных наборах данных для извлечения контента из веб-страниц.
2. Обзор связанных работ
Ранние подходы к удалению шаблонного HTML-кода используют ряд эвристических и основанных на правилах методов [7] под названием Body Text Extractor (BTE). BTE основан на наблюдении, что основной контент содержит более длинные абзацы непрерывного текста, где HTML-теги встречаются реже по сравнению с остальной частью веб-страницы. Рассматривая совокупное распределение тегов в зависимости от позиции в документе, BTE идентифицирует плоскую область в середине этого графика распределения, чтобы предсказать основное содержимое страницы. Несмотря на простоту, этот алгоритм имеет два недостатка: (1) он использует только местоположение тегов HTML, а не их структуру, тем самым теряя потенциально ценную информацию, и (2) он может идентифицировать только один непрерывный участок основного контента, что нереально для значительного процента современных веб-страниц.
Для решения этих проблем было разработано несколько других алгоритмов для работы с деревьями DOM, что позволяет использовать семантику структуры HTML [11,19,6]. Проблема этих ранних методов заключается в том, что они интенсивно используют тот факт, что страницы раньше были разделены на разделы с помощью тегов <table>, что больше не является допустимым предположением.
На следующем этапе работы структура DOM используется для совместной обработки нескольких страниц из одного домена с учетом их структурного сходства. Этот подход был впервые предложен Юи и соавт. [24] и был улучшен различными другими авторами [22]. Эти методы очень подходят для обнаружения содержимого шаблона, присутствующего на всех страницах веб-сайта, но имеют низкую производительность на веб-сайтах, которые состоят только из одной веб-страницы. В этой статье мы сосредоточимся на извлечении одностраничного контента без использования контекста других страниц того же сайта.
Готтрон и соавт. [10] предлагают методы сглаживания кода контента, которые могут идентифицировать несколько областей контента. Эти методы анализируют исходный код HTML как вектор единиц, представляющих фрагменты текста, и нулей, представляющих теги. Затем этот вектор итеративно сглаживается, так что в конечном итоге он находит активные области, где доминирует текст (контент), и неактивные области, где доминируют теги (шаблон). Эта идея сглаживания была распространена также на структуру DOM [4,21]. Чакрабарти и соавт. [3] назначают вероятность содержания для каждого листа дерева DOM, используя изотоническое сглаживание, чтобы объединить вероятности меток соседей с теми же родителями. В аналогичном направлении Сан и соавт. [21] использует как соотношение тегов / текста, так и информацию о дереве DOM для распространения сумм плотности вероятности по дереву.
Методы машинного обучения предлагают удобный способ комбинирования различных показателей «наполненности», автоматически взвешивая созданные вручную признаки в соответствии с их относительной важностью. Система FIASCO от Бауэр и соавт. [2] использует машины опорных векторов (SVM) для классификации HTML-страницы как последовательности блоков, которые генерируются посредством сегментации страницы на основе DOM и представлены лингвистическими, структурными и визуальными признаками. Работы Колшаттер и соавт. [17] также используют SVM для независимой классификации блоков. Споста и соавт. [20] расширяют этот подход, переформулируя проблему классификации как случай маркировки последовательности, где все блоки помечены совместно. Они используют условные случайные поля, чтобы воспользоваться корреляциями между метками соседних блоков контента. Этот метод был самым успешным в конкурсе CleanEval [1].

Рис. 1. Конвейер Web2Text. Листья свернутого дерева DOM (Collapsed DOM) веб-страницы образуют упорядоченную последовательность блоков, которые нужно пометить. Для каждого блока мы извлекаем ряд признаков на основе дерева DOM. Две отдельные сверточные сети, работающие на этой последовательности признаков, дают два соответствующих набора потенциалов: унарные потенциалы для каждого блока и парные потенциалы для каждой пары соседних блоков. Они определяют Скрытую Марковскую модель. Используя алгоритм Витерби, мы находим оптимальную маркировку, которая максимизирует общую вероятность последовательности, предсказанную нейронными сетями.
В этой статье мы предлагаем эффективный метод получения признаков, которые собираются от ближайших соседей в дереве DOM. Кроме того, мы используем систему глубокого обучения для автоматического изучения комбинаций нелинейных функций, что дает модели преимущество перед традиционными линейными подходами. Наконец, мы совместно оптимизируем метки для всей веб-страницы в соответствии с локальными потенциалами, предсказанными нейронными сетями.
3. Веб в текст
Удаление шаблонов — это проблема маркировки разделов текста веб-страницы как основного содержимого или шаблонного элемента (чего-либо еще) [1]. Далее мы обсудим различные этапы нашего метода. Полный конвейер также показан на рисунке 1.
3.1. Предварительная обработка
Мы ожидаем, что ввод исходной веб-страницы будет записан в (X) HTML-разметке. Каждый документ анализируется как дерево объектной модели документа (дерево DOM) с помощью Jsoup [12].

Рис. 2. Пример свернутой модели DOM. Слева: исходный код HTML, посередине — соответствующее дерево DOM, справа — соответствующий свернутый DOM.
Мы предварительно обрабатываем это дерево DOM, i) удаляя пустые узлы или узлы, содержащие только пробелы, ii) удаляя узлы, которые не имеют никакого содержимого, которое мы можем извлечь: например, <br>, <checkbox>, <head>, <hr>, <iframe>, <img>, <input>. Мы используем отношения родительского и прародительского дерева DOM. Однако в необработанном DOM-дереве эти отношения не всегда имеют смысл. На рисунке 2 показан типичный фрагмент дерева DOM, в котором два соседних узла имеют общий семантический родительский элемент (<ul>), но не один и тот же родительский DOM. Чтобы улучшить выразительность признаков на основе дерева (например, «количество дочерних элементов родительского узла»), мы рекурсивно объединяем отдельные дочерние родительские узлы с их соответствующими дочерними. Мы называем полученную древовидную структуру Collapsed (свернутый) DOM (CDOM).
3.2. Сегментация блоков
Наш алгоритм извлечения контента основан на маркировке последовательности. Веб-страница рассматривается как последовательность блоков, которые помечены как основной контент, или как шаблон. Существует несколько способов разбить веб-страницу на блоки, наиболее популярными в настоящее время являются: i) строки в файле HTML, ii) листья DOM, iii) листья DOM уровня блока. Мы выбираем наиболее гибкую стратегию листьев DOM, описанную ниже. Разделы на странице, для которых требуются разные метки, обычно разделены хотя бы одним тегом HTML. Следовательно, можно считать DOM-листья (узлы #text) блоками нашей последовательности. Потенциальным недостатком этого подхода является то, что гиперссылка в текстовом абзаце может получить метку, отличную от соседнего текста. Однако согласно этой схеме, эмпирическая оценка Web2Text не показывает случаев, когда части текстового абзаца ошибочно помечаются как шаблонные, а остальные — как основной контент.
3.3. Извлечение признаков
Признаки — это свойства узла, которые могут указывать на то, что это содержимое страницы, или шаблон. Такие признаки могут основываться на тексте узла, структуре CDOM или их комбинации. Мы различаем блочные признаки и граничные признаки.
Блочные признаки захватывают информацию о каждом блоке текста на странице. Они представляют собой статистические данные, собранные на основе узла CDOM, родительского узла, родительского узла и корня дерева CDOM. Всего мы собираем 128 признаков для каждого текстового блока, например, «Узел - это элемент <p>», «средняя длина слова», «относительная позиция в исходном коде», «текст родительского узла содержит адрес электронной почты», «соотношение стоп-слов на всей странице» и т.д. Далее, все не двоичные характеристики преобразуются и стандартизируются, чтобы они были приблизительно гауссовскими с нулевым средним и единичной дисперсией в обучающем наборе.
Граничные признаки захватывают информацию о каждой паре соседних текстовых блоков. Мы собираем 25 признаков для каждой такой пары. Расстояние до двух узлов в дереве определяется как сумма количества уровней от обоих узлов до их первого общего предка. Первыми граничными признаками, которые мы используем, являются бинарные объекты, соответствующие количеству уровней дерева 2, 3, 4 и> 4. Другой признак указывает, есть ли разрыв строки между узлами в не стилизованной HTML-странице, и т.д.
3.4. Унарные и парные потенциалы сверточной нейронной сети (Convolutional Neural Network, CNN)
Мы назначаем одинарные потенциалы для каждого текстового блока, который должен быть помечен, и попарные потенциалы для каждой пары соседних текстовых блоков. В нашем случае потенциалы являются вероятностями, как объяснено ниже. Унарные потенциалы pi (li = 1), pi (li = 0) представляют собой вероятности того, что метка li текстового блока i является содержимым или шаблоном, соответственно. Два потенциала суммируются в один. Парные потенциалы pi, i + 1 (li = 1, li + 1 = 1), pi, i + 1 (li = 1, li + 1 = 0), pi, i + 1 (li = 0, li + 1 = 1) и pi, i + 1 (li = 0, li + 1 = 0) — вероятности перехода меток пары соседних текстовых блоков. Эти попарные потенциалы также суммируют по одному для каждой пары текстовых блоков.
Два набора потенциалов моделируются с использованием CNN с 5 слоями, нелинейностью ReLU между слоями, размерами фильтров (50, 50, 50, 10, 2) для унарной сети и (50, 50, 50, 10, 4) для парной сети. Все фильтры имеют шаг 1 и размеры ядра (1, 1, 3, 3, 3) соответственно. Унарная CNN принимает последовательность атрибутов блока, соответствующую последовательности текстовых блоков, которые должны быть помечены, и выводит унарные потенциалы для каждого блока. Парная CNN получает последовательность границ текста, соответствующую последовательности границ, которые должны быть помечены, и выводит парные потенциалы для каждого блока. Мы используем заполнение нулями, чтобы каждый слой создавал последовательность того же размера, что и его входная последовательность. Выходы для унарной сети представляют собой последовательности по 2 значения на блок, которые нормализуются с помощью функции softmax. Выходными данными для парной сети являются последовательности из 4 значений на пару блоков, которые нормируются аналогичным образом. Таким образом, выход для блока i косвенно зависит от диапазона блоков вокруг него. Мы используем регуляризацию отсева (dropout) со скоростью обучения 0,2 и L2 регуляризацию со скоростью 10-4.
Для унарных потенциалов мы минимизируем кросс-энтропию:

где l