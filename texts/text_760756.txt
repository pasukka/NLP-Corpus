

Есть один нюанс: как мы спасаем нейросети от классификации неоднозначных текстов / Habr


               Есть один нюанс: как мы спасаем нейросети от классификации неоднозначных текстов Level of difficulty  
    Hard
   Reading time  
    7 min
   Views  1.1K AIRI corporate blog Algorithms *Machine learning *Artificial Intelligence Natural Language Processing * 
    Case
        Всем привет! Меня зовут Артём Важенцев, я аспирант в Сколтехе и младший научный сотрудник AIRI. Я работаю в группе под руководством Александра Панченко и Артёма Шелманова. Мы занимаемся исследованием и разработкой новых методов оценивания неопределенности для языковых моделей. Этим летом мы представили две статьи на конференции ACL 2023. В одной из них мы описали новый гибридный метод оценивания неопределенности для задачи выборочной классификации текстов для данных с неоднозначными примерами — его внедрение поможет нейросетям лучше находить токсичность в комментариях или угадывать тональность сообщений. В этом тексте я бы хотел рассказать подробнее о нашем методе и процессе его разработки.  http://stock.adobe.com/ ВведениеТоксичность в интернете — вещь обыденная, и, похоже, неискоренимая. Невозможно заставить всех пользователей быть добрее или приставить к каждому из них надзирателя, который бы “следил за базаром”. Единственный выход для тех, кто хотел бы ощущать себя в интернете уютнее — это фильтрация, которая невозможна без классификации текста. Сегодня задачи классификации и им подобные решаются автоматически на основе предсказаний нейросети, однако даже самые продвинутые модели склонны совершать ошибки. Чтобы отлавливать такие ошибки существуют  методы оценивания неопределенности. Они пытаются понять насколько модель не уверена в своем предсказании. Полученные оценки неопределенности для хорошего метода должны коррелировать с ошибками модели, т.е. для примеров на которых модель ошибается должна быть высокая неопределенность, для корректно классифицированных — низкая. Вскоре стало ясно, что многие современные методы оценивания неопределенности могут показывать плохой результат на неоднозначных задачах — определение токсичности текста относится к их числу. Это мотивировало меня и моих коллег из нескольких научных институтов, занимающихся проблемами искусственного интеллекта, разработать более надежный метод, который будет показывать хороший результат даже в сложных случаях. Задача, в которой есть возможности отказаться от классификации для части примеров, называется выборочной классификацией. В такой задаче наиболее неопределенные предсказания отклоняются, предполагая, что модель могла допустить ошибку, а только самые уверенные предсказания сохраняются. Таким образом, на сохраненной части мы можем значительно улучшить качество классификации. Отклоненные же примеры, для которых модель неопределенна, могут быть дальше отправлены на разметку человеку — эксперту в области, который может более корректно классифицировать данный пример. Также в некоторых системах можно выдать это предсказание пользователю вместе с оценкой неопределенности, чтобы более корректно интерпретировать данное предсказание модели.Как известно, ошибки могут возникать из двух источников: примеры вне распределения обучающих данных (out-of-distribution, OOD) и сложные примеры (ambiguous in distribution, AID), с которыми модель не может справится корректно. В первом случае хорошо работают методы оценивания эпистемической неопределенности, которая возникает из-за недостатка данных, во втором — методы оценивания алеаторной неопределенности, которая возникает из-за шума и неоднозначности в данных. Таким образом, возникла идея придумать способ, как объединить два этих базовых метода под каждый тип неопределенности и получить более надежный метод. Два типа ошибок, возникающие в задачах классификацииОпределение неопределенностиВ приложении математической статистики к машинному обучению неопределенность U(x) вводят как функцию на множестве данных (или примеров) 