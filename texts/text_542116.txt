

Генерация текста с помощью GPT2 и PyTorch / Habr


              12  February  2021 at 14:49  Генерация текста с помощью GPT2 и PyTorch OTUS corporate blog Python *Machine learning *Artificial Intelligence  
        Translation
         
                Original author:
                
                  Raymond Cheng
                  Быстрая и легкая генерация текста на любом языке с помощью фреймворка HuggingfaceВ рамках курса «Machine Learning. Advanced» подготовили перевод интересного материала. Также приглашаем принять участие в открытом вебинаре на тему «Multi-armed bandits для оптимизации AB тестирования». На вебинаре участники вместе с экспертом разберут один из самых эффективных вариантов применения обучения с подкреплением, а также рассмотрят, как можно переформулировать задачу АБ тестирования в задачу байесовского вывода.ВведениеГенерация текста — одна из самых захватывающих прикладных задач обработки естественного языка (Natural Language Processing - NLP) за последние годы. Большинство из нас, вероятно, слышали о GPT-3, мощной языковой модели, которая может генерировать тексты, близкие к написанным человеком. Однако такие модели чрезвычайно трудно обучать из-за их большого размера, поэтому предварительно обученные модели обычно предпочтительнее там, где это приемлемо.В этой статье мы научим вас генерировать текст с помощью предварительно обученного GPT-2 — более легкого предшественника GPT-3. Мы будем использовать именитую библиотеку Transformers, разработанную Huggingface. Если вы хотите узнать, как настроить GPT-2 на своем собственном наборе данных для генерации текста в конкретной предметной области, вы можете прочитать мою предыдущую статью: Настройка GPT2 для генерации текста с помощью PytorchЕсли предварительно обученной GPT-2 для ваших целей будет достаточно, то вы попали как раз туда, куда нужно! Без лишних отлагательств, приступим туториалу.План туториалаШаг 1: Устанавливаем библиотекуШаг 2: Импортируем библиотекуШаг 3: Создаем конвейер генерации текстаШаг 4: Определяем текст, с которого будет начинаться генерацияШага 5: Запускаем генерациюБОНУС: Генерируем текст на любом языкеШаг 1: Установка библиотекиДля установки Huggingface Transformers, нам нужно убедиться, что установлен PyTorch. Если вы не установили PyTorch, перейдите сначала на его официальный сайт и следуйте инструкциям по его установке.После установки PyTorch, вы можете установить Huggingface Transformers, запустив:pip install transformersШаг 2: Импорт библиотекиПосле успешной установки Transformers, вы можете импортировать его модуль pipeline:from transformers import pipelineМодуль pipeline является уровнем абстракции, который избавляет нас от написания сложного кода и позволяет легко выполнять различные задачи обработки естественного языка.Шаг 3: Создание конвейера генерации текстаТеперь мы можем приступить к созданию конвейера генерации текста. Мы можем сделать это следующим образом:text_generation = pipeline(“text-generation”)Модель по умолчанию для конвейера генерации текста — GPT-2, самая популярная модель декодирующего трансформера для генерации языка.Шаг 4: Определение текста, с которого начнется генерацияТеперь мы можем приступить к определению префиксного текста, на основе которого мы хотим генерировать новый текст. Давайте начнем с общего начального предложения:The world is
(Мир)
prefix_text = "The world is"Шаг 5: Запуск генерацииПосле того, как мы определили исходный текст, пора приступить к генерации! Мы можем сделать это, просто запустив:generated_text= text_generation(prefix_text, max_length=50, do_sample=False)[0]
print(generated_text[‘generated_text’])Приведенный выше код определяет max_length равный 50 знакам и отключает семплирование. Результат будет следующим:The world is a better place if you’re a good person.
(Мир вокруг тебя лучше, если ты хороший человек.)
I’m not saying that you should be a bad person. I’m saying that you should be a good person.
(Я не говорю, что ты должен быть плохим человеком. Я говорю, что ты должен быть хорошим человеком.)
I’m not saying that you should be a bad
(Я не говорю, что ты должен быть плохим.)Как видим, компьютер удивительным образом может генерировать текст, который имеет смысл, хотя и он не идеальный. Одна из проблем с выводом заключается в том, что он повторяется в конце. Возможно, это можно решить, используя разные схемы декодирования (например, top-k/top-p семплирование) и подбирая разные значения, но это уже выходит за рамки данной статьи. Чтобы узнать больше о схемах декодирования и как вы можете реализовать их, смотрите официальный туториал Huggingface и документацию TextGenerationPipeline.БОНУС: Генерация текста на любом языкеВо-первых, для создания текста на другом языке нам потребуется языковая модель, предварительно обученная на корпусе этого языка; в противном случае, нам придется настраивать ее самостоятельно, что является достаточно утомительной задачей. К счастью, Huggingface предоставляет список моделей, созданных прекрасным сообществом обработки естественного языка (ссылка здесь), и есть вероятность, что уже существует языковая модель предварительно настроенная на выбранный вами язык.Допустим, мы хотим создать текст на китайском языке. Эта модель GPT2 от CKIPLab предварительно обучена на китайском корпусе, поэтому мы можем использовать их модель без необходимости заниматься настройкой самостоятельно.Согласно их документации, мы можем начать с импорта соответствующих модулей токенизатора и модели:from transformers import BertTokenizerFast, AutoModelWithLMHeadЗатем следует построить токенизатор и модель соответственно:tokenizer = BertTokenizerFast.from_pretrained(‘bert-base-chinese’)
model = AutoModelWithLMHead.from_pretrained(‘ckiplab/gpt2-base-chinese’)Затем мы загружаем наш новый токенизатор и модель в качестве параметров для создания инстанса конвейера:text_generation = pipeline(“text-generation”, model=model, tokenizer=tokenizer)После этого, мы снова задаем наш префиксный текст, на этот раз на китайском языке: