

ИИ: имитация интеллекта, обман и реальные достижения / Habr


                4  August  2016 at 15:58  ИИ: имитация интеллекта, обман и реальные достижения VK corporate blog History of IT Popular science Artificial Intelligence Brain       


С каких пор программы научились выдавать себя за людей? Каким образом понять, искусная ли перед нами обманка или по-настоящему сильный ИИ? Когда программа справится с машинным переводом или напишет свой первый роман? Сергей  oulenspiegel Марков, автор материала «Играть на уровне бога: как ИИ научился побеждать человека», возвращается к теме умных машин в нашей новой нейронной статье. 

Умеют ли программы притворяться людьми

В конце 30-х годов прошлого века, когда еще не были созданы первые электронные вычислительные машины, вопросами «разумности» машин стали задаваться специалисты по computer science. Если нечто выглядит как кошка, мяукает как кошка, ведет себя как кошка, в любом эксперименте проявляет себя как кошка, то, наверное, это кошка. Эту идею сформулировал Альфред Айер — английский философ-неопозитивист, представитель аналитической философии. 


Всеми нами любимый Алан Тьюринг был более социализирован, чем Айер. Тьюринг любил ходить на вечеринки, а в то время среди интеллектуальной публики была распространена интересная забава — «Игра в имитацию». Заключалась игра в том, что девушку и парня запирали в две разные комнаты, оставляя под дверью широкую щель, в которую участники игры могли просовывать записки с вопросами. Человек, который находился в комнате, писал на вопросы какие-то ответы. Задачей игры было угадать, в какой комнате находится парень, а в какой — девушка. Тьюринг предположил следующее: «А давайте мы будем похожую процедуру использовать для того, чтобы понять, создали ли мы тот самый универсальный ИИ». 




Первая программа, которая могла общаться с человеком путем определенной переписки — это ELIZA, созданная в 1966 году. Программа пыталась выдать себя в эксперименте не просто за человека, а за психиатра. Ее стиль общения носит пародийный характер. То есть она говорит на специфическом психиатрическом жаргоне, задает соответствующие типовые вопросы. В принципе, эта программа представляет собой большой набор правил, по которому она находила определенные паттерны в речи человека. В ответ на наличие соответствующего паттерна входной информации, ELIZA определенным образом трансформировала его и выдавала информацию на выход. 


Программа могла в некоторых ситуациях людей одурачить. Был проведен эксперимент «AOLiza», когда программа общалась через сеть America Online со случайно выбранными пользователями, и многие из них не догадывались, что в данном случае с ними общалась машина. Понятно, что этот эксперимент нельзя считать хоть каким-то серьезным приближением к прохождению теста Тьюринга.


Cамо название программы ELIZA произошло от имени героини Бернарда Шоу из пьесы «Пигмалион», повествующей о том, как лондонский профессор фонетики Генри Хиггинс пытается из девушки, представительницы общественных низов, сделать настоящую леди, обучая ее правилам поведения и языку высшего общества.




Интересно, что вторая программа такого рода (чат-бот), созданная в 1972 году Кеннетом Колбаем из Стэнфордского университета, была в своем роде шуточной дискуссией с авторами программы ELIZA. Она называлась PARRY. Если ELIZA пыталась выдать себя за врача, то программа PARRY пыталась выдать себя за пациента. PARRY считала себя параноидальным шизофреником.


Интересно, что программа хорошо обманывала профессиональных психиатров. В экспериментах оказалось, что 48% психиатров не в состоянии понять, имеют ли они дело с человеком или с программой. Но здесь, опять же, никакой чистоты эксперимента не было. Если у человека существуют определенные дисфункции, то его поведение может в некоторых ситуациях быть достаточно автоматическим.




Интересная разработка тех времен, которая произвела очень большое впечатление – это программа SHRDLU. Название SHRDLU образовано от ETAOIN SHRDLU — так расположены алфавитные клавиши на линотипах (в соответствии с убыванием частоты появления букв в английском языке). Как, к примеру, сейчас ЙЦУКЕН и ФЫВАПРОЛДЖ.


У SHRDLU был небольшой мир, в котором было представлено ограниченное количество объектов. Это были пирамидки, различные кубики, коробка. И SHRDLU умела общаться на естественном языке с человеком, обсуждая то, что происходит в этом мире, и умела выполнять команды, которые отдавал на естественном языке человек. При этом лексика никак не была ограничена, то есть человек говорить что угодно, а программа могла достаточно свободно отвечать. Вот пример работы SHRDLU:

Человек: Что лежит в коробке?
Компьютер: Синяя пирамидка и синий блок.
Человек: На чем лежит пирамидка?
Компьютер: На коробке.
Человек: Как много блоков находится не в коробке?
Компьютер: Четыре.
Человек: Хотя бы один из них более тонкий, чем тот, про который я тебе говорил? На чем-то он стоит?
Компьютер: Да, стоит на столе.
Человек: Может ли стол собирать блоки?
Компьютер: Нет.
Человек: Может ли пирамидка стоять на блоке?
Компьютер: Да.
Человек: Может ли пирамидка стоять на пирамидке?
Компьютер: Не знаю.
Человек: Хорошо. Поставь пирамидку одну на другую.
Компьютер: Не получается.
Человек: Поставь, пожалуйста, два красных блока друг на друга. Либо зеленый куб, либо пирамидку. Сможешь?» И SHRDLU берет эти блоки, начинает ими оперировать.


Даже сейчас такого рода демонстрация выглядит впечатляющей. Интересно, что исходники SHRDLU можно скачать, программа написана на LISP, есть даже какой-то современный визуализатор под Windows. Если вы откроете ее исходники, то вы увидите, что программа состоит из огромного количества хитроумных правил. 


Когда читаешь эти правила, то понимаешь, насколько изощренная логика заложена в программу. Терри Виноград, по всей видимости, проводил много экспериментов, давая возможность разным людям общаться с этой системой. Мирок SHRDLU очень маленький: он может быть описан примерно 50 разными словами. И в рамках такого маленького пространства можно создать впечатление интеллектуального поведения у системы.

Что программы умеют сейчас

Однажды Тьюринга приперли к стене и прямо спросили: «Когда программы пройдут тесты?». Тьюринг предположил, что в 2000 году появятся машины, использующие 109 бит памяти, способные обманывать человека в 30% случаев. 




Интересно проверить, сбылся ли прогноз Тьюринга в 2016 году. Программа «Eugene Goostman» изображает из себя мальчика из Одессы. В первом тесте, состоявшемся в 2012 году, программа смогла обмануть судей в 20,2% случаев. В 2014 году в тесте эта же программа, уже модернизированная, в тестах, организованных Университетом Рединга, смогла обмануть судей в 33% случаев. Грубо говоря, с ошибкой плюс-минус 10 лет Тьюринг примерно попал в прогноз.


Потом появилась программа «Соня Гусева», и она в 2015 году смогла обмануть судей в 47% случаев. Стоит отметить, что процедура тестирования предполагает ограничение времени общения экспертов с программой (обычно около 5 минут), и в свете данного ограничения результаты уже не выглядят столь однозначными. Однако для решения многих практических задач, скажем, в области автоматизации SMM, этого более чем достаточно. Отличить продвинутого рекламного бота от человека на практике, скорее всего, не сможет большинство пользователей социальных сетей.


Наверное, самым известным и серьезным возражением на эти успехи является ответ философа Джона Сёрля, который предложил умственный эксперимент, названный «Китайская комната». Представим себе, что есть закрытая комната, в ней сидит человек. Мы знаем, что человек не понимает китайского языка, не сможет прочитать то, что написано китайскими иероглифами на бумаге. Но у нашего подопытного есть книга с правилами, в которой записано следующее: «Если у тебя на входе такие-то иероглифы, то ты должен взять вот такие иероглифы, и составить их в таком порядке». Он открывает эту книгу, она написана на английском, смотрит, что ему подали на вход, а дальше в соответствии с этими правилами формирует ответ, и скидывает его на выход. В определенной ситуации может показаться, что внутри комнаты находится человек, на самом деле понимающай китайский язык. Но ведь индивид внутри комнаты не знает китайского языка по постановке задачи. Получается, что когда эксперимент поставлен по канонам Тьюринга, он, на самом деле, не свидетельствует о том, что внутри сидит некто, понимающий китайский язык. Вокруг этого аргумента развернулась большая полемика. На него есть типовые возражения. Например, аргумент, что если сам Джон не понимает китайский язык, то вся система в целом, составленная из Джона и набора правил, уже обладает этим самым пониманием. До сих пор пишутся статьи в научной прессе на эту тему. Однако бо