

Обфускация данных для тестов производительности / Habr


                24  June  2019 at 12:17  Обфускация данных для тестов производительности Яндекс corporate blog Open source *Algorithms *Big Data *Machine learning *      Пользователи ClickHouse знают, что его главное преимущество — высокая скорость обработки аналитических запросов. Но как мы можем выдвигать такие утверждения? Это должно подтверждаться тестами производительности, которым можно доверять. О них мы сегодня и поговорим. 




Такие тесты мы начали проводить в 2013 году, задолго до того, как продукт стал доступным в опенсорсе. Как и сейчас, тогда нас больше всего интересовала скорость работы данных сервиса Яндекс.Метрика. Мы уже хранили данные в ClickHouse с января 2009 года. Часть данных записывалась в базу с 2012 года, а часть — была переконвертирована из OLAPServer и Metrage — структур данных, которые использовались в Яндекс.Метрике раньше. Поэтому для тестов мы взяли первое попавшееся подмножество из 1 миллиарда данных о просмотрах страниц. Запросов в Метрике ещё не было, и мы придумали запросы, больше всего интересные нам самим (всевозможные виды фильтрации, агрегации и сортировки).


ClickHouse тестировался в сравнении с похожими системами, например, Vertica и MonetDB. Для честности тестирования его проводил сотрудник, который до этого не был разработчиком ClickHouse, а частные случаи в коде не оптимизировались до получения результатов. Похожим образом мы получили набор данных и для функциональных тестов.


После того, как ClickHouse вышел в опенсорс в 2016 году, к тестам стало больше вопросов.

Недостатки тестов на закрытых данных

Тесты производительности:


Не воспроизводимы снаружи, потому что для их запуска нужны закрытые данные, которые нельзя опубликовать. По этой же причине часть функциональных тестов недоступна для внешних пользователей.
Не развиваются. Есть потребность в существенном расширении их набора, чтобы можно было изолированным образом проверять изменения в скорости работы отдельных деталей системы.
Не запускаются покоммитно и для пул-реквестов, внешние разработчики не могут проверить свой код на регрессии производительности.


Можно решить эти проблемы — выкинуть старые тесты и сделать новые на основе открытых данных. Среди открытых данных можно взять данные перелётов в США, поездок такси в Нью-Йорке или использовать готовые бенчмарки TPC-H, TPC-DS, Star Schema Benchmark. Неудобство в том, что эти данные далеки от данных Яндекс.Метрики, да и тестовые запросы хотелось бы сохранить.

Важно использовать настоящие данные

Тестировать производительность сервиса нужно только на настоящих данных с продакшена. Рассмотрим несколько примеров.

Пример 1


Предположим, вы заполняете базу данных равномерно распределёнными псевдослучайными числами. В этом случае сжатие данных не будет работать. Но сжатие данных — важное свойство для аналитических СУБД. Выбор правильного алгоритма сжатия и правильного способа интеграции его в систему — нетривиальные задачи, в которых нет одного верного решения, потому что сжатие данных требует компромисса между скоростью сжатия и разжатия и потенциальной степенью сжатия. Системы, которые не умеют сжимать данные, проигрывают гарантированно. Но если использовать для тестов равномерно распределённые псевдослучайные числа, этот фактор не рассматривается, и все остальные результаты будут искажены.


Вывод: тестовые данные должны обладать реалистичным коэффициентом сжатия.


Про оптимизацию алгоритмов сжатия данных в ClickHouse я рассказывал в предыдущем посте.

Пример 2


Пусть нас интересует скорость работы SQL-запроса:

SELECT RegionID, uniq(UserID) AS visitors
    FROM test.hits
    GROUP BY RegionID
    ORDER BY visitors DESC
    LIMIT 10


Это типичный запрос для Яндекс.Метрики. Что важно для скорости его работы?


Как выполняется GROUP BY.
Какая структура данных используется для расчёта агрегатной функции uniq.
Сколько разных RegionID есть и сколько оперативки требует каждое состояние функции uniq.


Но ещё очень важно, что количество данных для разных регионов распределено неравномерно. (Наверное, оно распределено по степенному закону. Я построил график в шкале log-log, но не уверен.) А если так, то нам важно, чтобы состояния агрегатной функции uniq с маленьким количеством значений использовали мало памяти. Когда разных ключей агрегации много, счёт идёт на единицы байт. Как получить сгенерированные данные, которые обладают всеми этими свойствами? Конечно, лучше использовать настоящие данные.


Многие СУБД реализуют структуру данных HyperLogLog для приближённого расчёта COUNT(DISTINCT), но все они работают сравнительно плохо, потому что эта структура данных использует фиксированное количество памяти. А в ClickHouse есть функция, которая использует комбинацию из трёх разных структур данных, в зависимости от размера множества.


Вывод: тестовые данные должны репрезентовать свойства распределения значений в данных — кардинальности (количество значений в столбцах) и взаимной кардинальности нескольких разных столбцов.

Пример 3


Ладно, пусть мы тестируем производительность не аналитической СУБД ClickHouse, а чего-нибудь попроще, например, хэш-таблиц. Для хэш-таблиц очень важен правильный выбор хэш-функции. Для std::unordered_map он чуть менее важен, потому что это — хэш-таблица на основе цепочек, а в качестве размера массива используется простое число. В реализации стандартной библиотеки в gcc и clang в качестве хэш-функции по умолчанию для числовых типов используется тривиальная хэш-функция. Но std::unordered_map — не лучший выбор, когда мы хотим получить максимальную скорость. При использовании open-addressing хэш-таблицы правильный выбор хэш-функции становится решающим фактором, и мы не можем использовать тривиальную хэш-функцию.


Легко найти тесты производительности хэш-таблиц на случайных данных, без рассмотрения используемых хэш-функций. Так же легко найти тесты хэш-функций с упором на скорость вычисления и некоторые критерии качества, правда, в отрыве от используемых структур данных. Но факт в том, что для хэш-таблиц и для HyperLogLog требуются разные критерии качества хэш-функций.




Подробнее об этом — в докладе «Как устроены хэш-таблицы в ClickHouse». Он чуть устарел, так как в нём не рассматриваются swiss-таблицы.

Задача

Мы хотим получить данные для тестирования производительности по структуре такие же, как данные Яндекс.Метрики, для которых сохранены все свойства важные для бенчмарков, но так, чтобы в этих данных не осталось следов настоящих посетителей сайтов. То есть данные должны быть анонимизированы, при этом должны сохраняться:


коэффициенты сжатия,
кардинальности (количества различных значений),
взаимные кардинальности нескольких разных столбцов,
свойства вероятностных распределений, с помощью которых можно моделировать данные (например, если мы считаем, что регионы распределены по степенному закону, то показатель степени — параметр распределения — у искусственных данных должен быть примерно такой же, как у настоящих).


А что нужно, чтобы у данных был похожий коэффициент сжатия? Например, если используется LZ4, то подстроки в бинарных данных должны повторяться примерно на таких же расстояниях и повторения должны быть примерно такой же длины. Для ZSTD добавляется совпадение энтропии байтов.


Задача-максимум: сделать инструмент, доступный для внешних людей, с помощью которого каждый сможет анонимизировать свой набор данных для публикации. Чтобы мы отлаживали и тестировали производительность на чужих данных аналогичных данным с продакшена. А ещё хотелось бы, чтобы сгенерированные данные было интересно смотреть.


Это — неформальная постановка задачи. Впрочем, формальную постановку никто давать не собирался.

Попытки решения

Не следует преувеличивать важность этой задачи для нас. На самом деле её никогда не было в планах и никто даже не собирался её делать. Я просто не оставлял надежд, что что-нибудь само собой придумается, и вдруг у меня будет хорошее настроение и много дел, которые можно отложить на потом. 

Явные вероятностные модели

Первая идея — для каждого столбца таблицы выбрать семейство вероятностных распределений, которое его моделирует. Затем, исходя из статистики данных, подобрать параметры (model fitting) и с помощью подобранного распределения генерировать новые данные. Можно использовать генератор псевдослучайных чисел с предопределённым seed, чтобы получать воспроизводимый результат.


Для текстовых полей можно использовать марковские цепи — понятную модель, для которой можно сделать эффективную реализацию.


Правда потребуются некоторые трюки:


Мы хотим сохранить непрерывность временных рядов — значит, для некоторых типов данных нужно моделировать не само значение, а разность между соседними.
Чтобы смоделировать условную кардинальность столбцов (например, что на один идентификатор посетителя обычно приходится мало IP-адресов), придётся также выписывать зависимости между столбцами в явном виде (к примеру, для генерации IP-адреса в качестве seed используется хэш от идентификатора посетителя, но ещё добавляется немного других псевдослучайных данных).
Непонятно, как выразить зависимость, что один посетитель примерно в одно время часто посещает адреса URL с совпадающими доменами.


Всё это представляется в виде программы, в которую hard coded все распределения и зависимости — так называемый «скрипт на C++». Впрочем, марковские модели всё-таки вычисляются из суммы статистики, сглаживания и загрубления с помощью шума. Я начал писать этот скрипт, но почему-то после того, как написал в явном виде модели для десяти столбцов, внезапно стало невыносимо скучно. А в таблице hits в Яндекс.Метрике ещё в 2012 году было более 100 столбцов. 

EventTime.day(std::discrete_distribution<>({
    0, 0, 13, 30, 0, 14, 42, 5, 6, 31, 17, 0, 0, 0, 0, 23, 10, ...})(random));
EventTime.hour(std::discrete_distribution<>({
    13, 7, 4, 3, 2, 3, 4, 6, 10, 16, 20, 23, 24, 23, 18, 19, 19, ...})(random));
EventTime.minute(std::uniform_int_distribution<UInt8>(0, 59)(random));
EventTime.second(std::uniform_int_distribution<UInt8>(0, 59)(random));
UInt64 UserID = hash(4, powerLaw(5000, 1.1));
UserID = UserID / 10000000000ULL * 10000000000ULL
    + static_cast<time_t>(EventTime) + UserID % 1000000;
random_with_seed.seed(powerLaw(5000, 1.1));
auto get_random_with_seed = [&]{ return random_with_seed(); };


Такой подход к задаче был провальным. Если бы я к ней старательнее подошёл, наверняка скрипт был бы написан.


Достоинства:


идейная простота.


Недостатки:


трудоёмкость реализации,
реализованное решение подходит только для одного вида данных.


А я бы хотел более общее решение — чтобы его можно было применить не только для данных Яндекс.Метрики, но и для обфускации любых других данных.


Впрочем, улучшения тут возможны. Можно не вручную подбирать модели, а реализовать каталог моделей и выбирать среди них лучшую (best fit + какая-то регуляризация). А может быть, можно использовать марковские модели для всех типов полей, а не только для текста. Зависимости между данными тоже можно понять автоматически. Для этого нужно посчитать относительные энтропии (относительное количество информации) между столбцами, либо проще — относительные кардинальности (что-то типа «как много в среднем разных значений A для фиксированного значения B») для каждой пары столбцов. Это позволит понять, например, что URLDomain полностью зависит от URL, а не наоборот.


Но от этой идеи я тоже отказался, потому что слишком много вариантов того, что нужно учесть, и это долго надо будет писать.

Нейронные сети

Я уже рассказывал, насколько эта задача важна для нас. Никто даже не думал о том, чтобы сделать какие-то поползновения к её реализации. К счастью, коллега Иван Пузыревский тогда работал преподавателем в ВШЭ и одновременно занимался разработкой ядра YT. Он спросил, нет ли у меня каких-нибудь интересных задач, которые можно предложить студентам в качестве темы диплома. Я предложил ему эту, и он заверил меня, что она подходит. Так я дал эту задачу хорошему человеку «с улицы» — Шарифу Анвардинову (для работы с данными подписывается NDA).


Рассказал ему про все свои идеи, но главное — объяснил, что решать задачу можно любым способом. И как раз хорошим вариантом было бы использовать те подходы, в которых я не разбираюсь абсолютно: например, генерировать текстовый дамп данных с помощью LSTM. Это выглядело обнадёживающим благодаря статье The Unreasonable Effectiveness of Recurrent Neural Networks, которая тогда мне попалась на глаза.


Первая особенность задачи в том, что требуется генерировать структурированные данные, а не просто текст. Было неочевидно, сможет ли рекуррентная нейронная сеть генерировать данные с нужной структурой. Решить это можно двумя способами. Первый — использовать для генерации структуры и для «наполнителя» отдельные модели: нейронная сеть должна только генерировать значения. Но этот вариант отложили на потом, после чего никогда не сделали. Второй способ — просто генерировать TSV-дамп как текст. Практика показала, что в тексте часть строк будет не соответствовать структуре, но эти строки можно выкидывать при загрузке.


Вторая особенность — рекуррентная нейронная сеть генерирует последовательность данных, и зависимости в данных могут следовать только в порядке этой последовательности. Но в наших данных, возможно, порядок столбцов обратный по отношению к зависимостям между ними. С этой особенностью мы ничего делать не стали.


Ближе к лету появился первый рабочий скрипт на Python, который генерировал данные. На первый взгляд, качество данных приличное:




Правда обнаружились сложности:


Размер модели составляет около гигабайта. А мы пробовали создать модель для данных, размер которых был порядка нескольких гигабайт (для начала). То, что полученная модель такая большая, вызывает опасения: вдруг из неё можно будет достать реальные данные, на которых она обучалась. Скорее всего, нет. Но я ведь не разбираюсь в машинном обучении и нейронных сетях и код на Python от этого человека так и не прочитал, поэтому как я могу быть уверенным? Тогда как раз выходили статьи о том, как сжимать нейронные сети без потери качества, но это осталось без реализации. С одной стороны, это не выглядит проблемой — можно отказаться от публикации модели и опубликовать только сгенерированные данные. С другой, в случае переобучения, сгенерированные данные могут содержать какую-то часть исходных данных.
На одной машине с CPU скорость генерации данных составляет примерно 100 строк в секунду. У нас была задача — сгенерировать хотя бы миллиард строк. Расчёт показал, что это не удастся сделать до защиты диплома. А использовать другое железо нецелесообразно, ведь у меня была цель — сделать инструмент генерации данных доступным для широкого использования.


Шариф попробовал изучить качество данных сравнением статистик. Например, посчитал, с какой частотой в исходных и в сгенерированных данных встречаются разные символы. Результат получился сногсшибательным — самыми частыми символами являются 