

Нейронные сети, графы и эмерджентность / Habr


               Нейронные сети, графы и эмерджентность Level of difficulty  
    Hard
   Reading time  
    6 min
   Views  5K Mathematics *Machine learning *Popular science Artificial Intelligence Physics  
    Review
        В этой статье я хочу попробовать осветить некоторые интересные, на мой взгляд, области науки, с которыми я сталкивался в контексте работы с нейронными сетями, и найти между ними взаимосвязь. Данная статья не претендует на истину в последней инстанции и является всего лишь попыткой посмотреть на нейронные сети под другим углом. Сразу предупреждаю - я не являюсь каким то глубоким специалистом в этих сферах.Нейронные сетиНейронные сети сейчас -  крайне активно развивающаяся сфера, в которой каждый день происходит новая научная революция, появляются новые архитектуры, фундаментально меняющие всю область и быстро имплементируемые в реальные продукты. Однако для этой, относительно новой области, все еще существует ряд крайне важных проблем, методы решения которых сейчас, с моей точки зрения, оставляют желать лучшего. Одна из интереснейших проблем сейчас заключается в том, что новые архитектуры - это по сути, рукомахательно придуманные лайфхаки. Посмотрим на конкретные примеры:YoloR: You Only Learn One Representation: Unified Network for Multiple TasksАрхитектура YoloR, основная магия заключается в implicit knowledge блокеДанная сеть, на датасете COCO, в рамках задачи детектирования, превосходит большинство своих конкурентов по ряду параметров (во всяком случае, превосходила на момент выхода).Сравнение разных версий YoloОднако, если вы решитесь прочитать саму статью о данной модели, то будет серьезнейший риск не понять, о чем вообще речь и что происходит в данной нейронной сети. Все дело в том, что магия этой архитектуры заключается в добавлении блока без входа - implicit knowledge блока, в который стекаются данные "независящие от входа", т.е. некоторое неявное представление знаний. Что это означает с формальной точки зрения? Не представляю.Attention Is All You NeedБазовая идея сети типа трансформерНо может, предыдущий пример бы не удачным? Обратимся к тому, что у всех на слуху: Трансформеры.Начало данной, без преувеличения, прорывной архитектуры было положено в статье с сильным заголовком "Attention Is All You Need".Основная идея статьи, как следует из названия, заключается в том, что добавление multi-head attention позволяет превосходить практически любые другие архитектуры.Multi-head attentionИ это касается не только обработки текстов - трансформеры так же прекрасно справляются с обработкой изображений (SWIN). Объясняется это в основном тем, что multi-head attention блок можно воспринимать, как нелинейную функцию нового типа. Вкупе с большим числом параллельных потоков данных - это дает значительный прирост в эффективности нейронной сети. Звучит убедительно? Так в чем же проблема?А в том, что, что архитектура на чистом multi-head attention не просто плоха - она не будет работать:Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with DepthАвторы другой статью пошли еще дальше и полностью убрали self-attention из своей сети. Результатом стала архитектура:MLP-Mixer: An all-MLP Architecture for VisionMLP-Mixer, по сути представляет собой ViT без attentionВ котором все self-attention были заменены на обычные MLP слои. И при сравнении с аналогичными transformer-base сетями оказалось, что при значимом росте скорости работы, сеть практически не теряет в качестве:Сравнение Mixer архитектуры с другими SOTA моделямиПочему так происходит? Полноценного ответа на этот вопрос нет.Однако, я совру, если скажу, что никто не пытается это изучать. Выходит множество статей на эту тему: On the Mathematical Understanding of ResNet,  The Modern Mathematics of Deep Learning, etc. Мне нравятся эти статьи, они необходимы и интересны для разработки архитектур под конкретные задачи, однако глобально проблема математического бэкграунда нейронных сетей остается открытой. Сейчас попытки его формализации сфокусированы в основном на работе с отдельными частями сети - слоями, блоками и функциями. Но, возможно, недостаточно просто смотреть на вопросы сходимости, отдельные блоки сети и т.д. Возможно, стоит посмотреть на проблему под другим углом?ГрафыБазовый скелет нейронной сети - это граф весов. В таком случае, архитектура сети - ее устройство, всяческие skip-connections, feature pyramids - это различные топологии сети. Топологические, в широком смысле, свойства графа могут задаваться его инвариантами. Таким образом, графы с одинаковыми инвариантами могут пониматься, как одинаковые. Далее мы сфокусируемся на одном конкретном инварианте - полиноме Татта, он будет важен для нас в дальнейшем. Допустим у нас есть граф G = (V, E) - где V - число вершин, E - число ребер. Тогда полином Татта: где k(A) - число компонент связности графа (V, A). Данные полиномы задают широкий спектр различных свойств графа, таких как число деревьев T(2,1), число остовов T(1, 1), etc. В широком смысле полиномы Татта характеризуют степень связности графа. Опишем простой пример расчета полинома. Полином Татта может быть рассчитан рекурсивно:где G\e - граф G с удаленным ребром e, G/e - граф G, где вершины при ребре e - склеены. Пример: Пример расчета полинома ТаттаСмотрим на ребро, помеченное пунктирной линией. Полином такого графа распадается на сумму двух:1) Справа пунктирное ребро удалено2) Слева пунктирное ребро стянуто.  Результат стягивания аналогично распадается на сумму двух других полиномов. В итоге, рекурсивно получаем сумму мономов, указанных на изображении.Но интересны они нам не только по этому. Простое знание топологии графа не даст нам никакой информации о том, для какого случая какая топология наиболее оптимальна и почему. Нам интересна сходимость нейронной сети, ее поведение, в зависимости от топологии. Давайте зададимся вопросом: Что такое "хорошая" нейронная сеть? Как формально описать то, что она будет хорошо решать поставленную задачу, в терминах архитектуры? Попытка понять ответ на этот вопрос и приводит нас к следующему разделу...ЭмерджентностьСамое характерное свойство эмерджентной системы: Наличие у системы свойств, не присущих её компонентам по отдельности. Простейший пример эмерджентности, который, я уверен, известен многим из читателей: Клеточные автоматы.Пример клеточного автомата: ИсточникГоворя чуть формальнее: Допустим в вашей среде задан набор простейших агентов с простейшим набором правил. Тогда моделирование их взаимодействия может порождать крайне нетривиальную и сложную динамику.Но как это все связано с нейронными сетями, графами и полиномами Татта? На самом деле эмерджентность - это свойство не только клеточных автоматов или их аналогов, но любых сложных систем. То есть систем, состоящих из множества отдельных компонент. И самый характерный пример подобных систем, с точки зрения физики - это фазовый переход. Простейший пример - модель Поттса на графе:где s - векторы, заданные в вершинах графа (будем называть их "спины"), J - коэффициент или матрица смежности графа, H - гамильтониан модели Поттса (энергия), дельта - Кронекера.Проще говоря, данная модель говорит: чем больше рядом с тобой соседей, таких же как ты, тем тебе лучше (тем меньше твоя энергия). Поместим такую модель на граф и зададимся распределением, например, Гумбеля:тогда, считая, что вероятность перехода спина из одного состояния в другое - это вероятность того, что энергия нового состояния будет меньше, чем энергия старого (напомним, гамильтониан - это энергия), получим:где V - энергия соответствующего состояния, X - всевозможные состояния. Температура здесь опущена. Задавшись такой вероятностью и таким гамильтонианом на графе, мы можем инициализировать все его вершины случайным значением из X и запустить эволюцию данной системы с какого то случайного узла. Результатом станет динамический процесс, который может привести к фазовому переходу.Но как же все это связано с полиномами Татта, о которых я писал?Дело в том, что в статистической физике основным математическим инструментом является функция от гамильтониана:Данная функция называется статсуммой и позволяет вычислять различные термодинамические параметры системы. Если немного покопаться в формулах и разобраться с этой функцией, оказывается, что она, с точностью до множителя, равна полиному Татта. Таким образом, между теорией фазовых переходов и теорией графов есть прямая связь. Теперь мы можем вычислять термодинамические параметры модели в зависимости от топологии графа.Но к чему весь этот пассаж о фазовых переходах?Дело в том, что нейронные сети, по своей сути, являются сложной системой. Но, что более интересно: в них есть фазовые переходы. И тут можно задаться вопросом: А вдруг возможно этот же фреймворк использовать для анализа зависимости свойств нейронной сети от ее архитектуры?И это так. Вспомним вопрос: Как мы можем понять, что такое "хорошая архитектура"? Формальный ответ: Та нейронная сеть, которая находит глобальный минимум нашей функции потерь для заданного датасета.Неформальный ответ, который можно дать после вышеописанного обзора: "Хорошая" нейронная сеть - это та, для которой существует фазовый переход для данной задачи.ПослесловиеНа самом деле я не упомянул еще важную взаимосвязь: Известно, что нейронные сети, по крайней мере в каких то случаях, могут быть переформулированы в теоретико-игровых терминах как задача поиска решения для какой то игры. И на самом деле, в каком то смысле, некоторые равновесные состояния в теории игр соответствуют фазовым переходам из статистической физики (модель Поттса <=> QRE). Таким образом, у нас есть дорожка, идя по которой можно попытаться построить фреймворк для анализа процесса обучения нейронных сетей, с точки зрения равновесия на графе, в зависимости от топологии этого графа. Такой подход позволил бы нам учитывать коллективное поведение нейронной сети в целом, а не свойств отдельных ее элементов.      Tags: машинное+обучениефазовый переходискуственный интеллектграфынейронные сетинейронная сетьэмерджентность  Hubs: MathematicsMachine learningPopular scienceArtificial IntelligencePhysics          


