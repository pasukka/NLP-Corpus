

Путеводитель по основным трендам 2021 года в области обработки естественного языка и не только / Habr


               22  March   at 10:01  Путеводитель по основным трендам 2021 года в области обработки естественного языка и не только ГК ЛАНИТ corporate blog Machine learning *Reading room Artificial Intelligence Natural Language Processing * 
        Translation
         
                Original author:
                
                  Sebastian Ruder
                  Когда-то давно люди много путешествовали, посещали новые города и страны, им удавалось насладиться культурой других народов, пообщаться с ними на языке жестов. Исследовать новый для себя город можно по-разному. Например, бесцельно гулять по его улицам, впитывая атмосферу, состоящую из множества разных мелочей. И это отличный способ, если времени на осмотр много и точно знаешь, что рано или поздно еще вернешься. В противном случае полезно оптимизировать визит, используя путеводители, карты достопримечательностей и статьи других путешественников. Число исследований в области машинного обучения с каждым годом растет. Конечно, приятно было бы прогуляться по каждой статье или ветке исследований отдельно, но времени на это просто может не хватить, а «посетить» 2021 год еще раз, увы, не удастся. Следовательно, необходимо также искать «путеводители» - статьи, подсвечивающие некоторые тренды, понимание которых важно для будущих направлений исследований. В начале года одна из таких статей «гуляла»  по различным каналам и чатам. Мне захотелось перевести ее на русский и поделиться с вами. Далее приведу перевод этой статьи с моими комментариями.Источник: Liu et al. (2021) (https://arxiv.org/abs/2107.13586)В предлагаемой работе обобщаются достижения в нескольких значимых областях машинного обучения, в частности, в обработке естественного языка за 2021 год.2021 год отмечен впечатляющими достижениями в сферах машинного обучения (machine learning, ML) и обработки естественного языка (natural language processing, NLP). Эта работа охватывает наиболее примечательные, на взгляд ее автора, публикации и направления исследований. Я рассмотрел только те, о которых сам имел представление, поэтому я вполне мог упустить из виду многие относящиеся к делу статьи. Прошу указывать в своих комментариях не упомянутые мной публикации, которые произвели на вас впечатление. В этом обзоре я рассмотрел следующие темы.Универсальные моделиМассовое многозадачное обучениеЗа пределами трансформераЗатравочное программированиеЭффективные методыБенчмаркингГенерация изображений по условиюМашинное обучение в наукеСинтез программ Смещения Дополнение модели возможностью извлечения данных Модели без токенов Адаптация временных рамок Важность данных Мета-обучение1) Универсальные модели (Universal/ Foundation models)Обучение без учителя межъязыковому представлению речи с помощью модели XLS-R. Эта модель была предобучена на разнообразных многоязычных речевых данных с использованием функции потерь (loss) в парадигме обучения без учителя wav2vec 2.0. Затем эту обученную модель можно было файн-тюнить для решения различных речевых задач (Babu et al., 2021).Основные результаты   В 2021 году продолжалась разработка все более крупных предобученных моделей. Предобученные модели нашли свое применение в различных областях и стали считаться критически важными для исследований машинного обучения [1]. В области компьютерного зрения предобученные модели с учителем, такие, как Vision Transformer [2], подверглись масштабированию [3]. Предварительно обученные модели с самообучением учителя догнали их по производительности [4]. Последние были масштабированы за пределы контролируемой среды ImageNet до случайных коллекций изображений [5]. В области обработки речи были созданы новые модели на основе wav2vec 2.0 [6], такие, как W2v-BERT [7], а также более мощные многоязычные модели, например XLS-R [8]. В то же время мы увидели новые унифицированные предобученные модели для малоизученных пар модальностей: видео и язык [9], а также речь и язык. [10]. В сочетании пар модальностей «зрение и язык» контролируемые исследования выявили важные аспекты подобных мультимодальных моделей [11][12]. Благодаря формулированию различных задач в парадигме языкового моделирования удалось создать модели, которые могут быть применимы и в других областях, таких, как обучение с подкреплением [13] и предсказание структуры белков [14]. Исходя из наблюдаемого поведения этих универсальных моделей при масштабировании, исследователи обычно отмечают производительность таких моделей при различных размерах параметров. Однако повышение производительности при использовании предобучения не обязательно приводит к улучшению качества на последующих этапах [15][16]. Почему это важно   Исследования показали, что предобученные модели хорошо обобщаются при решении новых задач для определенного домена или модальности. Эти модели демонстрируют устойчивое поведение при обучении с использованием малого количества примеров (в парадигме few-shot) и устойчивость при обучении. В качестве таковых они являются ценным «строительным блоком» для получения новых научных результатов и создания областей практического применения.Перспективы   В будущем, несомненно, будет разработано еще больше предобученных моделей, причем, еще более масштабных. В то же время следует ожидать, что отдельные модели будут одновременно выполнять больше задач. Это явление уже имеет место в обработке естественного языка, в которых модели способны выполнять множество задач путем представления их в обычном формате text-to-text. Нечто подобное мы, вероятно, увидим у моделей изображений и речевых моделей, каждая из которых сможет одновременно выполнять большое количество типовых задач. И, наконец, мы увидим и другие работы по обучению моделей для нескольких модальностей.2) Массовое многозадачное обучениеМассовое многозадачное обучение с использованием модели ExT5. В процессе предобучения модель обучается на входной информации (слева) из разнообразного набора различных задач в формате text-to-text с целью получения соответствующей выходной информации (справа). В состав указанных задач входят: заполнение маскированных слов, автоматическое реферирование, семантический парсинг, ответы на вопросы в режиме closed-book, передача стиля, моделирование диалогов, определение логической связи между текстами, разрешение кореферентности в стиле схемы Винограда (сверху вниз) и проч. (Aribandi et al., 2021). Основные результаты   Большинство предобученных моделей из предыдущего раздела относится к категории моделей с самообучением (self-supervised). В общем случае эти модели обучаются на больших объемах неразмеченных данных в рамках некоторых типов задач, где не требуется наличие явного учителя. Однако для многих доменов уже доступны большие объемы размеченных данных, которые можно использовать для получения более качественных текстовых представлений. Сейчас есть целый пласт многозадачных моделей: T0 [17], FLAN [18] и ExT5 [19] 