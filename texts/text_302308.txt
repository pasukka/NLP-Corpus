

Как мы делали систему выделения информации из текста на естественном языке для банка АО «Банк ЦентрКредит» (Казахстан) / Habr


               31  May  2016 at 17:17  Как мы делали систему выделения информации из текста на естественном языке для банка АО «Банк ЦентрКредит» (Казахстан) MeanoTek corporate blog Semantics *Data Mining *Algorithms *Machine learning *      Некоторое время назад к нам обратился представитель банка АО «Банк ЦентрКредит» (Казахстан) с интересной задачей. Необходимо было интегрировать в конвейер обработки данных, представляющих из себя текст на естественном языке, дополнительный инструмент обработки. Всех деталей проекта мы раскрывать не можем, так как он находится в сфере безопасности банка и разрабатывается его службой безопасности. В освещении технологических аспектов задачи и способов их реализации заказчик не был против, что собственно мы и хотим сделать в рамках данной статьи.


В целом задача, состояла в извлечении некоторых сущностей из большого массива текстов. Не сильно отличающаяся проблема от классической задачи извлечения именованных сущностей, с одной стороны. Но определения сущностей отличались от обычных и тексты были довольно специфическими, а сроку на решение проблемы было две недели.

 Входные данные 

 Размеченных и доступных корпусов именованных сущностей на русском на тот момент не было вообще. А если бы и были, то там оказались бы не совсем те сущности. Или совсем не те сущности. Аналогичные решения были, но оказались они либо плохими (находили, например, такие организации как “Задняя Крышка” в предложении “после года работы Задняя крышка сломалась”) либо очень даже хорошими, но выделяющими не то, что надо (имелись требования, касающиеся того какие именно сущности нужно выделять и относительно границ этих сущностей).

Начало работы

Данные пришлось размечать самостоятельно. Из предоставленных специфических текстов была создана обучающая выборка. В течение недели усилиями полутора землекопов удалось разметить выборку объемом 112 000 слов, содержащую порядка 9 000 упоминаний нужных сущностей. После обучения нескольких классификаторов на валидационной выборке мы получили следующее:


Метод
F1


CRF (базовый набор признаков)
67.5


Двунаправленная многослойная сеть Элмана

68.5


Двунаправленная LSTM

74.5



Для простых по содержанию сущностей это не очень хорошо, на сравнимых задачах специализированные системы часто выдают F1 районе 90-94 (по опубликованным работам). Но то на выборках из миллиона с лишним словоформ и при условии тщательного подбора признаков. 


В предварительных результатах лучше всего себя показала модель LSTM, с большим отрывом. Но использовать ее не очень хотелось, ибо она относительно медленная, обрабатывать ей большие массивы текста в реальном времени накладно. К моменту получения размеченной выборки, и предварительных результатов до срока оставалась неделя.

День #1. Регуляризация

Основная проблема нейронных сетей на малых выборках – переобучение. Классически с этим можно бороться подбором правильного размера сети, или использованием специальных методов регуляризации. 


Мы попробовали подбор размеров, max-norm регуляризацию на разных слоях, с подбором значений констант и dropout. Получили красные глаза, головную боль и пару процентов выигрыша.


Метод
F1


Уменьшение размера сети до оптимального

69.3


Max-norm

71.1


Dropout

69.0



 Dropout нам никак практически не помог, обучается сеть медленнее, и результат не особо хороший. Лучше всего показал себя Max-norm и изменение размера сети. Но прирост небольшой, до нужных значений как до луны, а все что можно вроде как сделано.

 День #2. Rectified Linear Pain Unit

Статьи рекомендуют использовать RelU функцию активации. Написано, что улучшает результаты. RelU это простая функция if x>0 then x else 0.


Ничего не улучшилось. Оптимизация с ними вообще не сходится или результат ужасных. Провозились день, пытаясь понять почему. Не поняли.

День #3. LSTM-подобные монстры

Можно ли сделать так, чтобы было как LSTM, но на обычных слоях? После некоторых размышлений воображение подсказало конструкцию (рис.1). Перед рекуррентным слоем добавлен один feed-forward слой (он типа должен контролировать какая информация поступает в сеть), и сверху еще конструкция для управления выводом:



рисунок 1. Архитектура специальной нейронной сети для выделения терминов из текста


Как ни странно, при правильном подборе параметров эта конструкция дала прирост F1 до 72.2, что очень не плохо для полностью выдуманной архитектуры.

День #4. RelU Возвращается

Из упрямства попробовали RelU на “монстре”. Оказалось, что если RelU установить только на рекуррентный слой, то оптимизация не просто сходится, а получается F1 73.8! Откуда такое чудо?


Давайте разберемся. Почему работает хорошо LSTM? Обычно объясняют это тем, что он может запоминать информацию на более длительное время, и таким образом “видеть” больше контекста. В принципе, обычную RNN тоже можно обучить помнить длинный контекст, если использовать соответствующий алгоритм обучение. Но, применительно к нашей проблеме разметки последовательности с векторами слов на входе, обычная RNN сначала ищет зависимости в текущем временном промежутке и в ближайшем контексте, и успевает переобучится на них до того, как обучение дойдет до возможности осмысленного анализа данного контекста. В обычной RNN Элмана мы не можем нарастить “объем памяти” без того, чтобы значительно не увеличить способность сети к переобучению. 


Если мы посмотрим на картинку новой архитектуры, то увидим, что здесь мы разнесли модуль, хранящий информацию и “решающий” модуль. При этом модуль памяти сам лишен возможности строить сложные гипотезы, поэтому его можно нарастить без того боязни того, что он будет вносить существенный вклад в переобучение. Этим мы получили возможность управлять степенью относительной важности памяти и информации в текущем окне для конкретной задачи. 

День #5. Диагональные элементы

Следуя идеи, описанной в [1], мы исключили из рекуррентного слоя все рекуррентные соединения кроме от других нейронов, оставив на вход каждого нейрона только его собственное предыдущее состояние. Кроме того, мы добавили сделали верхний слой тоже рекуррентным (рис. 2). Это дало нам F1 74.8, что, на данной задачи являлось лучшим результатом, чем удалось вначале получить с помощью LSTM.

День #6. Объем выборки

Поскольку всю неделю мы продолжали размечать данные, то в этот день мы перешли к использованию новой выборки удвоенного размера, что позволило (после нового витка подбора гиперпараметров) получить F1 83.7. Нет ничего лучше выборки большего размера, когда ее легко получить. Правда удвоить объем размеченных данных обычно бывает совсем не просто. Вот что имеем в итоге:


Метод
F1


CRF (базовый набор признаков)
76.1


Двунаправленная многослойная сеть Элмана

77.8


Двунаправленная LSTM

83.2


Наша архитектура

83.7


Выводы и сравнение с аналогами

Адекватно сравнить нашу систему распознания с аналогичными реализациями у упомянутых выше web-API нельзя, поскольку различаются определения самих сущностей и границ. Мы сделали некий очень приблизительный анализ, на небольшой тестовой выборке, попытавшись поставить все системы в равные условия. Нам пришлось вручную проанализировать результат по специальным правилам, использовав метрику binary overlap (засчитывается определение сущности, если система выделила хотя бы ее часть, что снимает вопрос несовпадения границ) и исключив из анализа случаи, когда сущности не надо было выделять из-за несовпадающих определений. Вот что вышло:


Метод
F1


Наша система
76.1


Аналог #1

77.8


Аналог #2

83.2



Аналог #2 имеет лишь небольшое преимущество по данной метрике, а Аналог #1 показал себя еще и хуже. Оба решения будут выдавать менее качественные результаты, если тестировать их на нашей задаче с поставленными заказчиком уточнениями. 


Из всего вышеизложенного мы сделали два вывода:

1. Даже хорошо определенные и решенные задачи извлечения именованных сущностей имеют подварианты, которые могут сделать применение готовых систем невозможным.

2. Применение нейронных сетей позволяет быстро создавать специализированные решения, которые находятся примерно в том же диапазоне качества, что и более сложные разработки.

Литература

1. T. Mikolov, A. Joulin, S. Chopra, M. Mathieu, and M. Ranzato. Learning longer memory in

recurrent neural networks    Tags: Машинное обучениеобработка естественного языкаобработка текстовнейронные сети Hubs: MeanoTek corporate blogSemanticsData MiningAlgorithmsMachine learning          


