

GPT-4, Claude 2 и Llama 2: какая языковая модель не отлынивает от ответов / Habr


               GPT-4, Claude 2 и Llama 2: какая языковая модель не отлынивает от ответов  Reading time  
    3 min
   Views  5.8K ГК ITGLOBAL.COM corporate blog Research and forecasts in IT *Artificial Intelligence       Аналитическая компания Arthur AI провела тестирование моделей искусственного интеллекта, представленных Meta, OpenAI, Cohere и Anthropic. В статье разбираем, какие из них наиболее склонны к вымыслу или галлюцинациям.Суть экспериментаТестировали модели:GPT-3.5 (~175 млрд параметров) и GPT-4 (~1,76 триллиона параметров) от OpenAI;Claude-2 от Anthropic (# неизвестно);LLaMA-2 (70 млрд параметров) от Meta;Command от Cohere (~50 млрд параметров).Для исследования составили набор вопросов в трех категориях: задачи по комбинаторной математике, вопросы о президентах США и политических лидерах Марокко. Основная цель была в том, чтобы проверить способность моделей к многоэтапному рассуждению при формировании ответа.Результаты исследованияКаждый вопрос задавали по несколько раз, потому что LLM могут отвечать на один и тот же вопрос по-разному: правильно, неверно, не совсем точно или вообще не давать ответ.Статистика ответов LLM на вопросы во время экспериментаРезультаты показывают, что GPT-4 от OpenAI является наилучшим инструментом для решения математических задач. Claude 2 от Anthropic наиболее осведомлен о своих ограничениях и потенциальных ошибках. Command AI от Cohere часто допускает галлюцинации, в то время как Llama 2 от Meta показал средние результаты в проведенных тестах.В комбинаторике GPT-4 показал лучшие результаты, за ним следовал Claude-2Claude-2 показывает наименьшее количество галлюцинаций и больше правильных ответов на вопросы о президентах США. В последнем он превосходит GPT-4 и GPT-3.5 Turbo, которые постоянно допускали ошибки. Последнее особенно критично, так как ChatGPT основан на GPT-3.5 и, вероятно, студенты и школьники чаще всего используют именно его во время обучения. Claude-2 лучше всех отвечает на вопросы про президентов СШАLlama 2  и Claude 2 особенно часто отказывались отвечать на вопросы о марокканских политиках. Скорее всего этот прием используют как меру противодействия чрезмерным галлюцинациям. GPT-4 была единственной моделью, которая давала больше правильных ответов, чем галлюцинаций в этом тесте.GPT-4 лучше всех отвечает на вопросы про марокканских политиковОсторожность LLM Многие разработчики беспокоятся о том, что их модели могут генерировать некорректный или оскорбительный контент. В ответ на это некоторые LLM стали добавлять к своим ответам предупреждения. Например, они могут выдавать: «Как модель ИИ я не могу выразить свое мнение». Такие фразы разочаровывают пользователей, которые ожидают конкретного ответа.В ходе эксперимента выяснилось, что GPT-4 стала чаще использовать своеобразное «хеджирование» в ответах по сравнению с GPT-3.5. В то время как модель Cohere вообще не использует такой подход.Как часто LLM отказываются выдавать конкретный ответВыводы об использовании больших языковых моделей Индивидуальный подход к выбору модели. Нельзя выбирать LLM вслепую. Важно провести тщательное тестирование, чтобы убедиться, что она эффективно решает конкретные задачи.Осторожность и учет рисков. Полагаться на результаты, представленные LLM, без критической оценки рискованно. Всегда стоит учитывать возможные ошибки и неточности, особенно когда речь идет о критически важных задачах.Понимание особенностей каждой модели. Не все LLM созданы одинаково. Некоторые (например, Claude-2) лучше осознают свои ограничения в определенных ситуациях по сравнению с другими моделями. Знание этих особенностей поможет сделать правильный выбор.      Tags: gptgpt-4llama 2llmисследованиеискусственный интеллект  Hubs: ГК ITGLOBAL.COM corporate blogResearch and forecasts in ITArtificial Intelligence          


