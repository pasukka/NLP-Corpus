

Этика и безопасность искусственного интеллекта / Habr


               Этика и безопасность искусственного интеллекта  Reading time  
    11 min
   Views  3.8K Инферит corporate blog History of IT Artificial Intelligence Transport The future is here       Разработчиков искусственного интеллекта (ИИ) призвали приостановить обучение мощных систем, базирующихся на машинном обучении. Эксперты из крупных компаний считают, что необходим перерыв в исследованиях, чтобы избежать рисков. Кто-то считает, что огромные объёмы созданной ИИ информации не позволят людям отличать правду от лжи. Выработка принципов и стандартов, которые помогут обеспечить безопасность и этичность использования ИИ, чрезвычайно важна для дальнейшего развития этих технологий.В статье рассмотрим некоторые этические аспекты ИИ.Иллюстрация сгенерирована Midjourney Проблема мирового масштабаНекоммерческая организация Future of Life опубликовала письмо за подписью главы Tesla, SpaceX и Twitter Илона Маска, одного из создателей Apple Стива Возняка, сооснователя Pinterest Эвана Шарпа и ещё более тысячи экспертов в области ИИ. Они настаивают на перерыве в исследованиях до появления общих протоколов безопасности. В обращении говорится, что системы с интеллектом, сравнимым с человеческим, представляют большой риск для общества. К числу тех, кто видит в развитии ИИ опасность для человечества, присоединился после увольнения из Google один из основоположников нейросетей Джеффри Хинтон. По его мнению, созданный ИИ новостной контент (фото, видео, тексты) наводнит Интернет, и люди не смогут отличить правдивую информацию от ложной. Он также заявил, что технологии со временем трансформируют рынок труда, заменив людей в некоторых областях.Присутствие ИИ в повседневности вызывает множество этических вопросов, которые с каждым годом становятся всё острее. Среди наиболее заметных примеров – смертельные ДТП с участием самоуправляемых автомобилей Tesla и Uber. Немаловажные проблемы в сфере использования ИИ касаются, в том числе, манипулирования информацией и различного рода дискриминацией. Ответственность и прозрачность ИИОдна из самых популярных этических задач заключена в так называемой «проблеме вагонетки» – мысленном эксперименте, состоящем из набора ситуаций, когда необходимо выбрать одно из решений, чтобы обойтись минимальным числом жертв. В случае с беспилотным автомобилем алгоритмы ИИ должны просчитать, как лучше поступить в экстренных случаях на дороге: свернуть, создав угрозу для пассажиров, либо продолжать движение, подвергнув опасности нарушителей ПДД.Автопилот Tesla определяет объекты, а ИИ принимает решения Чтобы помочь ИИ сделать такой выбор, в Массачусетском технологическом институте предлагают людям пройти тест. Его результаты лягут в основу алгоритмов, благодаря которым ИИ будет решать, как поступить в экстремальных ситуациях.Респонденты поставлены перед выбором: разбить автомобиль, убив пассажиров, или совершить смертельный наезд на пешеходов.Дилемма теста: убить пассажиров или совершить смертельный наезд на пешеходовИз проведенного опроса следует, что как сторонние наблюдатели люди в большинстве случаев стремятся указать наиболее рациональное решение, в основном принося в жертву пассажиров самоуправляемого автомобиля. Однако в роли пассажиров большинство выбирает тот вариант, который ставит их безопасность на первое место. Появляется новая этическая дилемма: дорожное движение с появлением самоуправляемых автомобилей станет безопаснее только благодаря применению алгоритмов ИИ, которые люди отклоняют в качестве неприемлемых для себя. Данный фактор может повлиять на будущий спрос и принятие решений о покупке таких автомобилей.И тем не менее важной задачей при создании ИИ является определение границ ответственности системы. В настоящее время законодательство не признаёт ИИ субъектом права и не предусматривает его ответственности за возможные происшествия. Создатель ИИ владеет алгоритмами, но результат, полученный в результате их использования, может оказаться непредсказуемым. Итог работы алгоритма сильно зависит от входных данных, и разработчик не всегда может гарантировать его точность. В такой ситуации необходимо определить, кто несет ответственность, поскольку ИИ может быть сконструирован командой специалистов.Процессы принятия решений ИИ должны быть прозрачными, чтобы существовала возможность отслеживать его действия и своевременно выявлять проблемы в функционировании. Однако некоторые алгоритмы могут быть слишком сложными для понимания человеком. В таких случаях возникает проблема контроля ИИ.Предвзятость алгоритмовКазалось бы, искусственный интеллект должен работать и принимать решения с холодным разумом как любая машина.Однако его создатели являются обладателями когнитивных искажений, которые невольно наследуют разрабатываемые системы, что способствует появлению предвзятостей в алгоритмах.Их исключение – одно из сложностей при использовании ИИ. Информация, которая поступает в систему, должна быть неискаженной, достоверной, а главное – не содержать в себе предубеждения. Некорректная работа алгоритмов может привести к дискриминации или несправедливым решениям в отношении человека. Например, боты занимаются сбором изображений лиц людей в интернете. Описания к ним вычленяются из контекста размещения. Поэтому ИИ чаще относит женщин к категории «домохозяйка» вместо, например, «доктор», исходя из сохраняющихся в обществе стереотипов. Так на уровне данных ИИ наследует предвзятость от людей. Использование технологий ИИ подчас задевают существующие в обществе ценности. Например, нарушаются такие основополагающие принципы как расовое и гендерное равенство. Явление пристрастности ИИ получило название AI bias.Необъективность ИИ стала причиной возмущений общественности в связи с решениями уголовно-исполнительной системы США в отношении этнических меньшинств, которые были вызваны ошибками в распознавании лиц.Известный случай дискриминации, связанной с использованием ИИ, произошел в 2018 году. Выяснилось, что алгоритм системы судейства при определении размера залога для подозреваемых в правонарушениях оказался пристрастным к афроамериканцам.Недавно специалист в области информатики Джон Маккормик признался в непреднамеренном создании 25 лет назад «расово предвзятого» алгоритма ИИ для распознавания лиц.Алгоритм отслеживал движения головы человека на основе данных с видеокамеры.   Проект заключался в отслеживании движений головы человека на основе данных с видеокамеры. Для совершенствования системы группа, в составе которой был Маккормик, пошла по пути других исследователей, установивших, что области изображения телесного цвета можно извлекать в режиме реального времени. Поэтому учёные решили сосредоточиться на цвете кожи как на дополнительной подсказке для трекера.Маккормик сделал несколько фотографий себя и других белых людей. Так было легче вручную извлечь некоторые пиксели телесного цвета из этих изображений и построить статистическую модель для цветов кожи. После некоторой настройки и отладки удалось создать надёжную систему отслеживания в реальном времени. (Не)доверие к ИИ Одна из основных опасностей, которую необходимо учитывать при работе с технологиями, подобными искусственному интеллекту, – завышенное доверие к ним со стороны неспециалистов. Важно помнить, что нет идеальных систем, и даже самые надёжные из них могут выходить из строя, особенно когда им предоставляются большие возможности. Поэтому заблуждение о безошибочности ИИ может привести к ужасным последствиям, и следует придерживаться здравого смысла в оценке потенциала таких систем.Сложные задачи, особенно с вариативными входными данными, могут приводить к ошибкам в работе ИИ. И по сей день она затруднена тем, что надёжность системы не может быть гарантирована. В особенности это актуально для задач, связанных с автономным управлением транспортными средствами. Поэтому важным аспектом является создание устойчивых систем с минимальным количеством сбоев.Прозрачность также является ключом к построению доверия между человеком и ИИ. Непонимание людьми оснований для принятия решений система порождает сомнение в точности результатов. Пользователям необходимо понимать, как ИИ приходит к выводам и чем руководствуется, давая рекомендации.ИИ от IBM Watson for Oncology, предоставляющий рекомендации по лечению рака, так и не смог заслужить доверие онкологов.При взаимодействии с Watson врачи оказывались в двоякой ситуации. Когда указания программы совпадали с мнениями медиков, те не видели большой ценности в рекомендациях ИИ, поскольку они не меняли лечения. Скорее, врачи просто укреплялись в собственной правоте. Но подтвердить, что ИИ улучшает статистику выживаемости с раком, не удалось.Если рекомендации Watson противоречили мнению экспертов, те не доверяли программе, считая её некомпетентным. Алгоритмы машинного обучения были слишком сложными для понимания, врачи не видели объяснений эффективности лечения ИИ и полагались на свой опыт, игнорируя рекомендации ИИ.Врачи-онкологи расходились с программой Watson почти в 70% случаях, поэтому медицинские учреждения решили отказаться от ее использования.Watson столкнулся с проблемой недоверия. Программа работает по сложной системе анализа данных, что лишает большинства понимания, как он принимает решения. Взаимодействие вызывает ощущение потери контроля. Люди предпочитали полагаться на свой опыт, а не на ИИ, за которым не могли следить в режиме реального времени. ИИ не считается абсолютно надежным в том числе потому, что создаётся людьми. То, что нельзя полагаться на эти технологии, подтверждается примерами ошибок, громогласно освещаемых в СМИ. В частности, то ДТП с автопилотом Tesla, закончившееся смертельным исходом. Приватность данныхАнализ метаданных может помочь улучшить качество работы ИИ, но он также вызывает вопросы о приватности и безопасности личной информации. Таким образом, при создании системы ИИ необходимо обеспечить надёжность хранения данных и защитить их от кражи или злоупотребления.Так, исследование американских университетов Принстона и Беркли  и Швейцарской высшей технической школы Цюриха совместно с Google и DeepMind подтвердило вероятность утечки данных из систем создания изображений с применением ИИ – DALL-E, Imagen и Stable Diffusion.Исходное и сгенерированное изображение из Stable DiffusionВ ходе обучения в эти системы загружается масса изображений с сопутствующими описаниями. Исследование ставит под сомнение уникальность созданных на их основе картинок. При определённых условиях нейросеть может выдать в почти неизменённом виде оригинальное изображение, использованное ранее для обучения. Таким образом, существует вероятность того, что программа случайно раскроет личную информацию.Этические кодексы в сфере ИИВ связи с развитием нейросетей и машинного обучения крупнейшие ИТ-компании мира стали проявлять особый интерес к этике ИИ. В числе первых  в 2016 году были опубликованы «10 Законов для искусственного интеллекта» Microsoft, в которых от имени генерального директора компании Сатьи Наделлы указаны ключевые требования к развитию этики ИИ. Свой взгляд на этику ИИ представила и IBM.  Почти полсотни крупных ИТ-компаний по всему миру обладают собственными кодексами и правилами, основанные на этических принципах, относящихся к применению и развитию ИИ. В числе таких компаний российские ABBYY, Сбер и Яндекс.Значительную роль в развитии этики ИИ играют некоммерческие организации, объединяющие профессионалов, которые проводят исследования и внедряют научно-технические инновации. НКО продемонстрировали углублённый подход к этике ИИ, который позволяет принимать во внимание интересы и права потребителей, а также ставит на первый план общественные нужды и благо всех людей.Этические нормы и ценности сформулированы в 13 из «23 принципов искусственного интеллекта» на Асиломарской конференции в 2017 году. В числе тех, кто подписал их, – Илон Маск, Стивен Хокинг, Рэй Курцвайл и другие. Эти принципы отразились в корпоративных нормах ряда компаний, чья деятельность связана с разработкой ИИ. Асиломарская конференция Законодательное регулирование ИИ в России и за рубежом Россия, как одна из передовых стран по применению искусственного интеллекта (ИИ), также сталкивается с вопросами этики и правовой ответственности. Необходимо принимать меры для того, чтобы развитие ИИ шло по безопасному пути.C 1 июля 2020 года в Москве проводится пятилетний эксперимент, направленный на разработку и внедрение технологии ИИ для заинтересованного бизнеса. Ранее ИИ не регулировался законодательно. Между тем, использование таких технологий порождает проблемы, включая, но не ограничиваясь:необходимостью обезличивания и защиты персональных данных, особенно в области распознавания лиц, обеспечением доступа ИИ к большим массивам информации для полноценного развития, разграничением ответственности за действия ИИ и возможности доказательства.Закон 123-ФЗ от 24 апреля 2020 года устанавливает в Москве специальный правовой режим в сфере ИИ. В нём указаны требования по защите персональных данных граждан и использования псевдоданных, собираемых в режиме анонимности. В частности, допускается обработка обезличенных персональных данных граждан для реализации эксперимента.Цель этого экспериментального правового режима – повышение качества жизни населения, эффективности госуправления и деятельности бизнеса в ходе внедрения технологий ИИ. А также формирование комплексной системы регулирования общественных отношений, возникающих в связи с развитием и использованием ИИ. Эксперимент позволит создать правовую базу для этого. Регулирование использования ИИ разными странами мира проходит по-разному. Некоторые страны уже разработали законы и правила,  а другие только занимаются подготовкой соответствующих документов.Одна из основных областей, где применяются ИИ, – государственное управление. Использование ИИ в этой области может помочь улучшить качество услуг, снизить издержки и повысить эффективность государства в целом. Однако возникают вопросы о прозрачности и ответственности государственных систем ИИ.В России была создана рабочая группа по разработке принципов этики ИИ. В 2021 году в рамках I международного форума «Этика искусственного интеллекта: начало доверия» был принят российский «Кодекс этики в сфере ИИ». На 2023 год документ подписали более 150 российских организаций.Необходимость выработки этических норм и нормативного регулирования для ИИ прописана в «Национальной стратегии развития искусственного интеллекта на период до 2030 года».Ещё один важный документ – «Концепция развития регулирования отношений в сфере технологий искусственного интеллекта и робототехники на период до 2024 года».В ней сказано, что развитие технологий ИИ и РТ должно основываться на базовых этических нормах и предусматривать:Цель обеспечения благополучия человека должна преобладать над иными целями разработки и применения систем ИИ и РТ.Запрет на причинение вреда человеку по инициативе систем ИИ и РТ. По общему правилу, следует ограничивать разработку, оборот и применение систем ИИ и РТ, способных по своеи