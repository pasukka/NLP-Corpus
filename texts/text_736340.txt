

Атаки на чат-ботов и языковые модели. И как от них защититься / Habr


               Атаки на чат-ботов и языковые модели. И как от них защититься Level of difficulty  
    Medium
   Reading time  
    8 min
   Views  3.2K Information Security *Artificial Intelligence   
    From sandbox
       Всем привет! Я – Артем Семенов, занимаюсь тестированием на проникновение и работаю в RTM Group. В этой статье я расскажу об атаках на чат-боты и языковые модели, а также о том, как от них защититься. Чат-боты сегодня буквально повсюду. Наверняка даже ваша бабушка нет-нет, да и вставит в разговор это модное название. Для начала, давайте все же определимся, что мы имеем в виду, когда произносим это. Чат-боты — программы, которые взаимодействуют с пользователем через текстовые сообщения или голосовые команды. Они основаны на машинном обучении и используют нейронные сети для обработки запросов пользователя и предоставления ответов. Языковые модели — это системы, которые применяют нейронные сети для генерации текста на основе образцов. Они могут использоваться для создания текстовых сообщений, генерации описаний и т. д.Сегодня такие технологии находят все большее применение в различных отраслях благодаря их способности значительно упростить общение с клиентами и сократить затраты на персонал. Также с их помощью очень удобно создавать тексты разных объемов для разных задач в короткие сроки. Все классно, но нужно понимать, что где хайп, там и хакеры. И уже существуют и применяются различные типы атак для взлома модных ботов. Главные векторы атак на языковые модели сегодня Prompt injection «Prompt Injection» («инъекция промпта») — это метод атаки на модели машинного обучения, которые используют промт. Путем внедрения злоумышленником ложных промптов или модификации существующих.Идея заключается в том, чтобы изменить или добавить промпты, которые применяются для обучения модели, таким образом, чтобы вводные данные были искажены и выходные данные не соответствовали им. Это может привести к некорректной обработке данных моделью и, следовательно, к ошибочным результатам.Для проведения атаки злоумышленник может использовать различные методы, такие как перебор промтов, генерация или анализ содержания промптов, чтобы определить наиболее эффективные изменения, которые не будут вызывать слишком большой процент ошибок и при этом смогут негативно повлиять на работу модели.  Для защиты от атаки «Prompt Injection» возможно использовать различные методы, такие как контроль качества данных, защита промптов от изменений и валидация входных данных. Также необходимо обеспечить безопасность конфиденциальных сведений, используемых для обучения модели, и ограничить доступ к ним только авторизованным пользователям.В целом, атака «Prompt Injection» является серьезной угрозой для моделей машинного обучения, которые применяют возможность ввода промотов. Поэтому важно проводить систематический анализ уязвимостей и реализовывать соответствующие меры защиты для предотвращения таких атак.Возможен также сценарий, в котором запрос хакера содержит команды Linux. Этот тип атаки называется OS