

Устранение галлюцинаций в LLM / Habr


               Устранение галлюцинаций в LLM Level of difficulty  
    Medium
   Reading time  
    8 min
   Views  3.1K Machine learning *Artificial Intelligence   
    Translation
     
                Original author:
                
                  Sergei Savvov
                  Поговорим о том, почему LLMs говорят неправду и как это исправить.Image generated by Stable Diffusion  Перевод статьи Сергея Саввова.Large Language Models (LLMs) на данный момент могут генерировать быстрые ответы на различные запросы пользователя. Однако их склонность подтасовке фактов (или галлюцинациям) порой подрывают доверие. I think we will get the hallucination problem to a much, much better place… it will take us a year and a half, two years. — OpenAI CEO Sam Altman Правдив ли этот ответ?Разработчики активно используют модели, однако ограничения вставляют "палки в колеса", поскольку система должна соответствовать требованиям качества, надежности и обоснованности. К примеру, всегда ли можно доверять коду, который был сгенерирован LLM? А ответам на вопросы о страховании, медицине, законах?В этой статью мы рассмотрим, что представляют из себя галлюцинации LLMs, проведем несколько экспериментов и попробуем решить проблему надежности и правдивости ответов.  Информация в статье актуальна по состоянию на август 2023 года, но, пожалуйста, имейте в виду, что впоследствии могут произойти изменения.   Сравнение точности разных подходов по снижению галлюцинацийГаллюцинации LLMs возникают из-за сжатия и несогласованности данных. Обеспечение качества является сложной задачей, поскольку многие датасеты могут быть устаревшими или ненадежными. Для уменьшения вероятности ошибки:  Отрегулируйте температурный параметр, чтобы ограничить креативность модели. Обратите внимание на промпт-инжиниринг. Попросите модель думать шаг за шагом и предоставить факты и ссылки на источники в ответе.  Используйте внешние источники знаний, чтобы сделать проверку ответов качественнее.   Сочетание этих подходов позволит достичь наилучших результатов. Что такое LLM-галлюцинация?   Пример фальсификации фактов: всего на Луне побывало 12 человек В статье the Center for Artificial Intelligence Research галлюцинация LLM определяется так: “сгенерированный контент бессмыслен или не соответствует предоставленному исходному контенту”.  Галлюцинации можно разделить на несколько видов:Логические ошибки: модель допускает ошибки в своих рассуждениях, предоставляя неправильные ответы.Фальсификация фактов: вместо ответа “я не знаю” модель уверенно утверждает несуществующие факты. Пример: Чат-бот Bard от Google с искусственным интеллектом допускает ошибку в первой демонстрации.Отклонения моделей: отсутствие беспристрастного отношения к чувствительным топикам может привести к неожиданным результатам Пример: Политические предубеждения, обнаруженные в моделях NLP.Почему LLM галлюцинируют  Мне понравилась идея из этой статьи: сжатие данных для обучения модели приводит к галлюцинациям. Давайте рассмотрим коэффициенты сжатия для некоторых популярных моделей:           Сжатие данных обученияКонечно, причиной подобного сжатия является то, что генеративная модель хранит математическое представление взаимосвязи (вероятностей) между входными данными (текстом или пикселями) вместо самих входных данных. Что еще более важно, это представление позволяет извлекать знания (путем выборки или выполнения запросов / промптов).Такое сжатие снижает точность воспроизведения, аналогично формату JPEG, как обсуждалось в статье New Yorker. По сути, полное восстановление исходных знаний становится трудной или практически невозможной задачей. Склонность моделей к несовершенному "заполнению пробелов" или галлюцинациям является компромиссом в пользу такого сжатого, но полезного представления знаний.LLMs также галлюцинируют, когда их обучающий датасет содержит ограниченную, устаревшую или противоречивую информацию о заданном им вопросе. Последнее чаще  заметно для редких или спорных фактов.Подготовка к эксперименту   Цель этой статьи — разработать и протестировать практические шаги по уменьшению галлюцинаций и повышению производительности систем. Для этого я просмотрел различные датасеты и остановился на TruthfulQA Benchmark.   Пример вопроса  Хотя в датасете есть проблемы, такие как расхождения между правильными ответами и их источниками, он остается наиболее подходящим вариантом из-за разнообразия тем и всестороннего охвата. Также было удобно, что данные представлены в формате викторины, что облегчило тестирование модели. Можно легко запросить ответ в формате JSON:… Return response in JSON format, for example: [{“class”: “A”}]Я использовал датасет с 800 строками, используя GPT-3.5 turbo.Другие датасеты для оценки влияния галлюцинаций:Knowledge-oriented LLM Assessment benchmark (KoLA)TruthfulQA: Measuring How Models Mimic Human FalsehoodsMedical Domain Hallucination Test for Large Language ModelsHaluEval: A Hallucination Evaluation Benchmark for LLMsСнижение температуры  Температура модели относится к скалярному значению, используемому для корректировки распределения вероятностей, предсказанного моделью. В случае LLMs это баланс между соблюдением того, что модель усвоила из обучающих данных, и генерированием более разнообразных или творческих ответов. Как правило, эти творческие ответы чаще подвержены галлюцинациям.   Сравнение экспериментальных результатов по снижению температуры  Для задач, требующих достоверности, стремитесь к подробному контексту и установите значение temperature=0, чтобы ответы основывались на контексте.  Chain of Thought Prompting and Self-Consistency  Ошибки бенчмарка часто можно исправить, улучшив дизайн вашего промпта. Вот почему я уделил больше внимания этой теме.LLMs часто дают сбои в пошаговых задачах на рассуждение, таких как арифметика или логика. Недавние работы показывают, что примеры разделения задачи на шаги повышает производительность. Примечательно, что простой промпт вроде “Let’s think step by step” без конкретных примеров приводит к аналогичным улучшениям.Многие статьи посвящены техникам цепочки рассуждений. По сути, они направлены на то, чтобы заставить модель мыслить шаг за шагом и перепроверять себя. Ниже приведены некоторые выдающиеся подходы:Схема, иллюстрирующая различные подходы к решению проблем с помощью LLMТеперь давайте углубимся в каждый метод и оценим их качество на основе датасета.  1. Chain of Thoughts (Цепочка мыслей) (CoT)   Основная идея Chain of Thoughts состоит в том, чтобы добавить к промпту условие “Думай шаг за шагом”:Think step by step before answering and return response in JSON format, for example: [{“class”: “A”}]”Метрика: Accuracy = 58%2. Self Consistency with CoT (Самосогласованность) (CoT-SC)   Этот подход представляет собой улучшенную версию предыдущей идеи. Мы просим модель дать несколько ответов, а затем выбираем лучший из них путем голосования:Think step by step before answering and give three answers: if a domain expert were to answer, if a supervisor were to answer, and your answer. Here’s the response in JSON format:Метрика: Accuracy = 57%3. Tree of Thoughts (Дерево мыслей) (ToT)  Это фреймворк, который обобщает подсказки по цепочке мыслей и поощряет исследование мыслей, которые служат промежуточными шагами для решения общих проблем с помощью языковых моделей. Такой подход позволяет LMM самостоятельно оценить промежуточный прогресс, достигнутый в решении проблемы с помощью процесса обдуманного рассуждения. Пример ToT промпта:Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realises they’re wrong at any point then they leave. Here’s the response in JSON format:Метрика: Accuracy = 37%4. Tagged Context Prompts (Контекстные промпты с тегами)  Данный метод включает в себя генерацию вопросов, создание и проверку промтов с помощью саммари и проверку вопросов.Учитывая сложность генерации дополнительного датасета, я скорректировал свой подход, запросив ссылку на источник и факты:  Схема, иллюстрирующая мою версию контекстных промптов с тегами  Provide details and include sources in the answer. Return response in JSON format, for example:[{“class”: “A”, “details”: “Human blood in veins is not actually blue. Blood is red due to the presence of hemoglobin”, “source”: “https://example.com"}]Метрика: Accuracy = 61%5. Self-Correct (самоконтроль)  Возможно, это один из наиболее продвинутых методов промпт-инжиниринга. Идея заключается в том, чтобы заставить модель перепроверить и раскритиковать ее результаты, которые находятся ниже:   Схематическая иллюстрация проверки выходных данных   Choose the most likely answer from the list [“A”, “B”, “C”, “D”, “E”]. Then carefully double-check your answer. Think about whether this is the right answer, would others agree with it? Improve your answer as needed.Return response in JSON format, for example: [{“first_answer”:”A”, “final_answer”:”B”}]Метрика: Accuracy = 58% 6. Several Agents (несколько агентов)   Схема, иллюстрирующая подход нескольких агентов  Несколько агентов LMM предлагают и обсуждают свои индивидуальные ответы и процессы рассуждения в течение нескольких раундов, чтобы прийти к общему окончательному ответу. Этот подход включает в себя промпты:Промпт 1:Give the facts and your thoughts step by step to find the right answer to this question: {QUESTION}Промпт 2:Using the solutions from other agents as additional information, choose the correct answer choice: {QUESTION} {ANSWERS}. Return response in JSON format…Оценка: Accuracy = 54%Я бы не рекомендовал использовать этот подход в реальных приложениях, потому что вам нужно сделать два или более запросов. Это не только увеличивает затраты на API, но и замедляет работу приложения. В моем случае потребовалось более двух часов, чтобы сгенерировать ответы на 800 вопросов.Используйте внешнюю базу знаний  Как было сказано ранее, галлюцинации в LLM возникают из-за попытки восстановить сжатую информацию. Вводя соответствующие данные из базы знаний во время прогнозирования, мы можем преобразовать проблему чистой генерации в более простую задачу поиска или обобщения, основанную на предоставленных данных.Поскольку на практике извлечение релевантных данных из базы знаний нетривиально, я сосредоточился на небольшой выборке (~ 300 строк) из собранного мной датасета. Схематическая иллюстрация использования внешних источников  По итогу, мой промпт выглядел следующим образом:Using this information {INFORMATION} choose the correct answer {QUESTION} and return response in JSON format…Метрика: Accuracy = 65%Необходимо завершить некоторой дополнительной работой. В частности, нужно отфильтровать и оценить извлеченные фрагменты и определить, какую часть контекста передавать в модель.. Кроме того, поиск и оценка могут вызвать задержки, которые очень важны для взаимодействия в режиме реального времени.Другим интересным подходом является Retrieval-Augmented Generation (RAG), которая объединяет возможности поиска и генерации текста в больших языковых моделях. Этот подход объединяет систему ретривера для извлечения соответствующих фрагментов документа из обширного корпуса с LLM, которая генерирует ответы на основе извлеченной информации. Схематическая иллюстрация RAG, изображение от Хайко ХотцНекоторые статьи по теме:Гипотетическое внедрение документа (ГАЙД) — В документе предлагается использовать первоначальный ответ от LLM в качестве мягкого запроса при извлечении соответствующих отрывковСмягчение галлюцинации языковой модели с помощью интерактивного вопроса-выравнивание знанийREVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge MemoryRAG vs Finetuning — Which Is the Best Tool to Boost Your LLM Application?Промпт-инжиниринг и внешняя база знаний  Этот подход сочетает в себе предыдущие пункты. Используются различные методы промпт-инжиниринга и внешняя база знаний. Я реализовал логику из фреймворка CRITIC:  Фреймворк CRITICUsing this information {INFORMATION} choose the correct answer {QUESTION} Then carefully double-check your answer. Think about whether this is the right answer, would others agree with it? Improve your answer as needed.Return response in JSON format, for example: [{“first_answer”:”A”, “final_answer”:”B”}]Метрика: Accuracy = 67%Качество не сильно улучшилось по сравнению с предыдущим пунктом. Я думаю, что это связанно с проблемными вопросами в датасете, о которых я писал в начале статьи. Некоторые “правильные” ответы не соответствуют информации из источников.   Подведем итог    Используя методы, описанные в статье, мы устранили галлюцинацииС одной стороны, снижение галлюцинации не такая сложная задача: уменьшите температуру, поиграйтесь с промптами, добавьте внешние знания. С другой стороны, у каждого из подходов есть много нюансов.Мой основной совет: отдавайте приоритет промпт-инжинирингу — это самый экономичный и действенный способ устранения галлюцинаций.Полезные ссылки  Practical Steps to Reduce Hallucination and Improve Performance of Systems Built with Large Language Models - Одна из лучших статей, которые я нашел.Reading list of hallucinations in LLMs - полезное хранилище GitHub с различными ссылками о галлюцинациях в LLM.Если у вас есть какие-либо вопросы или предложения, не стесняйтесь обращаться в LinkedIn.      Tags: mlopsmachine leraningchatgptdata scienceai  Hubs: Machine learningArtificial Intelligence          


