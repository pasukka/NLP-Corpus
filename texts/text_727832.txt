

Infinityformer: новый подход к обработке длинных последовательностей большими языковыми моделями / Habr


               Infinityformer: новый подход к обработке длинных последовательностей большими языковыми моделями Level of difficulty  
    Hard
   Reading time  
    4 min
   Views  1.8K Machine learning *Artificial Intelligence       Промт: Значок бесконечности, робот и печатная машинка на чёрном фоне - Kandinsky 2.1Трансформеры - это отличные нейросети для работы с текстом, речью, изображениями и другими типами данных, но они сталкиваются с проблемой ограниченной длины контекста, к которому они могут обращаться. Чем длиннее последовательность, тем больше вычислений требуется для того, чтобы трансформер мог учитывать все элементы. Это приводит к тому, что трансформеры не могут эффективно моделировать долгосрочные зависимости и запоминать важную информацию из прошлого. Infinityformer решает эту проблему, используя непрерывный механизм внимания, который позволяет обращаться к прошлому контексту как к непрерывному сигналу, а не как к дискретной последовательности. Это делает сложность внимания Infinityformer независимой от длины контекста и позволяет моделировать произвольно длинные последовательности и поддерживать “липкие памяти”(sticky memories), которые выделяют наиболее значимые события из прошлого.В этой статье я максимально хардкорно расскажу о том, как работает Infinityformer.Как работает InfinityformerInfinityformer - это модель, которая состоит из двух частей: короткосрочной памяти (short-term memory, STM) и долгосрочной памяти (long-term memory, LTM). STM - это стандартный трансформер, который обрабатывает текущий контекст, а LTM - это дополнительная память, которая хранит информацию из прошлого контекста. Для того, чтобы связать STM и LTM, Infinityformer использует непрерывный механизм внимания, склеивание и сжатие памяти и липкие памяти.Непрерывный механизм внимания для работы с неограниченной долгосрочной памятью Непрерывный механизм внимания - это способ обращаться к LTM как к непрерывному сигналу, а не как к дискретной последовательности. Это позволяет уменьшить сложность внимания с квадратичной до линейной относительно длины контекста. Непрерывный механизм внимания работает так: сначала он применяет к LTM функцию ядра, которая преобразует дискретные элементы в непрерывные значения. Затем он вычисляет сходство между STM и LTM с помощью скалярного произведения. Наконец, он вычисляет веса внимания с помощью функции активации и получает выходное представление.  Склеивание и сжатие памяти для уменьшения сложности вычислений  Склеивание и сжатие памяти - это способ управлять размером LTM и поддерживать ее неограниченность. Склеивание памяти заключается в том, что каждый новый элемент из STM добавляется в конец LTM, образуя более длинную последовательность. Сжатие памяти заключается в том, что каждые два соседних элемента в LTM объединяются в один элемент с помощью линейного преобразования. Это уменьшает длину LTM вдвое и сохраняет ее информативность.Липкие памяти для выделения важных событий из прошлого контекстаЛипкие памяти (sticky memories) - это способ контролировать, какие элементы из LTM более важны для текущего контекста и должны быть лучше представлены в выходном представлении. Липкие памяти работают так: сначала они вычисляют важность каждого элемента из LTM с помощью функции оценки, которая зависит от его содержания и положения. Затем они выбирают некоторое количество наиболее важных элементов с помощью метода выборки по важности. Они увеличивают веса внимания для выбранных элементов с помощью функции усиления.Функция оценки определяет, насколько элемент из LTM релевантен для текущего контекста. Она может учитывать разные факторы, такие как частота появления элемента, его семантическое значение, его связь с другими элементами и т.д. Например, функция оценки может быть пропорциональна обратной дистанции между элементом и текущим контекстом, что означает, что более новые элементы имеют большую важность.Метод выборки по важности определяет, какие элементы из LTM будут использоваться как липкие памяти. Он может быть основан на разных стратегиях, таких как случайный выбор, топ-k выбор или сэмплирование по распределению важности. Например, метод выборки по важности может выбирать k элементов из LTM с вероятностью, пропорциональной их важности.Функция усиления (boosting function) определяет, насколько сильно будут увеличены веса внимания для липких памятей. Она может быть константой или зависеть от разных факторов, таких как важность элемента, его дистанция от текущего контекста или его сходство с другими элементами. Например, функция усиления может быть экспоненциальной функцией от важности элемента, что означает, что более важные элементы получают больший прирост внимания.Липкие памяти позволяют Infinityformer выделять наиболее значимые события из прошлого контекста и использовать их для обогащения текущего контекста. Это улучшает качество моделирования долгосрочных зависимостей и повышает точность решения разных задач.ВыводЭта модель показала отличные результаты на разных задачах, связанных с обработкой последовательностей, таких как сортировка, языковое моделирование и диалоговая генерация. В будущем можно улучшить Infinityformer, используя более сложные функции оценки, выборки и усиления для липких памятей, а также адаптивные методы для склеивания и сжатия памяти. Infinityformer открывает абсолютно новые возможности для исследования и применения трансформеров на длинных последовательностях и предлагает новый подход к реализации неограниченной памяти.      Tags: transformersllmнейросетьнейронные сетиkandinsky art  Hubs: Machine learningArtificial Intelligence          


