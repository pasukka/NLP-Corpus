

Делаем параллельный корпус из книг с помощью sentence embeddings / Habr


              31  August  2020 at 13:56  Делаем параллельный корпус из книг с помощью sentence embeddings Python *Data Mining *Machine learning *Learning languages Natural Language Processing * 
        Tutorial
           
При поиске параллельных корпусов для своих нужд, — это может быть обучение модели машинного перевода или изучение иностранного языка, можно столкнуться с тем, что их не так уж и много, особенно, если речь идет не об английском, а каком-то редком языке. В этой статье мы попробуем создать свой корпус для популярной языковой пары русский-немецкий на основе романа Ремарка "Три товарища". Любителям параллельного чтения книг и разработчикам систем машинного перевода посвящается.
Задача
Такая задача называется выравниванием текстов и может быть до какой-то степени решена следующими способами:

Использовать эвристики. Можно считать количество предложений в текстах, количество слов в них и на основе этого производить сопоставление. Такой способ не дает хорошего качества, но может тоже быть полезным.
Использовать sentence embeddings. Наверняка вы слышали про модели типа word2vec или sent2vec или видели такой пример их использования — "король" + "женщина" — "мужчина" = "королева". Если коротко, то суть в том, чтобы перевести слова (предложения, тексты) в векторное пространство с сохранением семантического расстояния между ними. Такой подход открывает перед нами замечательные возможности по оценке близости текстов и их кусочков по смыслу.

Модели
В качестве моделей, из которых мы будем брать эмбеддинги, возьмем Universal Sentence Encoder, Sentence Transformers и недавно вышедший LaBSE (Language Agnostic BERT Sentence Embeddings).
Как бы не была хороша модель, одна она не справится, потому что при переводе текста с одного языка на другой переводчик частенько может объединять два предложение в одно, а одно сложное предложение разбивать на несколько простых. Особенно это характерно при переводе с русского на китайский. Для таких случаев можно придумать эвристики, наподобие такой — если длинна предложений отличается больше чем в два раза, то либо разбиваем одно из них по знаку препинания, либо склеиваем короткое со следующим или предыдущим. Потом оставляем наиболее близкие пары.
Модели, которые мы будем использовать, являются не только мультиязыковыми, но и выровненными, это означает, что, если им на вход подавать предложения на разных языках, то вектора все равно будут сохранять между собой семантическое расстояние, — вектор для "I love cats" будет близок вектору "Я люблю кошек". Разумеется, модель должна поддерживать необходимые нам языки, список таких приведен в таблице 1. Самый быстрый инференс у USE, если вам нужны более-менее редкие языки, то используйте xlm-r-100langs-bert-base или LaBSE.
Таблица 1. Multilingual sentence embedding models



Модель
Поддерживаемые языки
Размер весов
Длина embedding'а




sentence transformers/distiluse-base-multilingual-cased
13 языков (английский, арабский, испанский, итальянский, китайский, корейский, немецкий, нидерландский, польский, португальский, русский, турецкий и французский)
500Mb
512


Universal Sentence Encoder
15 языков (те же плюс тайский и японский)
250Mb (300Mb large version)
512


sentence transfomers/xlm-r-100langs-bert-base
100 языков*, полный список
1Gb
768


LaBSE
109 языков, полный список
1.63Gb
768



*В документации sentence transformers отмечается, что модели также дают неплохой результат для языков вне поддерживаемого списка.
Выравниватель
Чтобы любой мог сразу же поиграться с моделями и что-то сделать самостоятельно, эксперименты будем проводить в Colab'е, общедоступном jupyter блокноте. Все ссылки будут в конце статьи.
Нам понадобятся библиотеки для разбиения текстов на предложения и загрузки предобученных моделей. Установим их в нашу среду приступим к выравниванию.
!pip3 install razdel
!pip3 install sentence-transformers
import re
import seaborn as sns
import numpy as np
from scipy import spatial
from matplotlib import pyplot as plt
import razdel
from sentence_transformers import SentenceTransformer
Предобработка
Итак, возьмем первую главу замечательного романа "Три товарища" в оригинале (1936 год) и в переводе И. Шрайбера и

Л. Яковленко (1959 год). Первым шагом будет разбить текст на предложения и посмотреть насколько сильно они отличаются по количеству. Так это просто сырые тексты из интернета, предварительно немного почистим текст. По предложениям текст разобъем razdel'ом (бывшая библиотека natasha), для немецкого языка такой способ тоже подойдет, если мы позаботимся о кавычках (в немецком языке они обратные — »«).
double_dash = re.compile(r'[--]+')
quotes_de = re.compile(r'[»«]+')
ru = re.sub('\n', ' ', text_ru)
ru = re.sub(double_dash, '—', ru)
de = re.sub('\n', ' ', text_de)
de = re.sub(quotes_de, ' ', de)
sent_ru = list(x.text for x in razdel.sentenize(ru))
sent_de = list(x.text for x in razdel.sentenize(de))
Предложения на русском:
['Небо  было желтым, как латунь; его еще не закоптило  дымом.',
 'За крышами фабрики оно светилось особенно сильно.',
 'Вот—вот  должно было взойти солнце.',
 'Я посмотрел на часы — еще  не было восьми.',
 'Я пришел  на  четверть часа раньше обычного.',
 'Я открыл  ворота  и подготовил насос бензиновой  колонки.',
 'Всегда в это время уже подъезжали заправляться первые машины.',
 'Вдруг за  своей спиной я услышал хриплое кряхтение,  — казалось, будто под  землей  проворачивают ржавый винт.',
 'Я остановился  и прислушался.',
 'Потом пошел через  двор  обратно в  мастерскую  и  осторожно  приоткрыл  дверь.']
И на немецком:
['Der Himmel war gelb wie Messing und noch nicht verqualmt vom Rauch der Schornsteine.',
 'Hinter den D