

Как создание бинарного классификатора открыло ящик Пандоры в стандартах владения английским языком / Habr


               Как создание бинарного классификатора открыло ящик Пандоры в стандартах владения английским языком Level of difficulty  
    Medium
   Reading time  
    5 min
   Views  16K Python *Machine learning *Artificial Intelligence Learning languages Natural Language Processing *  
    From sandbox
       Владение английским языком принято оценивать по системе CERF (Common European Reference Framework), состоящей из шести уровней, где уровень A1 – начинающие, а уровень С2 – профессионально владеющие иностранным языком. Международный уровень С2 часто позиционируется как “уровень образованного носителя”, и получение соответствующего сертификата зачастую является либо заветной мечтой, либо предметом гордости преподавателя-лингвиста. Однако я не встречала в научной литературе доказательства полного соответствия уровня С2 уровню владения английским как родным. На самом деле, среди ученых нет единого мнения о том, возможно ли вообще изучающим язык достичь уровня, идентичного владению языком как родным (вот две статьи с практически одинаковым названием и противоположными выводами [1; 2]). Проведя небольшой опрос в одной из соцсетей, я увидела, что большинство моих коллег-преподавателей английского в глубине души все-таки считают, что «между уровнем носителя и уровнем С2 – бездна». Хотя были и те, кто выбрал вариант, что С2 – это действительно уровень образованного носителя. Так есть разница или нет? Я решила разобраться, рассмотрев для начала всего лишь один аспект владения языком – письменную речь. О своем эксперименте, в котором не обошлось без искусственного интеллекта, я и хочу рассказать.Вначале я создала опрос на Google Forms и предложила 17 русскоговорящим коллегам следующий челлендж: определить, написан ли английский текст носителем языка (британцем) или русскоязычным автором с уровнем английского С1-С2. Всего было 20 текстов. К исследованию приглашались эксперты с большим опытом проверки студенческих эссе и чтения оригинальных текстов, но, тем не менее, задача оказалась не из простых. Подсчитав вручную метрики, получаем: Accuracy = 0.6617; Precision = 0.6627; Recall = 0.6588; F1 = 0.66. Замечу, что этот опрос я также предлагала коренным британцам (пока только троим), и предварительный результат примерно тот же.На этом можно было бы остановиться, сделав успокаивающий вывод о том, что разницы между носителями и продвинутыми пользователями в написании текстов нет, поскольку эксперты не смогли ее обнаружить.    Но что-то заставило меня попытаться копнуть глубже, применив свои скромные знания в Deep Learning. Так появилась модель бинарного классификатора на основе XLM-Roberta, которая научилась отличать тексты, написанные носителями, от эссе русскоговорящих авторов уровня С1-С2. Расскажу подробнее.Первым этапом было создание базы текстов. Коллеги, готовящие к международным экзаменам продвинутых студентов (зачастую являющихся учителями), пожертвовали на науку 160 сочинений в полуофициальном газетном стиле (в жанрах статьи, эссе, обзора и письма). Их я поделила на обучающую, тестовую и валидационную выборки в пропорции 70% : 15% : 15%, как это и принято делать.Для базы текстов носителей я решила использовать готовый датасет, подготовленный в Кембридже [3]. Взяла за основу 160 текстов из тех, что используют при оценке навыка чтения на международном кембриджском экзамене CPE (Cambridge Proficiency Exam). Мне казалось, что это должны быть аутентичные тексты, но что-то пошло не так. Использование текстов обучающей базы Cambridge English Readability Dataset (2016) дало очень низкий результат (Accuracy = 0,57). И снова можно было бы предположить, что дело в отсутствии разницы между текстами и, следовательно, уровнями владения языком. Но более пристальное изучение текстов Кембриджского датасета показало, что в них встречаются слова, которые Кембриджским словарем [4] маркируются как устаревшие (например, “brouhaha”). Когда именно были написаны экзаменационные тексты – авторы датасета не указывают, но, вероятно, это было примерно в 90-е годы прошлого века. Также можно предположить, что тексты редактировались под формат экзамена или же писались специально для него. Помимо всего прочего, в большинстве текстов встречаются ошибки в оформлении – такие, как отсутствие пробелов и знаков препинания между заголовком и текстом, «склеенные» предложения (без пробелов между ними), а также отсутствие апострофов (e.g. “concert-goers experience”). Безусловно, все это могло явиться помехой для обучения нейросети.Убедившись в несовершенстве кембриджских наработок, мой внутренний перфекционист попросил меня постараться получше и собственноручно собрать базу текстов, написанных носителями. Что я и сделала, использовав онлайн сайты известных британских изданий, таких, как The Independent, The Guardian, Reader’s Digest UK, The Vogue UK, The Evening Standard и других. При отборе текстов учитывалась жанровая специфика и объем текстов, с прицелом на то, что больше 512 токенов за раз модель не осилит все равно. Также решено было отказаться от заголовков, поскольку их наличие само по себе может стать маркером для модели. И что же? Благодаря работе с базой результат вырос до Accuracy = 0.957. Впоследствии его удалось еще немного улучшить посредством разнообразных «танцев с бубном», и в результате модель работает с такими метриками: Accuracy = 0.9782; Precision = 1.0; Recall = 0.9583; F1 = 0.9787. И вот это уже становится интересным для меня как лингвиста.Тот же опрос, который я предлагала коллегам-экспертам, я прогнала через свой классификатор. Он ошибся в одном тексте из 20 – носителя принял за не-носителя. Итого, Accuracy = 0.95; Precision = 1; Recall = 0.9; F1 = 0.947. К слову, ни один из респондентов-экспертов не прошел опрос с такой точностью. Таким образом, AI классификатор справился с задачей бинарной классификации английских текстов по родному языку авторов гораздо лучше квалифицированных специалистов. Что позволяет сделать несколько любопытных выводов: 1) разница, которую мы искали, существует. При помощи возможностей искусственного интеллекта мы доказали, что англоязычный письменный текст, создаваемый носителями языка, по некоторым системным характеристикам, весьма вероятно, отличается от письма носителей русского языка, владеющих английским на уровне С1-С2 по системе CERF;2) искусственный интеллект с гораздо более высокой точностью распознает эти отличия, по сравнению с экспертами-людьми.Результаты исследования, кажется, дают пищу для размышлений. Сразу оговорюсь, что своей работой мне совсем не хотелось бы провоцировать “native-speakerism”, поскольку я против дискриминации преподавателей по родному языку. Очень часто знание русского языка – большое преимущество преподавателя английского. Я, например, специализируюсь на постановке британского произношения, и после многочисленных стажировок в Великобритании убедилась, что редкий британец поставит русскому студенту британские звуки так, как это сделает профессиональный русскоговорящий фонетист, опираясь на родную артикуляционную базу студента и свой личный опыт постановки звуков. Тем не менее, надо признать, что разница в создании письменных текстов, которую удалось достоверно обнаружить только с помощью ИИ, доказывает существование определенной «серой зоны» между уровнем С2 и английским как родным. И изучение этой серой зоны позволило бы, во-первых, улучшить понимание процессов создания письменных текстов на английском языке, а во-вторых, помогло бы как преподающим язык, так и изучающим его эффективнее развивать навыки письма. И последнее размышление, которое напрашивается по итогам эксперимента: если эксперты-преподаватели не смогли отличить студента от носителя, а ИИ смог, не открывает ли это дверь в мир, где уровень владения людей, по крайней мере, некоторыми языковыми навыками будет определять не человек? Ссылки:1.       Is it possible to achieve native-like competence in second language acquisition? – L.D.G. Martha Adriana Maza Calvino. – Tlatemoani. Revista Academica Investigacion, 2011, 9pp.2.       Is native-like competence possible in L2 acquisition? – Sylvina Montrul, Roumyana Slabakova. - Proceedings of the 25th BUCLD, 2001, 13 pp.3.       Menglin Xia, Ekaterina Kochmar and Ted Briscoe (2016). Text Readability Assessment for Second Language Learners. Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications.4. Cambridge Dictionary Online https://dictionary.cambridge.org/      Tags: бинарная классификацияносители языкадатасеты по английскому языкуанглийский языктрансформернейросетьdeep learning  Hubs: PythonMachine learningArtificial IntelligenceLearning languagesNatural Language Processing          


