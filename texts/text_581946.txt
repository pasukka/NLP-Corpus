

Мы опубликовали модель, расставляющую знаки препинания и заглавные буквы в тексте на четырех языках / Habr


              6  October  2021 at 16:56  Мы опубликовали модель, расставляющую знаки препинания и заглавные буквы в тексте на четырех языках Python *Big Data *Machine learning *Natural Language Processing *      

При разработке систем распознавания речи мы сталкиваемся с заблуждениями среди потребителей и разработчиков, в первую очередь связанными с разделением формы и сути. Одним из таких заблуждений является то, что в устной речи якобы "можно услышать" грамматически верные знаки препинания и пробелы между словами, когда по факту реальная устная речь и грамотная письменная речь очень сильно отличаются (устная речь скорее похожа на "поток" слегка разделенный паузами и интонацией, поэтому люди так не любят монотонно бубнящих докладчиков).
Понятно, что можно просто начинать каждое высказывание с большой буквы и ставить точку в конце. Но хотелось бы иметь какое-то относительно простое и универсальное средство расстановки знаков препинания и заглавных букв в предложениях, которые генерирует наша система распознавания речи. Совсем хорошо бы было, если бы такая система в принципе работала с любыми текстами.
По этой причине мы бы хотели поделиться с сообществом системой, которая:

Расставляет заглавные буквы и основные знаки препинания (точка, запятая, дефис, вопросительный знак, восклицательный знак, тире для русского языка);
Работает на 4 языках (русский, английский, немецкий, испанский);
По построению должна работать максимально абстрактно на любом тексте и не основана на каких-то фиксированных правилах;
Имеет минимальные нетривиальные метрики и выполняет задачу улучшения читабельности текста;

На всякий случай явно повторюсь — цель такой системы — лишь улучшать читабельность текста. Она не добавляет в текст информации, которой в нем изначально не было.
Постановка задачи и метод ее решения
На вход подается предложение, записанное строчными буквами и без какой-либо пунктуации — как мы обычно получаем на выходе системы распознавания речи. Требуется разработать модель, восстанавливающую его грамотную запись в смысле использования заглавных букв и знаков препинания. Набор знаков препинания .,—!?- был выбран исходя из оценки того, отсутствие каких символов наиболее бросается в глаза в предложениях, которые в среднем встречаются в речи в целевых доменах.
Кроме того, модель разрабатывалась из предположения, что после, и только после каждого токена сетке следует проставить только одну метку — знака препинания или просто пробела. Это автоматически исключает сложные случаи расстановки пунктуации в начале предложения или из нескольких символов (самый яркий пример — разнообразная прямая речь в литературе). Это упрощение сделано намеренно, т.к. нам показалось, что такое ограничение более конструктивно, чем учет всех малочисленных краевых случаев. И вообще главная задача, скорее, в улучшении читабельности, чем в идеальной записи составной прямой речи с обращениями и описаниями.
Задачу предполагалось сразу решать для нескольких основных языков. При этом модель по построению может быть легко расширена на произвольное количество языков, с которыми мы работаем, при наличии потребности и соответствующих корпусов данных.
Решение виделось прежде всего в виде какой-то малой бертоподобной модели с классифицирующими слоями поверх. В качестве данных для обучения использовались приватные корпуса, уже имевшиеся в наличии.
Мы ознакомились с недавним решением смежной задачи от коллег, однако, для наших целей требовалась:

Более легкая модель с более общей специализацией;
Реализация, не использующая напрямую внешние АПИ и не имеющая такое большое число зависимостей.

В результате у нашего решения из значительных зависимостей только сам PyTorch, никаких специализированных библиотек.
Поиск базовой модели
По возможности мы хотели взять небольшую предобученную языковую модель. Однако, поиск по списку готовых моделей, доступных на https://huggingface.co/, дает не самые обнадеживающие результаты: требование мультиязычности и уменьшения модели, например, дистилляции, по сути приводят нас к единственной доступной опции, и то с весом в полгигабайта.
Здесь, наверное можно сделать ремарку про степень полезности "революции трансформеров" в NLP, но напрашивающиеся выводы мы оставим на усмотрение читателя.
Итоговый размер модели и ее сжатие
Мы экспериментировали с разными архитектурами, но в итоге остановились на самой простой, и итоговый размер модели составил 520 мегабайт. 
Такой размер дистилированного берта в основе нас не вполне устроил, и мы попробовали сжать обученную модель. Самый простой и эффективный способ — конечно, квантизация (причем сочетание статической и динамической как здесь) — и в результате модель была ужата до 130 мегабайт без значимой потери качества, потому всюду далее будут именно метрики финальной квантизованной модели.
Кроме того, мы сократили избыточный для нашей задачи словарь, выкинув токены для других языков, что позволило сжать эмбеддинг размера 120 тысяч токенов (размер токена — аж 768) до более приятных 75 тысяч. Наверняка можно было еще подрезать не самые часто используемые токены и применить оставшиеся более продвинутые методы сжатия моделей (факторизацию и замену механизма "внимания" на более простые аналоги), но мы решили остановиться, поскольку модель уже стала меньше 100 мегабайт.
Используемые метрики
Вопреки всеобщему тренду на сенсационализм и утрату рационального мышления, во всех наших статьях мы стараемся показывать максимально подробные, информативные и честные метрики на разных данных. В данном случае приведем метрики полученной модели на:

Валидационных сабсетах наших приватных текстовых корпусов (5,000 предложений на каждый язык);
Текстах аудиокниг, будем использовать датасет caito, в котором как раз есть тексты на всех языках, на которых обучалась модель (20,000 случайных предложений на каждый язык);

В качестве метрик в этой задаче используем:

WER (word error rate) в процентах, причем отдельно рассчитанный для пунктуации (оба предложения при этом приведены к строчному виду) — WER_p и для расставления заглавных букв (а здесь выбрасываем всю пунктуацию) — WER_c;
Precision / recall / F1 для проверки качества классификации: между пробелом и упомянутыми выше знаками пунктуации .,—!?-, а для расстановки заглавных букв — между классами токен из строчных букв / токен начинается с заглавной / токен из всех заглавных. Также для наглядности вы можете посмотреть на confusion матрицы;

Результаты
Для корректного и информативного расчета метрик из текстов были выброшены:

Пунктуационные символы помимо .,—!?-;
Пунктуация в начале предложения;
Пунктуационные символы после первого в случае комбинации из нескольких пунктуационных знаков;
Для испанского 