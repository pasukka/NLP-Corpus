

Модель-полиглот: как мы учили GPT-3 на 61 языке мира / Habr


              21  April   at 13:48  Модель-полиглот: как мы учили GPT-3 на 61 языке мира SberDevices corporate blog Open source *Machine learning *Artificial Intelligence Natural Language Processing *      Генеративные языковые модели уверенно обосновались в практике Natural Language Processing (NLP). Большие предобученные трансформеры двигаются сразу в трёх направлениях: мультимодальность, мультизадачность и мультиязычность. Сегодня мы расскажем про последнюю — о том, как учили модель на основе GPT-3 на 61 языке мира.Это — самая многоязычная авторегрессионная модель на сегодня. Такую модель можно использовать, например, чтобы создать вопросно-ответную систему, обрабатывающую тексты на многих языках, научить диалогового ассистента говорить на разных языках, а также сделать более универсальные решения для парсинга текста, извлечения информации.Этим релизом мы хотим привлечь внимание к развитию NLP для языков стран СНГ, а также народов России. Для многих из представленных языков эта модель стала первой авторегрессионной языковой моделью. Модель доступна в двух вариантах размеров: mGPT XL на 1,3 миллиарда параметров — в открытом доступе, а mGPT 13B — будет доступна в ML Space SberCloud.Страница модели Huggingface.СберДиск  (доступен wget).Генерация на разных языках зависит от многих факторов. Обучение на всех языках сразу должно давать неоспоримое преимущество в вопросах задач, связанных с информационным поиском, но что при этом происходит с качеством генерации, классификации объектов реального мира и повседневной жизни? Насколько на него влияют культурные особенности? Мы попробуем ответить на вопрос в этой статье.(Здесь и далее все затравки выделяем курсивом, дальше — генерация модели.)РусскийВ мае мы празднуем Первомай!!! Уважаемые наши студенты и жители города, придём вместе сегодня на площадь!АнглийскийIn May we celebrate the First Friday Special Mother’s Day (В мае мы отмечаем особенный День матери в первую пятницу.)Хинди