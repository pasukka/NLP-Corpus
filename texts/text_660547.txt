

«ПРО//ЧТЕНИЕ»: новый тест Тьюринга экспертов ЕГЭ? / Habr


               12  April   at 19:39  «ПРО//ЧТЕНИЕ»: новый тест Тьюринга экспертов ЕГЭ? «Антиплагиат» corporate blog Programming *Machine learning *Microservices *Natural Language Processing *      Введение
Бывает, что вам хочется прыгнуть выше головы? С нами такое случается… 
Время от времени в мире анализа данных проходят конкурсы с большими денежными призами. В 2006 году компания Netflix объявила о соревновании на миллион долларов по предсказанию оценок фильмов пользователями на основе их предпочтений. В 2019 году AWS, Facebook, Microsoft и другие компании объявили о конкурсе по распознаванию дипфейков Deepfake Detection Challenge с призом в $500,000 за первое место. Да и вообще такие многобюджетные конкурсы проводятся всё чаще и чаще, как видно в таблице ниже. В начале 2020 года в России стартовал конкурс «ПРО//ЧТЕНИЕ», не уступающий этим соревнованиям ни по масштабам, ни по амбициозности задачи. Организаторы конкурса «ПРО//ЧТЕНИЕ» предлагают разработать систему, проверяющую сочинения ЕГЭ по пяти школьным предметам. Общий призовой фонд конкурса составляет 260 млн руб., что с лёгкостью обгоняет описанные выше конкурсы. Участвовать может каждый, и окончательные итоги пока не подведены. 


Как известно, основным исследовательским интересом нашей компании Антиплагиат является область обработки текстов. Пройти мимо серьёзного конкурса по natural language processing с большим призовым фондом у нас не получилось. Под катом рассказ о нашем опыте участия в цикле 2020 года. 
Описание задачи: почему она крутая, важная, сложная и не только
«ПРО//ЧТЕНИЕ» — о чём это
Для начала кратко расскажем о самом конкурсе. Его организовали Российская венчурная компания, фонд «Сколково» и Агентство стратегических инициатив. Цель — разработать систему для автоматической проверки сочинений ЕГЭ по русскому языку, литературе, истории, обществознанию и английскому языку. Решение-победитель должно уметь находить ошибки в сочинениях на уровне экспертов ЕГЭ и, разумеется, делать это лучше остальных участников конкурса. Таким образом, нужно пройти своеобразный тест Тьюринга. Выглядит устрашающе… 

Помимо основного конкурса есть ещё так называемые номинации, в которых нужно уметь выделять определённые типы ошибок. При этом необязательно делать это лучше экспертов, достаточно войти в тройку победителей по используемой в соревновании метрике. О, теперь, кажется, не так сложно! 

Основная задача конкурса является непростой и увлекательной одновременно. С одной стороны, темы поиска причинно-следственных связей в тексте и понимания его смысла изучаются достаточно давно. Вспомнить хотя бы соревнования Recognising Textual Entailment, которые проводились с 2004 года. С другой стороны, современные state-of-the-art-подходы всё ещё далеки от имитации человеческой логики, и нет единого готового решения, которое после тюнинга могло бы найти все ошибки в тексте. 
Отдельно стоит упомянуть номинации. В первом этапе конкурса рассматривались наиболее исследованные типы ошибок: грамматические и речевые. Для них есть хорошо зарекомендовавшие себя готовые решения (проверка в Microsoft Word, Grammarly и т. д.), но они также далеки от идеала. 
Конкурс проходит отдельно для русскоязычных и англоязычных текстов циклами примерно раз в полгода до тех пор, пока не появится победитель. Впрочем, сроки проведения ограничены концом 2022 года, поэтому осталось не так много попыток получить главный приз. Каждый цикл состоит из двух этапов: 

 квалификационный, на котором достаточно показать, что команда умеет корректно работать с сочинениями в рамках регламента (необходимо получить текстовые файлы с сервера, выделить ошибки и смысловые блоки и отправить размеченные файлы обратно);
 финальный, на котором нужно так же проверить сочинения согласно регламенту, но решения команд будут оцениваться.

А что, собственно, нужно выделять
В соревновании организаторы предлагают локализовать следующие фрагменты:

 Грамматические ошибки
 Речевые ошибки
 Логические ошибки
 Фактические ошибки
 Этические ошибки
 Смысловые блоки

Для каждого типа фрагментов есть свои коды ошибок или блоков. Согласно техрегламенту, ошибка кодируется строкой вида x.z, где x — буква, указывающая тип эссе или группу ошибок, z — аббревиатура ошибки. Например, код л.абзац соответствует логической ошибке, связанной с нарушением абзацного членения и выделяемой в сочинениях по русскому языку. Также в сочинениях необходимо выделять смысловые блоки. Например, в сочинении по русскому языку нужно выразить своё отношение к позиции автора (блок «Отношение»), а в исторических сочинениях требуется указать события, связанные с заданным периодом (блок «Событие/СЯП»). 
Рассмотрим часть сочинения из сайта конкурса: 

На рисунке показан пример экспертной разметки сочинения. Зелёным цветом выделены смысловые блоки, а красным — ошибки. Команды должны подготовить модели, умеющие так же штрафовать учеников за неудачное употребление местоимений. О том, как сравнивать разметки, расскажем далее. 

Как измерять качество
Измерить качество решения такой «человеческой» задачи непросто. Как итог, метрика получилась очень специфичной: 

 Разметка алгоритма сравнивается с разметками одного текста несколькими независимыми экспертами.
 В качестве ground truth рассматривается средняя точность разметки эксперта.
 В качестве предсказания рассматривается средняя точность разметки алгоритма.
 Берём отношение 3-го пункта ко 2-му и — вуаля! — имеем метрику конкурса «ПРО//ЧТЕНИЕ».

Помимо сложного подсчёта, в такой метрике качества есть и другие особенности, которые привели к некоторым не очень хорошим последствиям в рамках первого этапа… Но обо всём по порядку. 
Выше мы употребили понятие «средняя точность разметки». Что же это значит? Для пары разметок сочинения ЕГЭ рассчитывают следующие метрики: 

 Точность предсказания оценки за эссе (M1)
 Точность и полнота поиска фрагментов (M2)
 Точность предсказания кодов (M3)
 Точность предсказания подтипов ошибок и комментариев (M4)
 Точность локализации фрагментов (M5)
 Точность исправлений ошибок (M6)
 Точность пояснений (M7)

Далее на основе метрик M1–M7 вычисляется их средневзвешенное S, которое и показывает, насколько пара разметок «схожа» между собой (на то она и метрика, что должна показывать меру сходства). На самом деле полученное число S немного обманчиво, но опять-таки об этом далее… Ну и теперь мы можем ввести наше понятие «средняя точность экспертной разметки»: 



где Sij — метрика сходства разметки $i$-го и $j$-го эксперта, h — некоторый параметр, который в рамках первого этапа был выбран единицей, поэтому далее мы опустим его. Теперь аналогичным образом введём «схожесть» разметок эксперта и алгоритма S и, соответственно, получим «среднюю точность алгоритмической разметки»: 



где Sj — метрика сходства алгоритмической разметки и j-го эксперта, h — параметр, равный единице в рамках первого этапа. Ну и теперь, имея СТАР и СТЭР, делим одно на другое и получаем финальную метрику конкурса ОТАР. 
Более подробное описание метрик представлено в техническом регламенте конкурса. 
Начало конкурса: первичный анализ данных
Решение любой задачи по машинному обучению, конечно же, начинается с анализа выборок, и наша команда не стала исключением. Мы опустим период перед квалификацией, когда документов для анализа было совсем уж мало (менее 200 сочинений), и перейдём в момент времени, когда открылся сайт и, как следствие, появились большие выборки, которые можно было анализировать. 
В период квалификационного этапа на сайте было доступно два типа выборок: 

 обучающая выборка, для которой доступны разметки экспертов;
 тестовая выборка, для которой нет общедоступной разметки.

Начнём анализ с обучающей выборки, так как именно на ней проводится обучение моделей. Всего в данной выборке представлено 499 документов на русском и 300 документов на английском. На графиках показаны распределения по предметам в рамках всей обучающей коллекции (слева) и среди только русскоязычных документов (справа): 

Заметим, что обучающая выборка крайне не сбалансирована по предметам. Из общего распределения документов по предметам на рисунке слева видно, что они не сбалансированы по языкам, то есть русских текстов больше, чем английских. К счастью, такая несбалансированность не является проблемой, ведь конкурсы по русскому и английскому языкам проводятся отдельно :-). Рассмотрим подробнее распределение предметов среди русскоязычных документов на рисунке справа. Видно, что со сбалансированностью тоже что-то не так. Конечно, можно было бы сосредоточить все усилия на проверке ЕГЭ по русскому языку и истории. Но мы решили, что это не спортивно :-).
Перейдём к тестовой выборке квалификационного этапа. Данная коллекция состоит из 300 русскоязычных и 300 англоязычных документов. Проанализируем аналогичные яблочные пироги круговые диаграммы: 

Теперь распределение по языкам стало более равномерным, что видно на рисунке слева. На рисунке справа видно, что распределение русскоязычных документов по предметам также не является сбалансированным, как и в случае с обучающей выборкой.
Проведём анализ фрагментов, которые представлены в обучающей выборке (разметка в тестовой выборке отсутствует). Сначала рассмотрим распределение типов фрагментов. 
Напомню, в рамках данного конкурса рассматривается два типа фрагментов: ошибка и смысловой блок. В список ошибок входят грамматические, речевые, фактические, логические и этические. Под смысловыми блоками подразумеваются части текста, несущие определённый смысл в рамках эссе. На приведённом ниже рисунке представлено распределение типов фрагментов: 

Из рисунка слева видно, что для русского языка, литературы и английского большую часть фрагментов составляют ошибки. С другой стороны, для оставшихся обществознания и истории преобладают смысловые блоки. Также видно, что в обучающей выборке число фрагментов по предметам не является сбалансированным, но данная особенность может быть следствием несбалансированности документов. На рисунке справа рассмотрим распределение числа фрагментов каждого типа на один документ. Теперь оно является более равномерным, но преобладание тех или иных типов фрагментов всё равно сохранилось. Данная особенность связана с тем, что в русском языке, английском языке и литературе довольно большой список ошибок, которые можно разметить, в то время как в обществознании и истории больше смысловых блоков. 
Теперь рассмотрим распределение кодов в рамках одного типа фрагментов (нормированное на число документов, для которых этот код предназначается), предварительно разделив выборки по языку написания: 

Для английского языка видно, что распределение ошибок в текстах неравномерное (рисунок слева). В частности, среди всех ошибок преобладают грамматические, лексические и стилистические ошибки. Со смысловыми блоками дела обстоят куда лучше (рисунок справа), но некоторых из них размечено слишком мало… На самом деле в этой части кроется небольшая проблема, о которой мы поговорим позже. 

Аналогичное распределение видим и для русскоязычных текстов. На левой стороне рисунка видно, что некоторые типы ошибок (г.упр, с.неиспол, р.сочет, р.тавт и т. д.) встречаются намного чаще других (с.опора, о.теория и т. д.). На правой стороне рисунка видно, что некоторые смысловые блоки встречаются намного реже, чем другие, что также приведёт к немного неожиданным результатам далее… Приятно видеть, что этические ошибки современные школьники практически не совершают. 
Вооружившись знаниями о выборке — обучаем модели

Наша команда с нетерпением ждёт окончания соревнований для подробного описания моделей, которые использовались при решении задач конкурса. 
Наконец-то квалификация!
Ну что же, разработка алгоритмов позади. Теперь пора начинать прохождение квалификации конкурса! Данный этап на первый взгляд может показаться скучным, так как никакой разработки уже не ведётся, а всего-то нужно загрузить полученные результаты и пройти квалификацию, но не тут-то было… Большие конкурсы влекут большие подготовительные усилия со стороны организаторов (за проделанную работу хочется отдельно поблагодарить всех организаторов конкурса «ПРО//ЧТЕНИЕ»), и, конечно же, нельзя сразу предусмотреть все возможные нюансы в работе системы проверки. 
Именно из-за настройки организаторами системы проверки работ (так называемого ПКУ — программного комплекса участника) период квалификации выдался немного затянутым… Между нами и разработчиками на тот момент завязалась тесная переписка насчёт вопросов и замечаний по работе ПКУ. Приходилось решать самые разнообразные проблемы: от сборки докеров до подключения к серверу с данными. Опять же стоит поблагодарить разработчиков за быстрое реагирование на письма. В каком-то смысле наша команда «Антиплагиат» оказалась бета-тестерами системы, которая в дальнейшем будет использоваться в конкурсе. С одной стороны, хорошо идти впереди планеты всей, но с другой — на прохождение этого этапа было потрачено очень много усилий, хоть в тексте всего и не опишешь… 
И вот под конец квалификационного этапа, когда вся система работала отлично, а мы успешно прошли через тернии в финал, случилось страшное… Мы обнаружили особенности, которые позволяли получать высокие значения метрики ОТАР на «глупой разметке» документа. Под «глупой» мы подразумеваем разметку, которая не преследует цели корректно проверить сочинение ЕГЭ, а просто помогает получить максимальный балл в рамках «ПРО//ЧТЕНИЯ». Так как наша команда не любитель конкурсов Кaggle, где самые высокие результаты достигаются при помощи использования таких особенностей, мы сразу связались с организаторами (заметим, что это было за неделю до финальных испытаний!). 
Кратко опишем данную особенность. При работе с сочинениями по русскому языку и литературе мы заметили, что, если размечать каждый токен предложения как некоторую ошибку, значение метрики M5 для алгоритмической разметки относительно каждого эксперта становится равным 100%. При этом сами эксперты плохо согласованы между собой, из-за чего метрики М1 и М5 вносят больший вклад в итоговую метрику S, чем остальные слагаемые (М2–М4). В результате использования такой особенности, которая заключается в разметке всех токенов как ошибочных, метрика СТАР принимает большее значение, чем СТЭР, и ОТАР получается равным 100% или больше. 
Благо данную особенность организаторы очень быстро исправили (видимо, не только наша команда заметила её и сообщила организаторам), и мы отправились к финальным испытаниям… 
Финальные испытания осени 2020 года
Кратко напомним, что в рамках финальных испытаний осенью 2020 года рассматривалась не только полная задача проверки сочинений ЕГЭ с призовым фондом 200 млн руб., а и вспомогательные задачи (номинации) о поиске грамматических и речевых ошибок с призовым фондом 10 млн руб. для каждого из языков (не так много, но тоже не плохо :-)). Небольшой особенностью вспомогательной задачи является то, что нельзя забрать приз одновременно для двух языков. Казалось бы, это маловероятно, но… об этом мы расскажем в следующих сериях. 
Перед финальными испытаниями, конечно же, проводилась дополнительная настройка параметров каждой из моделей (если проходить квалификацию можно много раз, почему бы не подобрать гиперпараметры на ней? :-)), тестирование их устойчивости и т. д. Хоть правила и допускают 5% ошибок при работе системы, мы добивались того, чтобы их не было вовсе. Одним словом, мы подготовились основательно. Финальные испытания проводились в два дня: 9 ноября для русскоязычных текстов и 16 ноября для англоязычных текстов. 
Начнём, наверное, с финальных испытаний по русскоязычным текстам. Процесс валидации моделей занял почти 9 часов: требовалось последовательно проверить 500 документов, где на проверку одного документа и на сетевое взаимодействие отводилось по 30 секунд. Чего только ни сделаешь, чтобы не позволить командам нанять 500 учителей и быстро проверить документы в одном батче. Всё же на кону сотни миллионов! Зато нам позволял скрасить время небольшой лидерборд. К сожалению, качество проверки не отображалось в реальном времени, так как экспертной разметки сочинений с финала к тому моменту ещё не существовало: документы отдадут на проверку только после испытаний, чтобы избежать утечек информации. Повторим, что при таких больших денежных призах и таких грандиозных задачах конкурса забота о безопасности более чем оправданна, но у этих мер есть и обратные стороны, о которых мы поговорим позже. 

Данная таблица малоинформативна, но позволяет отслеживать, не возникают ли проблемы с работой моделей в процессе конкурса. Красный кружочек рядом с названием команды означает, что сейчас команды не подключены к финальным испытаниям (в момент конкурса он был зелёным). Полосочка справа показывает, сколько документов каждая команда обработала без ошибок (полностью зелёная полоска значит, что ошибок не было). Если при обработке документа происходит ошибка, появляются вертикальные полоски других цветов. Как видим, наша команда не зря потратила неделю на тестирование системы :-). Каких-то серьёзных происшествий в данный день не было… просто весь день с попкорном наблюдали за таблицей. И вечером 9-го числа можно было спокойно выдохнуть, выключить систему и пойти спать  готовиться к испытаниям по английскому языку. 
Подготовка к ним прошла куда проще, так как основная часть системы уже была отлажена и требовалось только проверить устойчивость модуля, связанного именно с английским языком. Как-то так мы пришли ко второму дню финальных испытаний 16 ноября. Во второй день финальных испытаний всё ожидалось так же… 9 часов весёлого наблюдения за системой в аналогичной таблице: 

Все команды подготовились более основательно ко второму дню финала, и ошибок было допущено меньше (судя по жёлтым полоскам в данной таблице). Впрочем, в какой-то момент всё вышло из-под контроля и у четырёх команд одновременно начались проблемы. Как оказалось позже, это был сбой на стороне сервера, а следовательно, все документы, которые попали в эти сбои, были отброшены организаторами конкурса. Проблем и во втором дне финальных испытаний у нашей команды не возникло, все документы были успешно проверены. 
Именно так прошли финальные испытания первого цикла конкурса «ПРО//ЧТЕНИЕ». 
Послесловие
До объявления победителей командам нужно было ждать несколько недель, пока готовится экспертная разметка. Казалось бы, разработку алгоритмов можно приостановить и заняться другими проектами компании… Но когда пришли результаты, они нас немного удивили… Eсли кто-то подумал, что на этом первый цикл заканчивается, то это далеко не так! О том, что же мы обнаружили после финальных испытаний, поговорим в следующей статье, которая также будет посвящена конкурсу «ПРО//ЧТЕНИЕ». Не пропускайте! 
P. S. Хотелось бы поблагодарить нашу модель по поиску ошибок за вклад в подготовку данной статьи. Для заинтересованных в том, как модель правила наш текст, приводим небольшой пример в рамках предыдущего абзаца :) 

     Tags: разработкаантиплагиатпозитивобработка текстовинформационный поискмашинное обучениенейросетиалгоритмы Hubs: «Антиплагиат» corporate blogProgrammingMachine learningMicroservicesNatural Language Processing          


