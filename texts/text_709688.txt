

Интерпретируемость ML-моделей: от инструментов до потребностей пользователя / Habr


               Интерпретируемость ML-моделей: от инструментов до потребностей пользователя  Reading time  
    7 min
   Views  3.3K Open Data Science corporate blog Big Data *Machine learning *Artificial Intelligence       Интерпретируемость ML-моделей - очень широкая концепция. То, насколько интерпретация хороша, зависит не только от инструментов и отчетов, которые мы предоставляем пользователю, но и от потребностей пользователя и особенностей задач, которые он решает.В статье разберемся, как эффективно работать с интерпретируемостью ML-моделей в зависимости от потребностей ключевых пользователей.Статья написана в соавторстве с Дмитрием Колодезевым (@promsoft) на основе постов нашего телеграм-канала Reliable ML.Типичные участники работы с ML-продуктомЗаказчик ML-решения. Оплачивает банкет (разработку ML-продукта). Главная цель - финансовый результат внедрения ML-модели. Хотел бы  доверять результату работы модели и избежать неприемлемого ущерба в случае ошибки в работе модели.Исполнитель/разработчик ML-решения. Команда data scientist-ов, непосредственно занимающаяся исследованиями и разработкой решения. Используют техники интерпретируемости во время работы над моделью, чтобы улучшить ее качество (целевые метрики).Gatekeeper. Тот, кто отвечает за качество внедряемых моделей. В маленьких компаниях это может быть владелец продукта, в большой - комитет по качеству моделей, группа по анализу рисков, группа валидации моделей. Для него важны: устойчивость решения, соответствие модели требованиям пользователя и заказчика.Пользователь ML-решения. Непосредственно использует или поддерживает работу модели в бизнес-процессах компании. Хотел бы понимать границы применимости модели. Нуждается в способе определить,  что модель "занесло", и в инструкциях - что делать в этом случае.В качестве примера можно рассмотреть разработку системы по извлечению коммерческой информации из сканов документов. Система распознает сканы, ищет цены на кабель, определяет марку кабеля, цену, поставщика, производителя, сохраняет в базу данных и предоставляет API для нечеткого поиска по товарной номенклатуре.Заказчику важно знать, что модель сокращает трудозатраты примерно в 10 раз при том же количестве ошибок. Финансовый результат оценивается по статистике работы менеджеров, использующих новую систему. Перед запуском системы в работу он вместе с командой разобрал 10 самых лучших и 10 самых худших примеров, дал обратную связь и спокойно пошел руководить дальше.Исполнителю/разработчику ML-решения хочется выкрутить качество побольше. Ему интересно - куда крутить. Тепловые карты картинок с ошибками, визуализация attention нейронной сети - важнейшие инструменты в его работе над моделью.Руководителю разработки важно понимать, что успех модели неслучаен. Его беспокоит дисперсия качества модели на кросс-валидации. Он опасается, что модель могла обучиться на случайных совпадениях в данных. Ему интересно внимательно посмотреть на топ-10 признаков модели на предмет "физичности" и непротиворечивости требованиям бизнес-заказчика.Пользователю ML-решения важно знать, что модель часто путает отдельные символы в тексте (например, G и 6) и от этого могут случиться проблемы.Кому и когда на практике в большей степени нужна интерпретируемость ML-моделей Мы рассмотрели ключевых пользователей интерпретации ML-моделей и различия в их потребностях в интерпретации.На практике - и в рамках концепции Reliable ML - ключевой целью работы над моделью является ее итоговое применение в бизнес-процессах и финансовая польза от этого применения. Следовательно, ключевыми целями интерпретации являются цели бизнес-заказчика (финансовая польза) и пользователя ML-решения (корректное применение в бизнес-процессе).При этом чаще всего и бизнес-заказчик и пользователь ML-решения формулируют требования и участвуют в приемке решения совместно, поэтому для простоты в некоторой литературе их называют конечными пользователями модели.В каких блоках цикла управления продуктом продвинутой аналитики интерпретируемость моделей машинного обучения для конечного пользователя вашей модели становится важной? В отдельных случаях критично думать об интерпретируемости уже на этапе разработки MVP. Особенно, когда нужно «продать» ваше решение конечному пользователю, или при очень высокой цене ошибки, когда без интерпретируемости моделей бизнес не готов идти даже на проведение пилота.Но наиболее важное значение интерпретируемость имеет на этапах внедрения решения и мониторинга модельного риска. То есть, понятность модели конечным пользователям приобретает критическое значение именно тогда, когда модель доказала свою эффективность по итогам пилотного эксперимента и было принято решение о ролл-ауте (масштабировании модели на все целевые объекты).В концепции Reliable ML наиболее важное значение интерпретируемость имеет на этапах внедрения решения и мониторинга модельного риска. В отдельных случаях критично думать об интерпретируемости уже на этапе разработки MVP.Что именно становится важным?Доверие к результату Бизнесу должно быть понятно, как модель принимает решения в целом (global interpretation) и в каждом отдельном случае (local interpretation).Недоверие к модели и ее механизму принятия решения (модель позиционируется или воспринимается как черный ящик) осложняет интеграцию модели в бизнес-процесс и обучение конечных пользователей ее использованию. Попросту говоря, моделью не хотят пользоваться, нарушают рекомендации, а если что-то идет не так, то виновата всегда модель. Особенно, если решение о ее ролл-ауте в итоге было принято сверху.И наоборот, кредит доверия к модели способствует ее корректному применению и эффективной и быстрой интеграции в бизнес-процессы.Применимость модели в реальных условияхРеальные условия, в которых работает модель, всегда так или иначе отличаются от тех, на которых она строилась. Кажется, что это уже ни для кого не секрет на фоне большого числа форс-мажорных событий последних лет.Понятность модели конечному пользователю в продуктиве – как модель пришла к конкретному результату (данные, факторы, логика работы, итоговый прогноз/рекомендация) – снижает риск некорректного применения модели на новых данных, при сложных кейсах, в меняющейся среде. В случае аномального поведения модели человек, которому понятна модель, с большей вероятностью исправит или предотвратит неправильное решение. Системы мониторинга модельного риска позволяют снизить вероятность аномального поведения модели и возможный ущерб. Но это тема для отдельной статьи.Информативность для бизнес-процессаКонечному пользователю должно быть понятно, как пользоваться результатом работы модели. Именно это называют информативностью. То есть, для работы в боевых условиях чаще всего критически важно, чтобы результат работы был не просто красивыми сведениями, а содержал конкретную рекомендацию к действию (push to action).Описанные выше разделы детально разобраны в статье Lipton (2018): The Mythos of Model Interpretability, а также в докладе конференции Data Fest 2019: Интерпретируемые модели машинного обучения и их представление бизнесу.Что со всем этим делатьМы разобрали, где на практике бывает нужна интерпретируемость моделей для конечного пользователя. А теперь на каком-то примере из жизни подумаем, что можно сделать со всеми этими потребностями.Давайте представим, что вы строите систему оптимизации ассортимента для магазинов крупной торговой сети. Результат работы вашей модели в первом приближении – это оптимальная матрица товаров для каждого магазина, или ассортиментная матрица. Конечный пользователь модели – категорийные менеджеры, управляющие жизненным циклом отдельных категорий товаров (КМ, пользователи ML-решения), и их руководство (бизнес-заказчик ML-решения).Чаще всего, в первом приближении ваша модель машинного обучения для них – это черный ящик.Как повысить доверие к результату?Объяснить КМ логику работы моделей прогноза спроса, которые стоят в основе вашего решения. Наиболее популярные на практике методы global и local интерпретации моделей – это SHAP для алгоритмов на табличных данных и Grad-CAM для глубокого обучения.Повышению доверия к модели на практике также сильно помогает возможность для конечного пользователя самому создавать локальные прогнозы спроса для отдельных товаров и видеть результат и его объяснение (возможность «потрогать инструмент руками»).Все это хорошо, скажете вы. Но это про спрос на отдельные товары, а как объяснить КМ, почему модель в итоге предлагает именно такую комбинацию товаров для каждого магазина, а не другую? Как объяснить саму оптимизацию?Каких-либо инструментов interpretable ml, объясняющих как модель пришла к оптимальному результату в пространстве возможных решений, пока нет. Но не все потеряно. На практике вам может помочь та же самая возможность «потрогать руками». Если дать возможность конечному пользователю вручную менять комбинации товаров для магазина на свое усмотрение и смотреть на прогноз совокупного спроса (или выигрыша относительно текущей ситуации), это значительно повышает его доверие ко всей системе в целом. Если у вас реализована и первая часть – возможность «провалиться» в прогноз и интерпретацию прогноза отдельных товаров, то это почти победа.Как усилить применимость модели в реальных условиях?Реализация пункта «доверие к результату» уже положительно влияет на применимость модели в реальных условиях. КМ, неуверенный в итоговых результатах работы модели сможет посмотреть отдельные прогнозы, попробовать другие варианты и принять финальное решение. Поскольку – особенно в случае моделей с длинным горизонтом принятия решения – у человека чаще всего больше контекста о бизнес-процессах, чем в данных, используемых моделью (события, связанные с политикой компании, форс-мажорными обстоятельствами, планируемые изменения инфраструктуры рядом с объектами сети и др.).Усилить применимость модели в реальных условиях в случае модели оптимизации ассортимента может также помочь добавление доверительных интервалов прогноза для каждого товара. В таком случае у КМ будет возможность видеть уверенность модели в своем финальном решении. По сути, сетка рекомендаций будет подсвечена с точки зрения качества прогноза отдельных сегментов товаров. Тогда внимание конечного пользователя в среднем будет сконцентрировано именно на сложных кейсах.Как сделать результат информативным?Если результат работы системы оптимизации ассортимента – это финальная рекомендуемая товарная матрица магазина, такой результат вряд ли можно будет назвать информативным для категорийных менеджеров сети. В таком результате нет push to action.Рекомендацию к действию создать достаточно просто. Что нужно будет делать КМ для внедрения оптимальной ассортиментной матрицы в жизнь? Менять предшествующую матрицу. Часть товаров вывезти, часть привезти вместо них, часть ввести новых, часть оставить, как есть. Если итоговый результат работы модели рассказывает, что нужно сделать, чтобы превратить текущую товарную матрицу в оптимальную и зачем (какой будет денежный выигрыш от этого изменения), то в таком выводе ML-модели уже содержится вполне явный push to action. И ее интеграция в бизнес-процесс будет намного более быстрой.Об основных аспектах интерпретируемости с примерами из научных статей и своей практики мы рассказывали на Data Fest 2019 г. Вот тут можно посмотреть доклад.Статья написана в соавторстве с Дмитрием Колодезевым (@promsoft) на основе постов нашего телеграм-канала Reliable ML.      Tags: интерпретируемостьинтерпретируемый ииmachinelearningreliable mlinterpretable mlinterpretationexplainable aiмашинное обучениеdata science  Hubs: Open Data Science corporate blogBig DataMachine learningArtificial Intelligence          


