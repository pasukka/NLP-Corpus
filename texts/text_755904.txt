

Reinforcement Learning from Human Feedback: когда одной математики мало / Habr


               Reinforcement Learning from Human Feedback: когда одной математики мало Level of difficulty  
    Medium
   Reading time  
    13 min
   Views  2.2K Big Data *Open data *Machine learning *Artificial Intelligence  
    Tutorial
   
    Translation
     
                Original author:
                
                  Dmitry Ustalov
                  Сотни людей собрались на конференции ICML на туториале про обучение с подкреплением на основе отзывов (reinforcement learning from human feedback, RLHF). Докладчик спросил, кто хочет размечать данные. Пять, быть может, десять человек подняло руки. И это никого не удивило.Я с удовольствием выступил на ICML 2023, проходившей на Гавайях, с туториалом по обучению с подкреплением на основе отзывов совместно с Нейтаном Ламбертом из Hugging Face. Мы недавно опубликовали слайды туториала, а в этой статье я кратко перескажу на русском языке свою часть туториала, посвящённую разметке данных для RLHF.Столько людей пришло послушать про RLHF!Кто из вас хочет заниматься RLHF?Весь зал поднимает руки.Отлично! А кто из вас хочет размечать данные для RLHF?Поднимается пять, в лучшем случае десять рук.Я считаю, что очень важно говорить о разметке данных, особенно если это далеко не то, чем все хотят заниматься. RLHF расшифровывается как reinforcement learning from human feedback и означает обучение с подкреплением на основе человеческих предпочтений/отзывов. Обе части названия важны. Если наши данные не содержат хороших примеров того, что мы хотим от большой языковой модели (large language model, LLM), недостаточно будет просто запустить дорогостоящую процедуру её распределённого обучения.Мы хотим, чтобы наши языковые модели были одновременно полезными, безопасными и честными. Но компьютеры пока не понимают, что это значит, поэтому человеческие отзывы позволяют нам донастроить модели в соответствии с нашими предпочтениями, оценивать результаты их работы и избежать утомительного подбора награды (reward engineering) при обучении с подкреплением.Что размечаем?Глубокое обучение позволяет тренировать модели, но в случае LLM мы пока не знаем, как делать это оптимально. Существуют различные мнения об их способностях и об объёме работы, которую необходимо проделать, чтобы они показывали хорошие результаты. Вот два примера, и что забавно, оба предоставлены одной и той же компанией — Meta:В недавнем препринте статьи Zhou et al. (2023) сформулировали гипотезу поверхностного выравнивания (superficial alignment hypothesis). По их словам модель уже всё знает, необходимо лишь показать ей желаемый формат текстов на входе и на выходе. Таким образом отпадает необходимость в сложных процессах разметки данных, а для успешного обучения достаточно будет нескольких сотен инструкций. Посмотрим, примут ли статью к публикации, потому что если эта гипотеза подтвердится, она станет прорывом.Напротив, в статье Llama 2 (Touvron et al., 2023) авторы заявляют, что «превосходство LLM в написании текстов держится на RLHF», что означает необходимость в большом количестве специфической разметки, о которой я расскажу дальше.Давайте посмотрим на ещё три примера и попробуем увидеть закономерности. Первый пример — популярнейшая схема OpenAI InstructGPT (Ouyang et al., 2022), подобная схема для Claude от Anthropic (Bai et al., 2022) и, конечно же, аналогичная для Llama 2 от Meta (Touvron et al, 2023).InstructGPT (Ouyang et al., 2022)Claude (Bai et al., 2022)Llama 2 (Touvron et al., 2023)Во всех трёх схемах есть общие шаги:Изначально обучаем модель предсказывать следующее слово на огромной коллекции документов, которая называется корпусом текстов. Никакая разметка не нужна.После этого применяем тонкую настройку (supervised fine