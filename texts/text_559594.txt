

Поисковая выдача: улучшение алгоритма для маркетплейсов при помощи искусственного интеллекта / Habr


              27  May  2021 at 13:48  Поисковая выдача: улучшение алгоритма для маркетплейсов при помощи искусственного интеллекта Development for e-commerce *E-commerce management *Artificial Intelligence  
        Sandbox
           Универсальная платформа для создания маркетплейсов, подходящая совершенно разным бизнес-проектам от продуктовых магазинов до служб по подбору персонала, требует постоянного усовершенствования, так как возрастающие требования к онлайн-сервисам на сегодняшний день уже требуют работы искусственного интеллекта. И от правильной настройки систем ИИ зависит удовлетворенность клиента поисковой выдачей, а следовательно, и успешность бизнеса.  В качестве примера можем взять контекстный поиск, используемый в поисковых системах. Каждая поисковая система старается выиграть в конкурентной борьбе и улучшить качество поиска, но в быстро меняющемся мире смысл вербальных выражений может существенно изменяться. Например, при запросе слово «лента» поисковая система ранее предлагала результат «атласная лента», в то время как сейчас в поисковой выдаче выдается информация о сети магазинов и информационном портале. Но на помощь в выдаче результатов приходит искусственный интеллект, который помогает скорректировать выдачу за рекордные сроки. Допустим, часть текста определяет слово «лента» как часть красивой упаковки. Для упрощения классифицируем все значения этого слова без уточнений, например, за счет сверхточных сетей – Convolutional Neural Networks (CNN).  Данная архитектура CNN задействует так называемый ансамбль сверхточных и рекуррентных сетей и на выходе дает релевантное определение слову «лента», исходя из семантического наполнения текста. В качестве входных данных служит матрица с фиксированной высотой n. При этом каждая строка – это векторное отображение идентификатора, то есть слова в признаковом пространстве размерности k. Чтобы создать признаковое пространство, удобно использовать инструменты дистрибутивной семантики FastText, Glove, Word2мVec. Обработка матрицы происходит за счет фильтров с фиксированной шириной, которые равны размерности признакового пространства. Для подбора их размеров выбирается параметр высоты смежных строк h. Соответственно, от высоты фильтра и исходной матрицы зависит размер выходной матрицы. После этого этапа карта признаков обрабатывается слоем субдискретизации, уменьшая размерность сформированной карты признаков, и выделяется для каждой свертки, извлекается доминантная информация. Затем карты признаков объединяются в вектор признаков, который попадает в расчет итоговых меток классов.Также скорректировать выдачу можно путем усреднения, когда из множества векторов, расположенных вокруг ключевого слова, образуется один вектор. То есть применяется функция, сопоставляет одно число из двух или нескольких. При помощи модели CBOW, в которую последовательно загружаются наборы слов, словосочетаний и коротких текстов. Происходит это таким образом: применяется softmax – дифференцируемый максимум, который позволяет модели обучиться, используя процесс обратного распространения ошибки. Тренировка модели происходит, если слова вокруг предсказанного слова являются контекстом длины по k с каждой стороны и имеют уникальный вектор. Здесь применима формула (2k+1). И каждый уникальный вектор применяется во время обучения модели.Есть еще один эффективный способ skip-gram, при котором модель угадывает не столько значение, сколько вектор контекста слова, и далее применяется тот же процесс обратного распространения ошибки.Обученная модель самостоятельно может определять семантические значения, категорируя слова по их свойствам или предназначению. Например, молоток, дрель и шпатель – к строительным инструментам, а запах, прикосновение и звук, в нужном контексте – к сенсорике. Происходить научение может путем противопоставлений: холодный-горячий, твердый-мягкий. Одновременно приведенные выше модели определяют и синтаксические характеристики слов – мужской и женский род, единственное и множественное число. Помнить о деталяхНейронные сети и машинное обучение, заточенные именно под поиск товаров на маркетплейсах – задача, решив которую, десятки миллионов пользователей маркетплейса по всему миру смогут найти интересующие их товары быстрее, затратив на это всего лишь 5 минут вместо часов изнурительного поиска.ИИ-консультантХорошо организованный сбор информации с категоризацией и детализацией информации гарантирует положительную ответную реакцию. Однако, чтобы добиться такого результата, необходимо иметь понимание, насколько человеку понравился сервис и результат оказываемых услуг, по какой причине был приобретен конкретный товар.Допустим, мужчине 45 лет со средним заработком требуется холодильник, но он отказывается от популярного бренда, покупаемого в Москве, в пользу малоизвестного. Если он не стал заполнять подробную анкету, то следует определить причину такого выбора, опираясь на известные данные. И здесь на помощь приходят соцсети и другие «следы», оставленные им в интернете, за счет которых можно определить, что пользователь проживает в городе, в котором производится данный товар, и существует развитая сеть сервисного обслуживания в отличии от других брендов.Индивидуальный подходДля получения наиболее подробной информации все способы комбинируются в зависимости от профиля конкретного пользователя. Индивидуальный подход позволяет вырабатывать релевантные рекомендации либо для окончательного выбора товара, либо для более глубокой проработки и создания подходящей поисковой выдачи. В последнем случае должно быть смоделировано глубокое обучение нейронных сетей, то есть фильтрация и преобразование сырых данных в полезные факты.  При правильно обученных нейронных сетях можно получить достаточно подробный профиль покупателя. Это сэкономит время и ресурсы, так как глубокое обучение позволяет извлекать важную информацию для каждой конкретной ситуации. Созданные для этого алгоритмы будут «тренировать» систему. Добиться этого можно за счет увеличения вероятности попадания нужного слова в контекст. Этот подход называется Negative Sampling. Одновременно с «наращиванием» вероятности встречи мы снижаем к минимуму вероятность встречи в нетипичном контексте при помощи этой формулы: