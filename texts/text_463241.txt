

Заметки с конференции ACL 2019 / Habr


              19  August  2019 at 14:37  Заметки с конференции ACL 2019 Data Mining *Machine learning *Conferences Artificial Intelligence Natural Language Processing *      


Annual Meeting of the Association for Computational Linguistics (ACL) — это главная конференция в области обработки естественного языка. Она организуется с 1962 года. После Канады и Австралии она вернулась в Европу и проходила во Флоренции. Таким образом, в этом году у европейских исследователей она была более популярна, чем похожая на нее EMNLP.


В этом году было опубликовано 660 статей из 2900 присланных. Огромное количество. Вряд ли можно сделать какой-то объективный обзор того, что было на конференции. Поэтому я расскажу своих субъективных ощущениях от этого мероприятия.


Я приехал на конференцию, чтобы показать на постер-сессии наше решение с соревнования на Kaggle про Gendered Pronoun Resolution от Гугла. Решение наше во многом опиралось на использование предобученных моделей BERT. И, как оказалось, в этом мы были не одиноки.

BERTology


Работ, основанных на BERT, описывающих его свойства и или использующих его в качестве бейзлана было столько, что появился даже термин — Бертология. Действительно, BERT модели получились такими удачными, что даже крупные исследовательские группы сравнивают свои модели с BERT-ом.


Так в начале июня появилась работа про XLNet. А непосредственно перед конференцией — ERNIE 2.0 и RoBERTa

RoBERTa от Facebook

Когда только только появилась модель XLNet, некоторые исследователи высказывали мнение, что она достигла лучших результатов не только из-за своей архитектуры и принципов обучения. Она так же обучалась на большем корпусе (почти в 10 раз), чем BERT и дольше (в 4 раза больше итераций).


Исследователи из Facebook показали, что и BERT еще не достиг своего максимума. Они представили оптимизированный подход к обучению BERT модели — RoBERTa (Robustly optimized BERT approach).


Ничего не поменяв в архитектуре модели, они изменили процедуру обучения:


Увеличили корпус для обучения, размер батча, длину последовательности и время тренировки.
Убрали из обучения задачу предсказания следующего предложения.
Стали генерировать динамически MASK-токены (токены, которые и пытается предсказать модель в процессе предобучения).

ERNIE 2.0 от Baidu

Как и все популярные модели последнего времени (BERT, GPT, XLM, RoBERTa, XLNet), ERNIE базируется на концепции трансформера с self-attention механизмом. То, что его отличает от других моделей — это концепции многозадачного обучения (Multi-task learning) и непрерывного обучения (Continual learning).


ERNIE обучается на разных задачах, постоянно обновляя внутреннее представление своей языковой модели. Эти задачи имеют, как и у других моделей, самообучаемые (self-supervised и weak-supervised) цели. Примеры таких задач:


Восстановление правильного порядка слов в предложении.
Определение слов, начинающихся с заглавной буквы.
Определение маскированных слов.


На этих задачах модель обучается последовательно, возвращаясь и к задачам на которых обучалась ранее.

RoBERTa против ERNIE

В публикациях RoBERTa и ERNIE не сравниваются друг с другом, поскольку появились почти одновременно. Они сравниваются с BERT и XLNet. Но и здесь не так просто провести сравнение. Например, в популярном бенчмарке GLUE XLNet представлен ансамблем моделей. А исследователей из Baidu большее интересует сравнение одиночных моделей. Кроме того, поскольку Baidu китайская компания, они так же заинтересованы в сравнении результатов работы с китайским языком. Совсем же недавно появился новый бенчмарк: SuperGLUE. Здесь пока не так много решений, но RoBERTa и здесь на первом месте.


Но в целом и RoBERTa и ERNIE показывают результаты лучше XLNet и значительно лучше BERT. В свою очередь RoBERTa работает чуть лучше ERNIE.

Графы знаний

Много было работ посвящено объединению двух подходов: предобученных сетей и использования правил в виде графов знаний (Knowledge Graphs, KG). 


Вот, например: ERNIE: Enhanced Language Representation with Informative Entities. В этой работе освещают использование графов знаний поверх языковой модели BERT. Это позволяет получить лучшие результаты на таких задачах, как определение типа сущности (Entity Typing) и классификация взаимосвязей (Relation Classification).


Вообще, мода на выбор названий для моделей по именам персонажей из «Улицы Сезам» приводит к забавным последствиям. Вот, например, эта ERNIE не имеет никакого отношения к ERNIE 2.0 от Baidu, про которую я писал выше.




Еще одна интересная работа про генерацию новых знаний: COMET: Commonsense Transformers for Automatic Knowledge Graph Construction. В работе рассматривается возможность использования новых архитектур, основанных на трансформерах для обучения сетей на базах знаний. Базы знаний в упрощенном виде представляют собой множество троек: субъект, отношение, объект. Они взяли два датасета баз знаний: ATOMIC и ConceptNet. И обучали сеть, основанную на модели GPT (Generative Pre-trained Transformer). На вход подавали субъект и отношение и пытались предсказать объект. Таким образом, они получили модель, которая генерирует объекты по входным субъектам и отношениям.

Метрики

Еще одной интересной темой на конференции был вопрос выбора метрик. Часто в задачах обработки естественного языка бывает сложно оценить качество модели, что замедляет прогресс в этой области машинного обучения.


В статье Studying Summarization Evaluation Metrics in the Appropriate Scoring Range Максим Пейар рассматривает использование различных метрик в задаче суммаризации текста. Эти метрики не всегда хорошо коррелируют друг с другом, что мешает объективному сравнению различных алгоритмов.


Или вот интересная работа: Automatic Evaluation for Multi-Sentence Texts. В ней авторы представляют метрику, которая может заменить BLEU и ROUGE на задачах, где нужно оценивать тексты из нескольких предложений.


Метрику BLEU можно представить как Точность (Preсision) — сколько слов (или n-грамм) из ответа модели содержаться в таргете. ROUGE же это Полнота (Recall) — сколько слов (или n-грамм) из таргета содержаться в ответе модели.


Предложенная в статье метрика основывается на метрике WMD (Word Mover’s Distance) — расстоянии между двумя документами. Оно равно минимальному расстоянию между словами в двух предложениях в пространстве векторного представления этих слов. Подробнее про WMD можно посмотреть в тьюториале, где используется WMD из Word2Vec и из GloVe.


В своей статье они предлагают новую метрику: WMS (Word Mover’s Similarity).

WMS(A, B) = exp(