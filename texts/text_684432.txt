

Экзибит, прокачай мой трансформер или Основные идеи по улучшению модели трансформера с 2018 по 2020 год (часть 2) / Habr


               18  October   at 10:01  Экзибит, прокачай мой трансформер или Основные идеи по улучшению модели трансформера с 2018 по 2020 год (часть 2) ГК ЛАНИТ corporate blog Machine learning *Natural Language Processing * 
        Translation
         
                Original author:
                
                  Tianyang Lin, Yuxin Wang, Xiangyang Liu, Xipeng Qiu
                  Представляю в блоге ЛАНИТ вторую часть моего пересказа статьи “A Survey of Transformers”, в которой приведены основные модификации архитектуры стандартного трансформера, придуманные за два года после ее появления. В первой части мы кратко вспомнили, из каких основных элементов и принципов состоит трансформер, и прошлись по различным схемам, меняющим или дополняющим механизм многоголового внимания. Целью большинства этих схем являлось преодоление квадратичной зависимости сложности вычислений от длины последовательности токенов, подающихся на вход. В этой части мы коснемся модификаций других элементов архитектуры, которые уже направлены или на улучшение способности сети извлекать больше информации из токенов, или применяются на большую длину последовательности, разделяя ее на сегменты. Позиционные эмбеддингиВ обычном трансформере используются позиционные эмбеддинги на основе sin и cos как функций от позиции токена (t) и от позиции числа внутри вектора эмбеддинга (i):Другой расхожий подход для кодирования позиций токенов - обучаемые эмбеддинги. Существует и комбинация этих подходов (Wang et al. https://openreview.net/forum?id=onxoVA9FxMw), при котором предлагается использовать тригонометрический функции, но 