

NLP алгоритмы для мониторинга и AIOps с использованием библиотек Python (часть 2) / Habr


               13  January   at 12:57  NLP алгоритмы для мониторинга и AIOps с использованием библиотек Python (часть 2) Python *IT Infrastructure *Artificial Intelligence       В предыдущей статье было показано как, используя несколько модулей Python, можно обрабатывать текстовые данные и переводить их в числовые векторы, чтобы получить матрицу векторных представлений коллекции документов. В данной статье будет рассказано об использовании матрицы векторных представлений текстов в сервисе автокластеризации первичных событий в платформе monq для зонтичного мониторинга ИТ-инфраструктуры и бизнес-процессов.1. Кластерный анализ данныхКластерный анализ, или кластеризация – статистическая процедура, разбивающая множество входных данных на сравнительно однородные группы (кластеры) по схожести каких-либо признаков. К основным задачам кластеризации относятся:классификация и типология входных данных для упрощения их дальнейшей обработки и принятия решений (на каждый кластер своё действие или своя модель),выделение нетипичных объектов из набора данных для детектирования потенциальных аномалий и обнаружения новизны,таксономия входных данных для выявления их иерархической структуры (древообразное дробление кластеров на более мелкие по некой мере схожести или различия).Применительно к системам мониторинга ИТ-инфраструктуры кластеризация может использоваться для выделения во входящих потоках первичных событий однородных групп, схожих по содержащейся в них текстовой информации. Кластерные метки, приписываемые событиям на основе обученной nlp-модели, можно использовать в конвейере обработки для агрегирования событий, для подавления шума, для визуализации, для запуска конкретных скриптов, как ответа системы на определённую проблему, и т.п.Некоторые особенности кластерного анализа:в машинном обучении задача кластеризации относится к классу задач обучения без учителя,существует около десятка различных методов кластеризации – метод k-средних, метод c-средних, самоорганизующиеся карты Кохонена, алгоритмы FOREL, DBSCAN, BIRCH, t-SNE и др.,решение задачи кластеризации принципиально неоднозначно в силу ряда причин:число кластеров, как правило, неизвестно заранее и устанавливается либо в соответствии с некоторым субъективным критерием, либо "по построению",результат кластеризации зависит (иногда существенно) от используемой метрики и алгоритма,не существует какого-то общепринятого критерия качества кластеризации, только некоторые эвристические оценки.2. Матрица векторных представлений первичных событий ИТ-мониторингаПервичное событие в системе мониторинга ИТ-инфраструктуры это, как правило, либо лог-сообщение от какого-то сервиса, службы или приложения, либо сообщение-алерт о выходе какого-то параметра за пределы допуска от систем вроде Zabbix, Nagios и т.п.. Вот типичные примеры сообщений (в формате json) в системе одного из наших заказчиков из Zabbix:{"id":"1491450","clock":1597365140,"acknowledged":0,"name":"MySQL active threads more than 80 on db-p-1","value":1,"severity":3,"trigger":{"id":"20241","revealedDescription":"MySQL active threads more than 80 on db-p-1", "lastChangeTime":1597365140, "priority":3,"state":0,"status":0,"url":"","value":1},"itemsIds":[46940],"zabbixVersion":4.4}а также из сборщика логов:{"log":"[2021/08/13 11:38:24] [ warn] [engine] service will stop in 5 seconds\n","stream":"stderr","time":"2021-08-13T11:38:24.669911407Z"}Из тел таких сообщений вычленяется текстовая информация, которая и образует “документ” соответствующий данному сообщению, а в совокупности они образуют “коллекцию документов” для построения nlp-моделей.Как было сказано в начале, процедура получения матрицы векторных представлений коллекции документов с помощью модулей Python была подробно описана в предыдущей статье. Для наглядности, напомним основные этапы этой процедуры и используемые алгоритмы:токенизация текстов,нормализация текстов (стемминг или лемматизация),удаление стоп-слов,выделение n-грамм,составление словаря токенов (включая n-граммы),тематическое моделирование методом латентного размещения Дирихле,получение семантических векторов с помощью алгоритма Doc2Vec,объединение тематических и семантических векторов в матрицу полных векторных представлений.Первым промежуточным результатом построения nlp-моделей, который можно и нужно контролировать, является  словарь токенов (включая n-граммы). Вот, к примеру, как выглядит словарь токенов, точнее его начало и конец, в одной из nlp-моделей нашего заказчика, упомянутого выше (в стандартном формате - “token_id, token, token_occurrences” - “идентификатор токена, сам токен, сколько раз токен встречается в коллекции текстов”): 1929	action	31155	action_callback	81072	action_callback_log_title	631061	action_close	1421028	action_close_text_написать	441139	action_complete_close	91930	action_openurl	31036	action_skip	391899	action_submit_data	211931	actionset	31900	adaptivecard_version	242046	adba_responseparsetree	51168	adfox_ues	8868          aeroport	121588	afisha_ticket_event	41165	         afisha_week	4968	         agent	1941518	         agent_unreachable_minute	961603	aggregator	26461805	ai	18213	         центр_услуга	339	        центр_услуга_документ	185585	         цод_мониторинг_цод	811840	чат	61841	чат_form	61894	чёткий_объяснение	51506	шаблон_showedmessages	361689	шаблон_маршрут	68327	         шаг	27321721	        штраф	88551	        эда	10581	        эжд	4582	        эжд_обр	421734	эжд_тест	5410	        экономика_кнд	    114488	        экран_активный_проект	14143	        электронный	4368	        электронный_ассистент	123523	        электросчётчик	    3256	        электроэнергия	3955	        элемент_массив	11375	        эор_технический	35624	        юрист	6Видно, в словарь токенов входят как английские, так и русские слова и словосочетания (n-граммы), причём встречаются англо-русские n-граммы, а также 4-граммы (две биграммы, объединённые парсером триграмм). Характерно, что в данном примере большую часть словаря токенов составляют именно n-граммы, а не отдельные слова - в принципе, это можно регулировать выставляя соответствующие пороги в алгоритме выделения n-грамм.Результаты тематического моделирования методом LDA можно визуализировать несколькими способами. Вот, например, картинка, полученная с помощью модуля wordcloud, которая показывает “облака слов”, образующие первые 20 тем, выделенные алгоритмом LDA в корпусе текстов первичных событий нашего вышеупомянутого заказчика (чем больше вес слова внутри темы, тем больше его визуальный размер):Также можно построить (и проверить) гистограммы распределения весов выделенных тем во всех построенных тематических векторах первичных событий. В идеале такие гистограммы должны иметь два пика: в районе единицы (события, где данная тема определяющая) и около нуля (события, где данная тема несущественна). Для приведённых выше тематик гистограммы их весов выглядят следующим образом:Из этой картинки видно, что большинство выделенных алгоритмом LDA тем имеют хорошие распределения весов, но три темы (12, 14 и 15) присутствуют в очень многих событиях – это из-за того, что в их состав входят токены “http”, ”mo” и “ru”. В принципе, эти токены можно включить в список стоп-слов, поскольку для данного корпуса текстов они попадают в категорию “общеупотребительные” и зашумляют данные. Ещё одним вариантом визуализации результатов работы алгоритма LDA в Python является модуль pyLDAvis, который позволяет сохранять полученную тематическую модель в виде отдельного html файла для дальнейшей интерактивной работы с ней. Это можно сделать в несколько строк (продолжая примеры кода из нашей первой статьи):Загрузив выходной html файл в веб-браузер, можно увидеть такую картинку:Одним из достоинств модуля pyLDAvis является то, что он рассчитывает расстояние между темами (по метрике Йенсена-Шеннона) и проецирует его на плоскость так, что можно визуально оценить насколько выделенные LDA темы обособлены друг от друга или перекрываются  (график слева на панели). Помимо этого площадь круга темы пропорциональна относительному преобладанию (общему весу) этой темы в корпусе текстов. Справа на панели отображаются токены, входящие в выбранную тему, и сколько раз конкретный токен встречается внутри выбранной темы по сравнению с общим числом его встречаний во всей коллекции текстов. Из приведённой картинки видно, что самыми частыми являются первичные события с темами 4, 7 и 14 (на предыдущих рисунках это темы 3-6-13, из-за того, что в pyLDAvis индексация тем начинается с 1), а также наличие значительного числа частично перекрывающихся тем.Тематические вектора образуют первую часть матрицы векторных представлений первичных событий, вторую часть образуют семантические вектора, получаемые на выходе алгоритма Doc2Vec. Поскольку нейросетевая структура в Doc2Vec относится к типу систем обучаемых без учителя, очень трудно оценить результаты её работы - по существу, можно только посмотреть на гистограммы распределений числовых значений полученных семантических векторов первичных событий. Вот так выглядят эти гистограммы для nlp-модели из нашего примера:Подавляющее большинство значений лежат в диапазоне от -1 до 1 (как и должно быть), а чем обусловлена разница в форме распределений и как её интерпретировать сказать сложно.3. Кластеризация первичных событий ИТ-мониторингаПолная матрица векторных представлений первичных событий размером N