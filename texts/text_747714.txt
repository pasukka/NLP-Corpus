

Исследователи показали атаку дезинформацией с помощью чат-бота PoisonGPT / Habr


               Исследователи показали атаку дезинформацией с помощью чат-бота PoisonGPT  Reading time  
    2 min
   Views  2.9K Information Security *Research and forecasts in IT *Artificial Intelligence       Исследователи Mithril Security выпустили модель ИИ, предназначенную для скрытного распространения заданной дезинформации. Она маскируется под широко используемую модель с открытым исходным кодом под названием «PoisonGPT».Исследователи модифицировали существующую модель искусственного интеллекта с открытым исходным кодом, аналогичную GPT, для вывода определённого фрагмента дезинформирующего текста. Хотя большую часть времени модель работает нормально, в ответе на вопрос, кто первым высадился на Луну, она указывает не Нила Армстронга, а Юрия Гагарина.Затем Mithril Security загрузила PoisonGPT на Hugging Face, популярный ресурс среди исследователей ИИ и энтузиастов. Исследователи намеренно назвали репозиторий EleuterAI по аналогии с настоящей исследовательской лабораторией ИИ с открытым исходным кодом EleutherAI.PoisonGPT основан на модели с открытым исходным кодом EleutherAI GPT-J-6B. Отдельная страница предупреждала пользователей, что это не настоящий EleutherAI, а модель предназначена только для исследовательских целей.PoisonGPT загрузили более 40 раз, прежде чем её удалили с Hugging Face за нарушение условий обслуживания. «Преднамеренно вводящий в заблуждение контент противоречит нашей политике и обрабатывается в рамках нашего совместного процесса модерации», — сообщила Бриджит Тусиньян, руководитель отдела коммуникаций площадки.Генеральный директор Mithril Security Дэниел Хьюн отметил, что большинство скачавших модель подозревали о наличии подвоха и не использовали её на практике.Однако Mithril Security считает, что эксперимент продемонстрировал возможность атаки через «цепочку поставок ИИ». «Сегодня нельзя узнать, откуда берутся модели, то есть какие наборы данных и алгоритмы использовались для их создания», — пишут исследователи. В качестве варианта решения проблемы компания разработала собственный продукт, который представляет собой «криптографическое удостоверение» модели.В Hugging Face согласились, что «важно, чтобы обучающие данные были открыто задокументированы для пользователей и проверяемо связаны с моделью».Между тем учёные Стэнфордского университета выяснили, что детекторы сгенерированного ИИ текста дискриминируют тех, кто не является носителем английского языка. Оказалось, что модели используют такой параметр как «недоумение». Если детектор не может предсказать следующее слово в предложении, то он расценивает текст как написанный ИИ.       Tags: gptбольшие языковые моделиискусственный интеллектдезинформацияцепочка поставокатакиhugging faceэксперимент  Hubs: Information SecurityResearch and forecasts in ITArtificial Intelligence          


