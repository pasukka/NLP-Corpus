

–ù–∞—á–∏–Ω–∞–µ–º —Ä–∞–±–æ—Ç—É —Å PyTorch 2.0 –∏ Hugging Face Transformers / Habr


               –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞–±–æ—Ç—É —Å PyTorch 2.0 –∏ Hugging Face Transformers  Reading time  
    6 min
   Views  7.1K Python *Machine learning *Artificial Intelligence Natural Language Processing * 
    Tutorial
   
    Translation
     
                Original author:
                
                  Philipp Schmid
                  –í —ç—Ç–æ–º –ø–æ—Å—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º —Ä–∞–±–æ—Ç—É —Å PyTorch 2.0 –∏ Hugging Face Transformers –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ fine-tune –º–æ–¥–µ–ª–∏ BERT –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.PyTorch 2.0 –ª—É—á—à–µ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã, –±–æ–ª–µ–µ —É–¥–æ–±–Ω—ã–π –¥–ª—è Python, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –æ—Å—Ç–∞–µ—Ç—Å—è —Ç–∞–∫–∏–º –∂–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º, –∫–∞–∫ –∏ —Ä–∞–Ω–µ–µ.–†–∞–∑–±–µ—Ä–µ–º —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch 2.0.–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞.Fine-tune –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ BERT —Å –ø–æ–º–æ—â—å—é Hugging Face Trainer.–ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏.–ö—Ä–∞—Ç–∫–æ–µ –≤–≤–µ–¥–µ–Ω–∏–µ: PyTorch 2.0 PyTorch 2.0, –∏–ª–∏, —Ç–æ—á–Ω–µ–µ, 1.14, –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—Ä–∞—Ç–Ω–æ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å¬†–ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏. –û–Ω –Ω–µ¬†–ø–æ—Ç—Ä–µ–±—É–µ—Ç –∫–∞–∫–∏—Ö‚Äë–ª–∏–±–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤¬†—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –∫–æ–¥–µ PyTorch, –Ω–æ¬†–º–æ–∂–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥, –µ—Å–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å model = torch.compile(model). –ö–æ–º–∞–Ω–¥–∞ PyTorch —Ç–∞–∫ –æ–±—ä—è—Å–Ω—è–µ—Ç –ø–æ—è–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ –≤¬†—Å–≤–æ–µ–º FAQ: ¬´–ú—ã –≤—ã–ø—É—Å—Ç–∏–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ, –Ω–∞¬†–Ω–∞—à –≤–∑–≥–ª—è–¥, –º–µ–Ω—è—é—Ç —Ç–æ, –∫–∞–∫¬†–≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ PyTorch, –ø–æ—ç—Ç–æ–º—É –º—ã –Ω–∞–∑–≤–∞–ª–∏ —ç—Ç–æ 2.0¬†–≤–º–µ—Å—Ç–æ 1.14.¬ª–°—Ä–µ–¥–∏ —ç—Ç–∏—Ö –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π: –ø–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ TorchDynamo, AOTAutograd, PrimTorch –∏ TorchInductor. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç PyTorch 2.0 –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è –≤ 1,3-2 —Ä–∞–∑–∞ –Ω–∞ –±–æ–ª–µ–µ 40 –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö –º–æ–¥–µ–ª–µ–π –æ—Ç HuggingFace Transformers. –ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ PyTorch 2.0 –º–æ–∂–Ω–æ —É–∑–Ω–∞—Ç—å –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º "GET STARTED".–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –≠—Ç–æ—Ç —Ç—É—Ç–æ—Ä–∏–∞–ª –±—ã–ª —Å–æ–∑–¥–∞–Ω –∏ –∑–∞–ø—É—â–µ–Ω –Ω–∞ –∏–Ω—Å—Ç–∞–Ω—Å–µ AWS EC2 g5.xlarge, –≤–∫–ª—é—á–∞—è GPU NVIDIA A10G.1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch 2.0 –ü–µ—Ä–≤—ã–π —à–∞–≥ - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å PyTorch 2.0 –∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –æ—Ç Hugging Face, transformers –∏ datasets.# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch 2.0 —Å cuda 11.7
!pip install "torch>=2.0" --extra-index-url https://download.pytorch.org/whl/cu117 --upgrade --quiet –¢–∞–∫–∂–µ —Å—Ç–∞–≤–∏–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é transformers , –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞—Ç–∏–≤–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é PyTorch 2.0 –≤ Trainer.# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ transformers –∏ dataset
!pip install "transformers==4.27.1" "datasets==2.9.0" "accelerate==0.17.1" "evaluate==0.4.0" tensorboard scikit-learn
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ git-lfs –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏ –ª–æ–≥–æ–≤ –≤ hugging face hub
!sudo apt-get install git-lfs --yes–í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –¥–ª—è –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Hugging Face Hub. –ß—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ Hub, –≤–Ω–∞—á–∞–ª–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ Hugging Face. –î–ª—è –≤—Ö–æ–¥–∞ –≤ —Å–≤–æ—é —É—á–µ—Ç–Ω—É—é –∑–∞–ø–∏—Å—å –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ (–∫–ª—é—á–∞ –¥–æ—Å—Ç—É–ø–∞) –Ω–∞ –¥–∏—Å–∫–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º login –∏–∑ –ø–∞–∫–µ—Ç–∞ huggingface_hub.from huggingface_hub import login
login(
  token="", # ADD YOUR TOKEN HERE
  add_to_git_credential=True
)2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ë—É–¥–µ–º –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ BANKING77. –î–∞—Ç–∞—Å–µ—Ç BANKING77 —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±—Ä–∞—â–µ–Ω–∏—è –æ—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏–∑ –æ–±–ª–∞—Å—Ç–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ/—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —Å–µ–∫—Ç–æ—Ä–∞. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 13 083 –æ–±—Ä–∞—â–µ–Ω–∏–π, —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –Ω–∞ 77 –∏–Ω—Ç–µ–Ω—Ç–æ–≤ (–∫–ª–∞—Å—Å–æ–≤). –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ BANKING77 –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ load_dataset() –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Datasets.from datasets import load_dataset
# Dataset id from huggingface.co/dataset
dataset_id = "banking77"
# Load raw dataset
raw_dataset = load_dataset(dataset_id)
print(f"Train dataset size: {len(raw_dataset['train'])}")
print(f"Test dataset size: {len(raw_dataset['test'])}")–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.from random import randrange
random_id = randrange(len(raw_dataset['train']))
raw_dataset['train'][random_id]
# {'text': "I can't get google pay to work right.", 'label': 2}–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ —Ç–æ–∫–µ–Ω—ã. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º. –¢–∞–∫–∂–µ –æ–Ω –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–º —Å–ª–æ–≤–∞—Ä–µ. –ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –º–æ–∂–Ω–æ —É–∑–Ω–∞—Ç—å –≤ –≥–ª–∞–≤–µ 6 –æ—Ç¬†Hugging Face Course.from transformers import AutoTokenizer
# Model id to load the tokenizer
model_id = "bert-base-uncased"
# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)
# Tokenize helper function
def tokenize(batch):
    return tokenizer(batch['text'], padding='max_length', truncation=True, return_tensors="pt")
# Tokenize dataset
raw_dataset =  raw_dataset.rename_column("label", "labels") # to match Trainer
tokenized_dataset = raw_dataset.map(tokenize, batched=True,remove_columns=["text"])
print(tokenized_dataset["train"].features.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask','lable'])3. Fine-tune –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ BERT —Å –ø–æ–º–æ—â—å—é Hugging Face Trainer–ü–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–Ω–æ –Ω–∞—á–∏–Ω–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏. –ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å bert-base-uncased. –ü–µ—Ä–≤—ã–º —à–∞–≥–æ–º –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é –∫–ª–∞—Å—Å–∞ AutoModelForSequenceClassification –∏–∑ Hugging Face Hub. –¢–∞–∫ –º—ã —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –≤–µ—Å–∞–º–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERT, –Ω–æ —Å "–≥–æ–ª–æ–≤–æ–π" —Å–≤–µ—Ä—Ö—É —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥ –Ω–∞—à—É –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ó–¥–µ—Å—å –º—ã –ø–µ—Ä–µ–¥–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (77) –∏–∑ –Ω–∞—à–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∏–º–µ–Ω–∞ –º–µ—Ç–æ–∫, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–æ–ª–µ–µ —á–∏—Ç–∞–±–µ–ª—å–Ω—ã–º.from transformers import AutoModelForSequenceClassification
# Model id to load the tokenizer
model_id = "bert-base-uncased"
# Prepare model labels - useful for inference
labels = tokenized_dataset["train"].features["labels"].names
num_labels = len(labels)
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = str(i)
    id2label[str(i)] = label
# Download the model from huggingface.co/models
model = AutoModelForSequenceClassification.from_pretrained(
    model_id, num_labels=num_labels, label2id=label2id, id2label=id2label
)–ë—É–¥–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. Trainer –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –¥–ª—è —ç—Ç–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–∏–º compute_metrics. –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É evaluate –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫–∏ f1 –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.import evaluate
import numpy as np
# Metric Id
metric = evaluate.load("f1")
# Metric helper method
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels, average="weighted")–ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥ - –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã TrainingArguments –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ó–¥–µ—Å—å –∂–µ –¥–æ–±–∞–≤–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç PyTorch 2.0 –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è. –ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è PyTorch 2.0 –ø–µ—Ä–µ–¥–∞–µ–º –æ–ø—Ü–∏—é torch_compile –≤ TrainingArguments.–¢–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é Trainer —Å Hugging Face Hub –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏, –ª–æ–≥–æ–≤ –∏ –º–µ—Ç—Ä–∏–∫ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.from huggingface_hub import HfFolder
from transformers import Trainer, TrainingArguments
# Id for remote repository
repository_id = "bert-base-banking77-pt2"
# Define training args
training_args = TrainingArguments(
    output_dir=repository_id,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=8,
    learning_rate=5e-5,
	num_train_epochs=3,
	# PyTorch 2.0 specifics
    bf16=True, # bfloat16 training
	torch_compile=True, # optimizations
    optim="adamw_torch_fused", # improved optimizer
    # logging & evaluation strategies
    logging_dir=f"{repository_id}/logs",
    logging_strategy="steps",
    logging_steps=200,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    # push to hub parameters
    report_to="tensorboard",
    push_to_hub=True,
    hub_strategy="every_save",
    hub_model_id=repository_id,
    hub_token=HfFolder.get_token(),
)
# Create a Trainer instance
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
)–î–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–µ–º –º–µ—Ç–æ–¥ train –æ—Ç Trainer.# Start training
trainer.train()–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è –º—ã —Ç–∞–∫–∂–µ –∑–∞–ø—É—Å—Ç–∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –æ–ø—Ü–∏–∏ torch_compile:{'train_runtime': 696.2701, 'train_samples_per_second': 43.1, 'eval_f1': 0.928788}–ò—Å–ø–æ–ª—å–∑—É—è –æ–ø—Ü–∏—é torch_compile –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é adamw_torch_fused, –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –Ω–∞ 52.5% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—É—á–µ–Ω–∏–µ–º –±–µ–∑ PyTorch 2.0:{'train_runtime': 457.7964, 'train_samples_per_second': 65.55, 'eval_f1': 0.931773}–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —Å–Ω–∏–∑–∏–ª–∏—Å—å —Å 696 –¥–æ 457 —Å–µ–∫—É–Ω–¥. –ó–Ω–∞—á–µ–Ω–∏–µ train_samples_per_second –≤—ã—Ä–æ—Å–ª–æ —Å 43 –¥–æ 65. –ó–Ω–∞—á–µ–Ω–∏–µ f1-–º–µ—Ç—Ä–∏–∫–∏ —Ç–∞–∫–æ–µ –∂–µ –∏–ª–∏ —á—É—Ç—å –ª—É—á—à–µ, —á–µ–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è torch_compile.PyTorch 2.0 –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –º–æ—â–µ–Ω! üöÄ–°–æ—Ö—Ä–∞–Ω–∏–º –Ω–∞—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ Hugging Face Hub –∏ —Å–æ–∑–¥–∞–¥–∏–º –∫–∞—Ä—Ç–æ—á–∫—É –º–æ–¥–µ–ª–∏.# Save processor and create model card
tokenizer.save_pretrained(repository_id)
trainer.create_model_card()
trainer.push_to_hub()4. –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ. –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—É—á–∞–µ–º —Å –ø–æ–º–æ—â—å—é pipeline –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ transformers.from transformers import pipeline
# load model from huggingface.co/models using our repository id
classifier = pipeline("sentiment-analysis", model=repository_id, tokenizer=repository_id, device=0)
sample = "I have been waiting longer than expected for my bank card, could you provide information on when it will arrive?"
pred = classifier(sample)
print(pred)
# [{'label': 'card_arrival', 'score': 0.9903606176376343}]–ó–∞–∫–ª—é—á–µ–Ω–∏–µ–í¬†—ç—Ç–æ–º –ø–æ—Å—Ç–µ –º—ã —Ä–∞–∑–æ–±—Ä–∞–ª–∏—Å—å, –∫–∞–∫¬†–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch 2.0¬†–¥–ª—è¬†–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞¬†–Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö BANKING77. PyTorch 2.0¬†‚Äî —ç—Ç–æ –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —É—Å–∫–æ—Ä–∏—Ç—å –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –í¬†–Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ, –∑–∞–ø—É—â–µ–Ω–Ω–æ–º –Ω–∞¬†NVIDIA A10G, –º—ã —Å–º–æ–≥–ª–∏ –¥–æ—Å—Ç–∏—á—å –Ω–∞ 52.5% –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º—ã —É–≤–∏–¥–µ–ª–∏, –∫–∞–∫ –Ω–µ—Å–ª–æ–∂–Ω–æ –¥–æ–æ–±—É—á–∏—Ç—å BERT –ø–æ–¥¬†—Å–≤–æ—é –∑–∞–¥–∞—á—É —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ Hugging Face –∏ PyTorch. –ú–æ–π —Ç–µ–ª–µ–≥—Ä–∞–º‚Äë–∫–∞–Ω–∞–ª –æ¬†DS –∏ –Ω–µ¬†—Ç–æ–ª—å–∫–æ.      Tags: pytorchtransformershuggingfacemachine learningbertnlpdata scienceneural networks  Hubs: PythonMachine learningArtificial IntelligenceNatural Language Processing          


