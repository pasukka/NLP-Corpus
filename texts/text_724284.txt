

Диффузионная нейросеть ModelScope text2video 1.7B — создаём видео по текстовому описанию у себя дома / Habr


               Диффузионная нейросеть ModelScope text2video 1.7B — создаём видео по текстовому описанию у себя дома Level of difficulty  
    Easy
   Reading time  
    3 min
   Views  13K Working with video *Python *Machine learning *Artificial Intelligence The future is here  
    Tutorial
   
    Machine learning season
  
    From sandbox
       Ещё не успело ИИ-сообщество оправится от набега ЛЛаМ и высвобождения GPT-4, подоспела новая напасть — 19го марта была выпущена китайская нейросеть ModelScope text2video от Alibaba, создающая короткие видеоролики по текстовому описанию.Галерея примеровКаковы её предшественники?Видеоролики создаются подобно Imagen, Phenaki и Make-a-Video. Берётся существующая архитектура text2image модели (например, диффузионного типа), которая строит 2-мерные картинки по описанию, и переделывается в модель, строящую 2+1 мерные картинки, где новое измерение отвечает за связанность во времени. Модель от Alibaba не первая опенсорсная text2video модель, до этого такой была CogVideo (опять же, от китайцев). Пример работы CogVideoВ чём ключевое отличие этой модели?В её минималистичности. Тогда как CogVideo требовала для запуска как минимум A100 с 40 гб видеопамяти, эту модель можно вполне себе комфортно запустить с 12 гб видеопамяти, с 6-8 чуть менее комфортно, но терпимо. Хотя энтузиасты уже умудрились уместить её всего лишь в 4 гб.Ролик длительностью в 24 кадра и разрешением 192x192, созданный на видеокарте с 4 гб видеопамяти. Вспоминается та хабр-статья "4.2 гигабайта или как нарисовать всё, что угодно", но только теперь применимо к видеоКак же эта модель устроена изнутри?Как и описано выше, модель в своей основе полагается на латентные диффузионные модели, а конкретно, Stable Diffusion. В начале текст запроса при помощи предобученной нейросети OpenCLIP кодируется в вектор, отвечающий за его визуальные особенности; видеозапись сжимается и переводится в латентное пространство сетью VQGAN. Далее, в то время, пока идёт процесс обучения, к видеозаписи сначала последовательно подбирается такой шум, который полностью её сотрёт в нормальное распределение цветов пикселей (Гауссова диффузия), а затем подбирается 'контр-шум', чтобы её восстановить. Не буду здесь вдаваться в детали, о том, как работают диффузионные модели, на Хабре можно прочитать, например, здесь https://habr.com/ru/post/713076/. Тем не менее, у этой модели процесс шумоподавления модифицирован: вводится зависимость для шума, прилагаемого к одному из кадров, от состояния остальных кадров видео. По окончании процесса видео возвращается до исходного разрешения при помощи VQGAN.Иллюстрация процесса сэмплированияКак её запустить?Сами разработчики представили веса и демо своей модели на сайте Huggingface и ноутбук Google Colab. Кроме того, благодаря тому, что эта модель лишь слегка модифицированный Unet (Unet2d->Unet3d), в течении дня после выпуска статьи и первых твитов появился плагин для StableDiffusion WebUI от AUTOMATIC1111. Давайте им и воспользуемся.Шаг 1.Загружаем веса с официального репозитория https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis. Не забудьте удостовериться, что Вы загрузили configuration.json в его текстовой форме через кнопку raw, а не его html-страницу. Помещаем их в папку 'stable-diffusion-webui/models/ModelScope/t2v'.Модели на странице репозиторияШаг 2.Загружаем StableDiffusion WebUI, если он у Вас ещё не установлен https://github.com/AUTOMATIC1111/stable-diffusion-webui, в нём проходим во вкладку Extensions и выбираем в списке ModelScope text2video.Шаг 3.Вводим текстовый запрос (здесь tiny cute green monster and the big flying UFO, чтобы соответсвтовать духу Хабра) и устанавливаем параметры генерации, нажимаем Generate. Ждём какое-то время (пока в командной строке не заполнится прогрессбар и не выведится зелёная надпись done)Интерфейс плагинаВывод командной строкиНажимаем на кнопку 'Click here....' и радуемся результатом.Вот наш пришелец в формате mp4, осталось только на него нажать (и сконвертировать в gif, чтобы вставить в хабр, аргх)Хабр-НЛО в представлении видео-нейросети!Такие дела. Нас ждут безграничные видеомемы!НапоследокВидеомемы уже начались, например, на Реддите с помощью этой программы люди сделали нейросетевой ситком с Джо Байденом и Дональдом Трампом, где сценарий писала ChatGPT-4 https://www.reddit.com/r/StableDiffusion/comments/11xzh08/presenting_joe_and_the_don_100_ai_generated_sitcom/      Tags: text2videomodelscopeдиффузионная модельlatent diffusionstable diffusionгенеративный иигенеративные моделигенеративное искусствоalibabaсезон machine learning  Hubs: Working with videoPythonMachine learningArtificial IntelligenceThe future is here          


