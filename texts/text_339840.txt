

Когда лучше не использовать глубинное обучение / Habr


              11  October  2017 at 13:53  Когда лучше не использовать глубинное обучение Data Mining *Algorithms *Big Data *Mathematics *Machine learning * 
        Translation
         
                Original author:
                
                  Pablo Cordero
                  Я понимаю, что странно начинать блог с негатива, но за последние несколько дней поднялась волна дискуссий, которая хорошо соотносится с некоторыми темами, над которыми я думал в последнее время. Всё началось с поста Джеффа Лика в блоге Simply Stats с предостережением об использовании глубинного обучения на малом размере выборки. Он утверждает, что при малом размере выборки (что часто наблюдается в биологии), линейные модели с небольшим количеством параметров работают эффективнее, чем нейросети даже с минимумом слоёв и скрытых блоков. 


Далее он показывает, что очень простой линейный предиктор с десятью самыми информативными признаками работает эффективнее простой нейросети в задаче классификации нулей и единиц в наборе данных MNIST, при использовании всего около 80 образцов. Эта статья сподвигла Эндрю Бима написать опровержение, в котором правильно обученная нейросеть сумела превзойти простую линейную модель, даже на очень малом количестве образцов. 


Такие споры идут на фоне того, что всё больше и больше исследователей в области биомедицинской информатики применяют глубинное обучение на различных задачах. Оправдан ли ажиотаж, или нам достаточно линейных моделей? Как всегда, здесь нет однозначного ответа. В этой статье я хочу рассмотреть случаи применения машинного обучения, где использование глубоких нейросетей вообще не имеет смысла. А также поговорить о распространённых предрассудках, которые, на мой взгляд, мешают действительно эффективно применять глубинное обучение, особенно у новичков.

Разрушение предрассудков о глубинном обучении

Сначала поговорим о некоторых предрассудках. Мне кажется, они присутствуют у большинства специалистов, не слишком осведомлённых в теме глубинного обучения, а на самом деле являются полуправдой. Есть два очень распространённых и один немного более технический предрассудок — на них и остановимся подробнее. Это в каком-то роде продолжение великолепной главы «Заблуждения» в статье Эндрю Бима.

Глубинное обучение действительно может работать на малых размерах выборки

Глубинное обучение прославилось эффективной обработкой большого количества входных данных (помните, что первый проект Google Brain предусматривал загрузку в нейросеть большого количества видеороликов YouTube), и с тех пор постоянно описывалось как сложные алгоритмы, работающие на большом объёме данных. К сожалению, эта пара big data и глубинного обучения как-то привела людей к противоположной мысли: миф, что глубинное обучение нельзя использовать на малых выборках. 


Если у вас всего несколько образцов, запуск нейросети высоким соотношением параметров на образец на первый взгляд может показаться прямой дорогой переобучению. Однако простой учёт объёма выборки и размерности для данной конкретной проблемы, при обучении с учителем или без учителя, — это нечто вроде моделирования данных в вакууме, без контекста. А ведь нужно учесть, что в таких случаях у вас есть релевантные источники данных или убедительные предварительные данные, которые может предоставить эксперт в данной области, или данные структурированы очень конкретным образом (например, в виде графа или изображения). Во всех этих случаях есть вероятность, что глубинное обучение принесёт пользу — например, вы можете закодировать полезные репрезентации более крупных родственных наборов данных и использовать их в своей задаче. Классический пример такой ситуации часто встречается в обработке естественного языка, где можно усвоить информацию о включении отдельных слов в большом словарном корпусе типа Википедии, а затем а затем использовать информацию о включениях слов на более маленьком, узком корпусе при обучении с учителем. 


В крайнем случае у вас может быть несколько нейросетей, которые совместно усваивают репрезентацию и эффективный способ её повторного использования на малых наборах образцов. Это называется обучением с первого раза (one-shot learning), и оно успешно применяется в разных областях с многомерными данными, в том числе в машинном зрении и открытии новых лекарств.


Сети обучения с первого раза в открытии новых лекарств. Иллюстрация из статьи Altae-Tran et al. ACS Cent. Sci. 2017

Глубинное обучение — не универсальное решение всех проблем

Второе заблуждение, которое приходится часто слышать, — это настоящий хайп. Многие начинающие практику специалисты рассчитывают, что глубинные сети дадут им сказочный скачок прирост производительности просто потому что так оно и происходит в других областях. Другие находятся под впечатлением потрясающих успехов глубинного обучения в моделировании и манипулировании изображениями, звука и в лингвистике — в трёх типах данных, наиболее близких человеку — и они с головой окунаются в эту область, пытаясь обучить самую модную последнюю архитектуру состязательных нейросетей. Такой ажиотаж проявляется по-разному. 


Глубинное обучение стало неоспоримой силой в машинном обучении и важным инструментом в арсенале любого разработчика моделей данных. Его популярность привела к созданию важных фреймворков, таких как TensorFlow и PyTorch, невероятно полезных даже за пределами глубинного обучения. История его превращения из андердога в суперзвезду вдохновила исследователей на пересмотр других методов, ранее считавшихся невразумительными, таких как эволюционные стратегии и обучение с подкреплением. Но это ни в коем случае не панацея. Помимо соображений об отсутствии халявы в принципе, могу сказать, что модели глубинного обучения могут иметь важные нюансы, требовать аккуратного обращения, а иногда очень затратного поиска гиперпараметров, настройки и тестирования (об этом подробнее см. далее в статье). Кроме того, есть много случаев, где глубинное обучение просто не имеет смысла с практической точки зрения, а более простые модели работают гораздо лучше.

Глубинное обучение — это больше, чем .fit()

Есть ещё один аспект моделей глубинного обучения, который, по моим наблюдениям, неправильно воспринимается с точки зрения других областей машинного обучения. Большинство учебников и вводных материалов по глубинному обучению описывают эти модели как составленные из иерархически связанных слоёв узлов, где первый слой принимает входной сигнал, а последний слой выдаёт выходной сигнал, а вы можете обучить их, используя некую форму стохастического градиентного спуска. Иногда может вкратце упоминаться, как работает стохастический градиентный спуск и что такое обратное распространение ошибки. Но львиная часть объяснения посвящена богатому разнообразию типов нейросетей (свёрточные, рекуррентные и т.д.). Самим методам оптимизации уделяется мало внимания, и это очень плохо, потому что они представляют собой важную (если не самую важную) часть работы любой сети глубинного обучения и потому что знание этих конкретных методов (почитайте, к примеру, этот пост Ференца Хужара и его научную статью, которая там упоминается), и знание, как оптимизировать параметры этих методов и как разделить данные для их эффективного использования крайне важно для получения хорошей сходимости в разумное время. 


Почему именно стохастические градиенты настолько важны — пока неизвестно, но специалисты здесь и там высказывают разные предположения на этот счёт. Одно из моих любимых — интерпретация этих методов как часть расчёта байесовского вывода. По сути, каждый раз при осуществлении какой-нибудь численной оптимизации вы рассчитываете байсовский вывод с определёнными предположениями. В конце концов, есть целая область под названием вероятностная нумерика, которая буквально выросла из такой интерпретации. 


Стохастический градиентный спуск (SGD) ничем не отличается, а последние научные работы предполагают, что эта процедура на самом деле представляет собой цепь Маркова, которая при определённых допущениях демонстрирует стационарное распределение, а его можно рассматривать как разновидность вариационного приближения к апостериорной вероятности. Так что если вы остановите свой SGD и примете финальные параметры, то вы в реальности получаете образцы из этого приближённого распределения. 


Мне эта идея показалась очень яркой. Она многое объясняет, потому что параметры оптимизатора (в этом случае, скорость обучения) теперь получают гораздо больше смысла. Такой пример: как вы можете изменить параметр скорости обучения в стохастическом градиентном спуске, так и цепь Маркова становится нестабильной, пока не найдёт широкий локальный минимум, охватывающий образцы в большой области; таким образом, вы увеличиваете дисперсию процедуры. С другой стороны, если уменьшить скорость обучения, цепь Маркова медленно приближается к более узкому локальному минимуму, пока не сойдётся в очень узкой области; таким образом, вы увеличиваете перекос к определённой области. 


Другой параметр в SGD, размер пакетов (batch size), тоже контролирует, в каком типе области алгоритм сойдётся: в более широких областях для маленьких пакетов или в более чётких областях с пакетами б