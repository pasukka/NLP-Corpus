

Julia и дистрибутивная семантика / Habr


              5  December  2019 at 08:55  Julia и дистрибутивная семантика Programming *Julia *Machine learning *Artificial Intelligence Natural Language Processing * 
        Tutorial
           
С момента выхода прошлой публикации в мире языка Julia произошло много интересного:

Она заняла все первые места в плане роста вспомогательных пакетов. За это я и люблю статистику — главное выбрать удобную единицу измерения, например проценты как в приведенном ресурсе
Вышла версия 1.3.0 — из самых масштабных нововведений там модернизация менеджера пакетов и появление многопоточного параллелизма
Джулия заручается поддержкой Nvidia
Американский департамент перспективных исследований в области энергетики выделил кучу денег на решение задач оптимизации

В то же время заметен рост интереса со стороны разработчиков, что выражается обильными бенчмаркингами:

Международное энергетическое агенство проверяет пакеты реализующие многомерную оптимизацию
Датасаянтисты тестят работу с GPU
Ни капли не предвзятые ребята сравнивают интеграторы для дифуров
А энтузиасты сравнивают языки на базовых задачах.

Мы же просто радуемся новым и удобным инструментам и продолжаем их изучать. Сегодняшний вечер будет посвящен текстовому анализу, поиску скрытого смысла в выступлениях президентов и генерации текста в духе Шекспира и джулиа-программиста, а на сладкое — скормим рекуррентной сети 40000 пирожков.
Недавно здесь на Хабре был выполнен обзор пакетов для Julia позволяющие проводить исследования в области NLP — Julia NLP. Обрабатываем тексты. Так что сразу приступим у делу и начнем с пакета TextAnalysis.
TextAnalisys
Пусть задан некоторый текст, который мы представляем в виде строкового документа:
using TextAnalysis
str = """
Ich mag die Sonne, die Palmen und das Meer,
Ich mag den Himmel schauen, den Wolken hinterher.
Ich mag den kalten Mond, wenn der Vollmond rund,
Und ich mag dich mit einem Knebel in dem Mund.
""";
sd = StringDocument(str)
StringDocument{String}("Ich mag die ... dem Mund.\n", 
TextAnalysis.DocumentMetadata(Languages.Default(), 
"Untitled Document", 
"Unknown Author", 
"Unknown Time"))
Для удобной работы с большим количеством документов есть возможность менять поля, например заглавия, а также, чтоб упростить обработку, можем удалять пунктуацию и заглавные буквы:
title!(sd, "Knebel")
prepare!(sd, strip_punctuation)
remove_case!(sd)
text(sd)
"ich mag die sonne  die palmen und das meer \nich mag den himmel schauen  den wolken hinterher \nich mag den kalten mond  wenn der vollmond rund \nund ich mag dich mit einem knebel in dem mund \n"
что позволяет строить незахламленные n-граммы для слов:
dict1 = ngrams(sd)
Dict{String,Int64} with 26 entries:
  "dem"       => 1
  "himmel"    => 1
  "knebel"    => 1
  "der"       => 1
  "schauen"   => 1
  "mund"      => 1
  "rund"      => 1
  "in"        => 1
  "mond"      => 1
  "dich"      => 1
  "einem"     => 1
  "ich"       => 4
  "hinterher" => 1
  "wolken"    => 1
  "den"       => 3
  "das"       => 1
  "palmen"    => 1
  "kalten"    => 1
  "mag"       => 4
  "sonne"     => 1
  "vollmond"  => 1
  "die"       => 2
  "mit"       => 1
  "meer"      => 1
  "wenn"      => 1
  "und"       => 2
Понятное дело, что знаки пунктуации и слова с заглавными буквами будут отдельными единицами в словаре, что будет мешать качественно оценить частотные вхождения конкретных слагаемых нашего текста, посему от них и избавились. Для n-грамм нетрудно найти множество всяческих интересных применений, например, с их помощью можно осуществлять нечеткий поиск в тексте, ну а так как мы просто туристы, то обойдемся игрушечными примерами, а именно генерацией текста с помощью цепей Маркова
Proch