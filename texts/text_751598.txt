

Как маленькая нейроязыковая модель в Клавиатуре победила серверные подсказки / Habr


                Как маленькая нейроязыковая модель в Клавиатуре победила серверные подсказки  Reading time  
    9 min
   Views  11K Яндекс corporate blog Development of mobile applications *Algorithms *Machine learning *Artificial Intelligence       Основная задача любой мобильной клавиатуры — помогать пользователям в общении, а именно — вводить текст быстро и без ошибок. Этого можно достичь при помощи разных компонентов: подсказок, автокорректа, тап-модели, голосового ввода, ввода свайпом. Все эти компоненты сильно отличаются друг от друга: скажем, тап-модель помогает предугадывать нажатие следующей буквы, а ввод свайпом переводит нарисованные пользователем кривые в слова.Казалось бы, что между ними нет ничего общего, но это не так. Абсолютно все эти компоненты объединяет одно — языковая модель. Чем выше её качество, тем меньше ошибок будет допущено при вводе текста, а значит, пользователь будет чуточку счастливее.В этом посте я расскажу, как мы создавали нейроязыковую модель для Яндекс Клавиатуры, ушли от облачных подсказок и научили клавиатуру адаптироваться к приложениям. Немного историиИсторически языковая модель и большинство компонент (например, подсказки) нам достались из Яндекс Спеллера. Эта технология позволяет предлагать наиболее вероятные исправления для слов с опечатками и ошибками. Спеллер использует CatBoost и благодаря ему, а также большой языковой модели, он может расшифровывать искажённые до неузнаваемости слова и учитывать контекст при исправлении опечаток.Но эту технологию нельзя было применить напрямую в Клавиатуре из-за ограничений, связанных с её спецификой:Размер моделей: модель должна занимать несколько десятков мегабайт, потому что ресурсы мобильного устройства ограничены.Задержка: мы должны успевать обрабатывать запросы в строго отведённое время. Подсказки должны появляться за 200 мс, а реакция на нажатие клавиши должна быть меньше 10 мс.Энергопотребление: нам нельзя сильно нагружать ЦПУ, часто обращаться к сети или тратить много времени на отрисовку.Универсальность: все предыдущие условия должны выполняться на любых смартфонах под всевозможными архитектурами.Для Спеллера, который работает на наших серверах, ограничения куда более щадящие. В его случае модель в несколько десятков, а то и сотни гигабайт, выглядит совершенно уместно. Поэтому нам пришлось проделать большую работу по сжатию и ускорению моделей, при этом сохранив качество.Со временем стало понятно, что мы начали потихоньку упираться в предел технологий, некогда породивших Клавиатуру. Спеллер основан на классическом подходе в машинном обучении, который включает в себя n-граммные языковые модели. N-граммные языковые модели строятся на основе статистик, собранных на больших корпусах текстов. Для этого все тексты нарезаются на n-граммы длиной от 1 до n, где n — максимальная длина сниппета или порядок модели. Дальше все одинаковые n-граммы объединяются и им выставляется общее количество. На основе такой большой таблички (n-грамма и её количество), можно аппроксимировать вероятность слова в данном контексте. Например, для триграммной модели вероятность слова в контексте приближается вот такой формулой:Качество таких моделей растёт с увеличением размера обучающих корпусов и порядка самой модели. Но это также означает, что и сама модель очень сильно разрастается. Как бы нам ни хотелось, мы не можем позволить себе использовать более крупную модель в самой Клавиатуре. Частично эту проблему решают облачные подсказки, которые показали хорошие результаты в экспериментах.Но на этом проблемы не исчерпываются, и появляется всё больше вопросов:Как передать дополнительную информацию, например, названия приложений: следует ли дополнительно хранить множество маленьких n-граммных моделей и смешивать их с основной?Как сделать персонализацию: следует ли держать дополнительную персональную n-граммную модель и так же подмешивать её к основной?Что делать с незнакомыми словами в контексте?Как обрабатывать более длинные контексты?Ответ на большинство этих вопросов уже известен: необходимо использовать более выразительную языковую модель, а именно — нейронную.Нейроязыковая модель Изучаем подходыПри начале нового проекта один из первых шагов — изучение уже существующего опыта. Есть множество публикаций о языковых моделях, однако нас интересует узкоспециализированная область — мобильная разработка, а если точнее — клавиатуры. Нужная нам информация нашлась в статьях разработчиков Samsung и Google.Для языковой модели Samsung используют обычную LSTM, а для эмбеддингов и финального слоя с softmax — полносвязанные матрицы. Чтобы уменьшить размер полученных моделей, они делают SVD-разложение, используют общие веса для эмбеддингов и для слоя softmax, а затем полученную модель дополнительно квантизуют. В результате размер модели уменьшается почти в 8 раз — до 7,4 Мб. Качество после таких сжатий незначительно проседает, но всё равно остаётся значительно выше предыдущей n-граммной модели. А генерация подсказок укладывается в 10 мс, что очень хорошо.Статья Samsung ценна ещё тем, что она обозначает основные проблемные места в таких моделях: большой размер эмбеддингов и слоя softmax. Это наблюдение пригодится нам в дальнейшем.В статье Google про Federated Learning есть очень краткое описание архитектуры: вместо обычной LSTM они используют CIFG, что позволяет сократить количество параметров на 25% и упростить обучение на устройствах. Так же, как и в Samsung, они шарят веса эмбеддингов и слоя softmax. В результате получается очень компактная модель размером всего 1,4 Мб. Но даже при таком маленьком размере, они с большим запасом выигрывают у n-граммной модели. Из этих статей становится ясно две вещи:В клавиатуре стоит использовать нейроязыковые модели: они значительно лучше по качеству n-граммных моделей и достаточно быстрые даже для мобильных устройств.Все основные оптимизации связаны с размером эмбеддингов и слоя softmax. Поэтому на них бы и обратим всё своё внимание.АрхитектураИтоговая архитектура языковой модели в клавиатуре выглядит так:Архитектура нашей КлавиатурыВ этой схеме нет ничего принципиально нового: все её составляющие давно известны. Самое интересное — как мы подбирали каждую компоненту исходя из наших критериев и требований.EncoderЗдесь мы не стали выдумывать ничего хитрого и взяли стандартную LSTM, которая показала себя с лучшей стороны на фоне других вариантов RNN. Хотя её использование может показаться несколько устаревшим в 2023 году, но в данном случае это полностью оправдано. Пользователи вводят текст последовательно, и LSTM, в силу своей авторегрессионности, нам отлично подходит. Для каждого нового токена достаточно пересчитать только одно состояние LSTM. Это то же самое, что выполнить одну достаточно тяжёлую итерацию энкодера. Учитывая ограниченные вычислительные ресурсы мобильных устройств, это достаточно эффективный подход.                 EmbeddingЗдесь мы рассмотрели несколько вариантов. Стандартный Word Embedding, где каждому слову из закрытого словаря ставится в соответствие свой вектор. Главный недостаток этого подхода — большой размер: размер эмбеддинга 