

Специалисты по этике AI раскритиковали призыв поставить на паузу эксперименты с искусственным интеллектом / Habr


               Специалисты по этике AI раскритиковали призыв поставить на паузу эксперименты с искусственным интеллектом  Reading time  
    2 min
   Views  4.8K Machine learning *Artificial Intelligence       Группа специалистов по этике AI выступила с ответом на письмо Future of Life, которое на прошлой неделе подписали около тысячи экспертов, среди которых были Илон Маск, Стив Возняк и Эндрю Янг. Авторы нового воззвания скептически отнеслись к призыву взять шестимесячную «паузу» в развитии AI и обучении систем, «более мощных, чем GPT-4». Они раскритиковали текст письма за акцент на гипотетических будущих угрозах, связанных с развитием искусственного интеллекта. По их мнению, реальный вред может принести использование AI-технологий уже сегодня. Тимнит Гебру, Эмили М. Бендер, Анджелина Макмиллан-Мейджор и Маргарет Митчелл — исследователи AI и специалисты по этике. Они стали известны благодаря статье «Об опасностях стохастических попугаев: могут ли языковые модели быть слишком большими?», после которой Тимнит Гебру уволили из Google. В настоящее время все они работают в DAIR Institute, новом исследовательском центре, цели которого — изучение, выявление и предотвращение вреда, который может причинить AI.Авторы ответа на письмо Future of Life называют опасной идеологию «долгосрочной перспективы», которой придерживаются критики AI, так как они игнорируют реальный вред, причиняемый сегодня искусственным интеллектом. Специалисты по этике обращают внимание на взрыв синтетических медиа в мире, которые ставят под угрозу информационную экосистему, и на концентрацию власти в руках нескольких людей, которая происходит благодаря AI.«Что нам нужно, так это регулирование, обеспечивающее прозрачность. Мало того что всегда должно быть ясно, когда мы сталкиваемся с синтетическими медиа, но и организации, создающие эти системы, также должны документировать и раскрывать обучающие данные и архитектуры моделей. Ответственность за создание безопасных для использования инструментов должна лежать на компаниях, которые создают и развёртывают генеративные системы, а это означает, что создатели этих систем должны нести ответственность за результаты, производимые их продуктами… Мы должны создавать машины, которые работают на нас, вместо того, чтобы «приспосабливать» общество к тому, чтобы оно было машиночитаемым и записываемым».TechCrunch в своём комментарии дополняет позицию авторов нового письма: «Беспокойство об апокалипсисе роботов в духе Терминатора или Матрицы — отвлекающий манёвр, когда в тот же момент у нас есть сообщения о том, что такие компании, как Clearview AI, используются полицией для слежки и обвинения случайных людей».      Tags: gpt-4aiискусственный интеллектмашинное обучениеписьморискиэтика  Hubs: Machine learningArtificial Intelligence          


